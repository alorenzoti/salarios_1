2023-03-03 16:25:56,225:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-03 16:25:56,225:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-03 16:25:56,226:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-03 16:25:56,226:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-03 16:25:57,717:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-03-03 16:26:30,338:INFO:PyCaret ClassificationExperiment
2023-03-03 16:26:30,339:INFO:Logging name: clf-default-name
2023-03-03 16:26:30,339:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-03-03 16:26:30,339:INFO:version 3.0.0.rc9
2023-03-03 16:26:30,340:INFO:Initializing setup()
2023-03-03 16:26:30,340:INFO:self.USI: f441
2023-03-03 16:26:30,340:INFO:self._variable_keys: {'gpu_n_jobs_param', 'pipeline', 'memory', 'data', 'fold_generator', 'y', 'X_test', 'target_param', '_ml_usecase', 'X_train', 'fold_groups_param', 'fix_imbalance', 'y_test', 'seed', 'gpu_param', 'exp_id', 'y_train', 'fold_shuffle_param', 'log_plots_param', '_available_plots', 'n_jobs_param', 'html_param', 'logging_param', 'USI', 'idx', 'X', 'exp_name_log', 'is_multiclass'}
2023-03-03 16:26:30,340:INFO:Checking environment
2023-03-03 16:26:30,340:INFO:python_version: 3.9.13
2023-03-03 16:26:30,341:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-03-03 16:26:30,341:INFO:machine: AMD64
2023-03-03 16:26:30,341:INFO:platform: Windows-10-10.0.19044-SP0
2023-03-03 16:26:30,341:INFO:Memory: svmem(total=17009516544, available=3393830912, percent=80.0, used=13615685632, free=3393830912)
2023-03-03 16:26:30,341:INFO:Physical Core: 4
2023-03-03 16:26:30,341:INFO:Logical Core: 8
2023-03-03 16:26:30,342:INFO:Checking libraries
2023-03-03 16:26:30,342:INFO:System:
2023-03-03 16:26:30,342:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-03-03 16:26:30,342:INFO:executable: C:\Users\Alvaro\anaconda3\python.exe
2023-03-03 16:26:30,342:INFO:   machine: Windows-10-10.0.19044-SP0
2023-03-03 16:26:30,342:INFO:PyCaret required dependencies:
2023-03-03 16:26:30,342:INFO:                 pip: 22.2.2
2023-03-03 16:26:30,342:INFO:          setuptools: 63.4.1
2023-03-03 16:26:30,342:INFO:             pycaret: 3.0.0rc9
2023-03-03 16:26:30,343:INFO:             IPython: 7.31.1
2023-03-03 16:26:30,343:INFO:          ipywidgets: 7.6.5
2023-03-03 16:26:30,343:INFO:                tqdm: 4.64.1
2023-03-03 16:26:30,343:INFO:               numpy: 1.21.5
2023-03-03 16:26:30,343:INFO:              pandas: 1.4.4
2023-03-03 16:26:30,343:INFO:              jinja2: 2.11.3
2023-03-03 16:26:30,343:INFO:               scipy: 1.9.1
2023-03-03 16:26:30,343:INFO:              joblib: 1.2.0
2023-03-03 16:26:30,343:INFO:             sklearn: 1.0.2
2023-03-03 16:26:30,343:INFO:                pyod: 1.0.7
2023-03-03 16:26:30,344:INFO:            imblearn: 0.10.1
2023-03-03 16:26:30,344:INFO:   category_encoders: 2.6.0
2023-03-03 16:26:30,344:INFO:            lightgbm: 3.3.5
2023-03-03 16:26:30,344:INFO:               numba: 0.55.1
2023-03-03 16:26:30,344:INFO:            requests: 2.28.1
2023-03-03 16:26:30,344:INFO:          matplotlib: 3.5.2
2023-03-03 16:26:30,344:INFO:          scikitplot: 0.3.7
2023-03-03 16:26:30,344:INFO:         yellowbrick: 1.5
2023-03-03 16:26:30,344:INFO:              plotly: 5.9.0
2023-03-03 16:26:30,344:INFO:             kaleido: 0.2.1
2023-03-03 16:26:30,345:INFO:         statsmodels: 0.13.2
2023-03-03 16:26:30,345:INFO:              sktime: 0.16.1
2023-03-03 16:26:30,345:INFO:               tbats: 1.1.2
2023-03-03 16:26:30,345:INFO:            pmdarima: 2.0.2
2023-03-03 16:26:30,345:INFO:              psutil: 5.9.0
2023-03-03 16:26:30,346:INFO:PyCaret optional dependencies:
2023-03-03 16:26:30,393:INFO:                shap: Not installed
2023-03-03 16:26:30,393:INFO:           interpret: Not installed
2023-03-03 16:26:30,393:INFO:                umap: Not installed
2023-03-03 16:26:30,393:INFO:    pandas_profiling: Not installed
2023-03-03 16:26:30,395:INFO:  explainerdashboard: Not installed
2023-03-03 16:26:30,395:INFO:             autoviz: Not installed
2023-03-03 16:26:30,395:INFO:           fairlearn: Not installed
2023-03-03 16:26:30,395:INFO:             xgboost: Not installed
2023-03-03 16:26:30,395:INFO:            catboost: Not installed
2023-03-03 16:26:30,395:INFO:              kmodes: Not installed
2023-03-03 16:26:30,395:INFO:             mlxtend: Not installed
2023-03-03 16:26:30,395:INFO:       statsforecast: Not installed
2023-03-03 16:26:30,395:INFO:        tune_sklearn: Not installed
2023-03-03 16:26:30,395:INFO:                 ray: Not installed
2023-03-03 16:26:30,395:INFO:            hyperopt: Not installed
2023-03-03 16:26:30,395:INFO:              optuna: Not installed
2023-03-03 16:26:30,395:INFO:               skopt: Not installed
2023-03-03 16:26:30,395:INFO:              mlflow: Not installed
2023-03-03 16:26:30,395:INFO:              gradio: Not installed
2023-03-03 16:26:30,395:INFO:             fastapi: Not installed
2023-03-03 16:26:30,395:INFO:             uvicorn: Not installed
2023-03-03 16:26:30,395:INFO:              m2cgen: Not installed
2023-03-03 16:26:30,395:INFO:           evidently: Not installed
2023-03-03 16:26:30,395:INFO:               fugue: Not installed
2023-03-03 16:26:30,395:INFO:           streamlit: Not installed
2023-03-03 16:26:30,395:INFO:             prophet: Not installed
2023-03-03 16:26:30,395:INFO:None
2023-03-03 16:26:30,396:INFO:Set up data.
2023-03-03 16:27:14,027:WARNING:C:\Users\Alvaro\AppData\Local\Temp\ipykernel_15312\109572210.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  outliers = outliers.append(results)

2023-03-03 16:27:14,035:WARNING:C:\Users\Alvaro\AppData\Local\Temp\ipykernel_15312\109572210.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  outliers = outliers.append(results)

2023-03-03 16:27:14,042:WARNING:C:\Users\Alvaro\AppData\Local\Temp\ipykernel_15312\109572210.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  outliers = outliers.append(results)

2023-03-03 16:27:14,047:WARNING:C:\Users\Alvaro\AppData\Local\Temp\ipykernel_15312\109572210.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  outliers = outliers.append(results)

2023-03-03 16:27:28,042:INFO:PyCaret ClassificationExperiment
2023-03-03 16:27:28,055:INFO:Logging name: clf-default-name
2023-03-03 16:27:28,055:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-03-03 16:27:28,056:INFO:version 3.0.0.rc9
2023-03-03 16:27:28,056:INFO:Initializing setup()
2023-03-03 16:27:28,056:INFO:self.USI: 481d
2023-03-03 16:27:28,056:INFO:self._variable_keys: {'gpu_n_jobs_param', 'pipeline', 'memory', 'data', 'fold_generator', 'y', 'X_test', 'target_param', '_ml_usecase', 'X_train', 'fold_groups_param', 'fix_imbalance', 'y_test', 'seed', 'gpu_param', 'exp_id', 'y_train', 'fold_shuffle_param', 'log_plots_param', '_available_plots', 'n_jobs_param', 'html_param', 'logging_param', 'USI', 'idx', 'X', 'exp_name_log', 'is_multiclass'}
2023-03-03 16:27:28,056:INFO:Checking environment
2023-03-03 16:27:28,056:INFO:python_version: 3.9.13
2023-03-03 16:27:28,056:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-03-03 16:27:28,056:INFO:machine: AMD64
2023-03-03 16:27:28,057:INFO:platform: Windows-10-10.0.19044-SP0
2023-03-03 16:27:28,057:INFO:Memory: svmem(total=17009516544, available=3362242560, percent=80.2, used=13647273984, free=3362242560)
2023-03-03 16:27:28,057:INFO:Physical Core: 4
2023-03-03 16:27:28,057:INFO:Logical Core: 8
2023-03-03 16:27:28,057:INFO:Checking libraries
2023-03-03 16:27:28,057:INFO:System:
2023-03-03 16:27:28,058:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-03-03 16:27:28,058:INFO:executable: C:\Users\Alvaro\anaconda3\python.exe
2023-03-03 16:27:28,058:INFO:   machine: Windows-10-10.0.19044-SP0
2023-03-03 16:27:28,058:INFO:PyCaret required dependencies:
2023-03-03 16:27:28,058:INFO:                 pip: 22.2.2
2023-03-03 16:27:28,059:INFO:          setuptools: 63.4.1
2023-03-03 16:27:28,059:INFO:             pycaret: 3.0.0rc9
2023-03-03 16:27:28,059:INFO:             IPython: 7.31.1
2023-03-03 16:27:28,059:INFO:          ipywidgets: 7.6.5
2023-03-03 16:27:28,059:INFO:                tqdm: 4.64.1
2023-03-03 16:27:28,059:INFO:               numpy: 1.21.5
2023-03-03 16:27:28,059:INFO:              pandas: 1.4.4
2023-03-03 16:27:28,060:INFO:              jinja2: 2.11.3
2023-03-03 16:27:28,060:INFO:               scipy: 1.9.1
2023-03-03 16:27:28,060:INFO:              joblib: 1.2.0
2023-03-03 16:27:28,060:INFO:             sklearn: 1.0.2
2023-03-03 16:27:28,061:INFO:                pyod: 1.0.7
2023-03-03 16:27:28,061:INFO:            imblearn: 0.10.1
2023-03-03 16:27:28,061:INFO:   category_encoders: 2.6.0
2023-03-03 16:27:28,062:INFO:            lightgbm: 3.3.5
2023-03-03 16:27:28,062:INFO:               numba: 0.55.1
2023-03-03 16:27:28,062:INFO:            requests: 2.28.1
2023-03-03 16:27:28,062:INFO:          matplotlib: 3.5.2
2023-03-03 16:27:28,062:INFO:          scikitplot: 0.3.7
2023-03-03 16:27:28,063:INFO:         yellowbrick: 1.5
2023-03-03 16:27:28,063:INFO:              plotly: 5.9.0
2023-03-03 16:27:28,063:INFO:             kaleido: 0.2.1
2023-03-03 16:27:28,063:INFO:         statsmodels: 0.13.2
2023-03-03 16:27:28,064:INFO:              sktime: 0.16.1
2023-03-03 16:27:28,064:INFO:               tbats: 1.1.2
2023-03-03 16:27:28,064:INFO:            pmdarima: 2.0.2
2023-03-03 16:27:28,064:INFO:              psutil: 5.9.0
2023-03-03 16:27:28,064:INFO:PyCaret optional dependencies:
2023-03-03 16:27:28,065:INFO:                shap: Not installed
2023-03-03 16:27:28,065:INFO:           interpret: Not installed
2023-03-03 16:27:28,066:INFO:                umap: Not installed
2023-03-03 16:27:28,066:INFO:    pandas_profiling: Not installed
2023-03-03 16:27:28,066:INFO:  explainerdashboard: Not installed
2023-03-03 16:27:28,066:INFO:             autoviz: Not installed
2023-03-03 16:27:28,066:INFO:           fairlearn: Not installed
2023-03-03 16:27:28,067:INFO:             xgboost: Not installed
2023-03-03 16:27:28,067:INFO:            catboost: Not installed
2023-03-03 16:27:28,067:INFO:              kmodes: Not installed
2023-03-03 16:27:28,067:INFO:             mlxtend: Not installed
2023-03-03 16:27:28,067:INFO:       statsforecast: Not installed
2023-03-03 16:27:28,068:INFO:        tune_sklearn: Not installed
2023-03-03 16:27:28,068:INFO:                 ray: Not installed
2023-03-03 16:27:28,068:INFO:            hyperopt: Not installed
2023-03-03 16:27:28,068:INFO:              optuna: Not installed
2023-03-03 16:27:28,068:INFO:               skopt: Not installed
2023-03-03 16:27:28,068:INFO:              mlflow: Not installed
2023-03-03 16:27:28,069:INFO:              gradio: Not installed
2023-03-03 16:27:28,069:INFO:             fastapi: Not installed
2023-03-03 16:27:28,069:INFO:             uvicorn: Not installed
2023-03-03 16:27:28,069:INFO:              m2cgen: Not installed
2023-03-03 16:27:28,069:INFO:           evidently: Not installed
2023-03-03 16:27:28,069:INFO:               fugue: Not installed
2023-03-03 16:27:28,070:INFO:           streamlit: Not installed
2023-03-03 16:27:28,070:INFO:             prophet: Not installed
2023-03-03 16:27:28,070:INFO:None
2023-03-03 16:27:28,070:INFO:Set up data.
2023-03-03 16:27:28,096:INFO:Set up train/test split.
2023-03-03 16:29:51,398:INFO:PyCaret ClassificationExperiment
2023-03-03 16:29:51,401:INFO:Logging name: clf-default-name
2023-03-03 16:29:51,401:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-03-03 16:29:51,401:INFO:version 3.0.0.rc9
2023-03-03 16:29:51,401:INFO:Initializing setup()
2023-03-03 16:29:51,401:INFO:self.USI: cb37
2023-03-03 16:29:51,401:INFO:self._variable_keys: {'gpu_n_jobs_param', 'pipeline', 'memory', 'data', 'fold_generator', 'y', 'X_test', 'target_param', '_ml_usecase', 'X_train', 'fold_groups_param', 'fix_imbalance', 'y_test', 'seed', 'gpu_param', 'exp_id', 'y_train', 'fold_shuffle_param', 'log_plots_param', '_available_plots', 'n_jobs_param', 'html_param', 'logging_param', 'USI', 'idx', 'X', 'exp_name_log', 'is_multiclass'}
2023-03-03 16:29:51,401:INFO:Checking environment
2023-03-03 16:29:51,401:INFO:python_version: 3.9.13
2023-03-03 16:29:51,401:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-03-03 16:29:51,401:INFO:machine: AMD64
2023-03-03 16:29:51,402:INFO:platform: Windows-10-10.0.19044-SP0
2023-03-03 16:29:51,402:INFO:Memory: svmem(total=17009516544, available=3346968576, percent=80.3, used=13662547968, free=3346968576)
2023-03-03 16:29:51,402:INFO:Physical Core: 4
2023-03-03 16:29:51,403:INFO:Logical Core: 8
2023-03-03 16:29:51,403:INFO:Checking libraries
2023-03-03 16:29:51,403:INFO:System:
2023-03-03 16:29:51,403:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-03-03 16:29:51,403:INFO:executable: C:\Users\Alvaro\anaconda3\python.exe
2023-03-03 16:29:51,403:INFO:   machine: Windows-10-10.0.19044-SP0
2023-03-03 16:29:51,403:INFO:PyCaret required dependencies:
2023-03-03 16:29:51,403:INFO:                 pip: 22.2.2
2023-03-03 16:29:51,403:INFO:          setuptools: 63.4.1
2023-03-03 16:29:51,403:INFO:             pycaret: 3.0.0rc9
2023-03-03 16:29:51,403:INFO:             IPython: 7.31.1
2023-03-03 16:29:51,403:INFO:          ipywidgets: 7.6.5
2023-03-03 16:29:51,403:INFO:                tqdm: 4.64.1
2023-03-03 16:29:51,403:INFO:               numpy: 1.21.5
2023-03-03 16:29:51,403:INFO:              pandas: 1.4.4
2023-03-03 16:29:51,403:INFO:              jinja2: 2.11.3
2023-03-03 16:29:51,403:INFO:               scipy: 1.9.1
2023-03-03 16:29:51,403:INFO:              joblib: 1.2.0
2023-03-03 16:29:51,403:INFO:             sklearn: 1.0.2
2023-03-03 16:29:51,404:INFO:                pyod: 1.0.7
2023-03-03 16:29:51,404:INFO:            imblearn: 0.10.1
2023-03-03 16:29:51,404:INFO:   category_encoders: 2.6.0
2023-03-03 16:29:51,404:INFO:            lightgbm: 3.3.5
2023-03-03 16:29:51,404:INFO:               numba: 0.55.1
2023-03-03 16:29:51,404:INFO:            requests: 2.28.1
2023-03-03 16:29:51,404:INFO:          matplotlib: 3.5.2
2023-03-03 16:29:51,404:INFO:          scikitplot: 0.3.7
2023-03-03 16:29:51,404:INFO:         yellowbrick: 1.5
2023-03-03 16:29:51,404:INFO:              plotly: 5.9.0
2023-03-03 16:29:51,404:INFO:             kaleido: 0.2.1
2023-03-03 16:29:51,405:INFO:         statsmodels: 0.13.2
2023-03-03 16:29:51,405:INFO:              sktime: 0.16.1
2023-03-03 16:29:51,405:INFO:               tbats: 1.1.2
2023-03-03 16:29:51,405:INFO:            pmdarima: 2.0.2
2023-03-03 16:29:51,405:INFO:              psutil: 5.9.0
2023-03-03 16:29:51,406:INFO:PyCaret optional dependencies:
2023-03-03 16:29:51,406:INFO:                shap: Not installed
2023-03-03 16:29:51,406:INFO:           interpret: Not installed
2023-03-03 16:29:51,406:INFO:                umap: Not installed
2023-03-03 16:29:51,406:INFO:    pandas_profiling: Not installed
2023-03-03 16:29:51,406:INFO:  explainerdashboard: Not installed
2023-03-03 16:29:51,406:INFO:             autoviz: Not installed
2023-03-03 16:29:51,406:INFO:           fairlearn: Not installed
2023-03-03 16:29:51,406:INFO:             xgboost: Not installed
2023-03-03 16:29:51,406:INFO:            catboost: Not installed
2023-03-03 16:29:51,406:INFO:              kmodes: Not installed
2023-03-03 16:29:51,406:INFO:             mlxtend: Not installed
2023-03-03 16:29:51,406:INFO:       statsforecast: Not installed
2023-03-03 16:29:51,406:INFO:        tune_sklearn: Not installed
2023-03-03 16:29:51,406:INFO:                 ray: Not installed
2023-03-03 16:29:51,406:INFO:            hyperopt: Not installed
2023-03-03 16:29:51,407:INFO:              optuna: Not installed
2023-03-03 16:29:51,408:INFO:               skopt: Not installed
2023-03-03 16:29:51,408:INFO:              mlflow: Not installed
2023-03-03 16:29:51,408:INFO:              gradio: Not installed
2023-03-03 16:29:51,408:INFO:             fastapi: Not installed
2023-03-03 16:29:51,408:INFO:             uvicorn: Not installed
2023-03-03 16:29:51,408:INFO:              m2cgen: Not installed
2023-03-03 16:29:51,408:INFO:           evidently: Not installed
2023-03-03 16:29:51,408:INFO:               fugue: Not installed
2023-03-03 16:29:51,408:INFO:           streamlit: Not installed
2023-03-03 16:29:51,408:INFO:             prophet: Not installed
2023-03-03 16:29:51,408:INFO:None
2023-03-03 16:29:51,408:INFO:Set up data.
2023-03-03 16:29:51,426:INFO:Set up train/test split.
2023-03-03 16:35:30,037:INFO:PyCaret ClassificationExperiment
2023-03-03 16:35:30,038:INFO:Logging name: clf-default-name
2023-03-03 16:35:30,038:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-03-03 16:35:30,038:INFO:version 3.0.0.rc9
2023-03-03 16:35:30,038:INFO:Initializing setup()
2023-03-03 16:35:30,038:INFO:self.USI: a025
2023-03-03 16:35:30,038:INFO:self._variable_keys: {'gpu_n_jobs_param', 'pipeline', 'memory', 'data', 'fold_generator', 'y', 'X_test', 'target_param', '_ml_usecase', 'X_train', 'fold_groups_param', 'fix_imbalance', 'y_test', 'seed', 'gpu_param', 'exp_id', 'y_train', 'fold_shuffle_param', 'log_plots_param', '_available_plots', 'n_jobs_param', 'html_param', 'logging_param', 'USI', 'idx', 'X', 'exp_name_log', 'is_multiclass'}
2023-03-03 16:35:30,039:INFO:Checking environment
2023-03-03 16:35:30,039:INFO:python_version: 3.9.13
2023-03-03 16:35:30,039:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-03-03 16:35:30,039:INFO:machine: AMD64
2023-03-03 16:35:30,040:INFO:platform: Windows-10-10.0.19044-SP0
2023-03-03 16:35:30,040:INFO:Memory: svmem(total=17009516544, available=4446887936, percent=73.9, used=12562628608, free=4446887936)
2023-03-03 16:35:30,040:INFO:Physical Core: 4
2023-03-03 16:35:30,040:INFO:Logical Core: 8
2023-03-03 16:35:30,041:INFO:Checking libraries
2023-03-03 16:35:30,041:INFO:System:
2023-03-03 16:35:30,041:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-03-03 16:35:30,041:INFO:executable: C:\Users\Alvaro\anaconda3\python.exe
2023-03-03 16:35:30,042:INFO:   machine: Windows-10-10.0.19044-SP0
2023-03-03 16:35:30,042:INFO:PyCaret required dependencies:
2023-03-03 16:35:30,042:INFO:                 pip: 22.2.2
2023-03-03 16:35:30,042:INFO:          setuptools: 63.4.1
2023-03-03 16:35:30,042:INFO:             pycaret: 3.0.0rc9
2023-03-03 16:35:30,042:INFO:             IPython: 7.31.1
2023-03-03 16:35:30,042:INFO:          ipywidgets: 7.6.5
2023-03-03 16:35:30,043:INFO:                tqdm: 4.64.1
2023-03-03 16:35:30,043:INFO:               numpy: 1.21.5
2023-03-03 16:35:30,043:INFO:              pandas: 1.4.4
2023-03-03 16:35:30,043:INFO:              jinja2: 2.11.3
2023-03-03 16:35:30,043:INFO:               scipy: 1.9.1
2023-03-03 16:35:30,043:INFO:              joblib: 1.2.0
2023-03-03 16:35:30,043:INFO:             sklearn: 1.0.2
2023-03-03 16:35:30,044:INFO:                pyod: 1.0.7
2023-03-03 16:35:30,044:INFO:            imblearn: 0.10.1
2023-03-03 16:35:30,044:INFO:   category_encoders: 2.6.0
2023-03-03 16:35:30,044:INFO:            lightgbm: 3.3.5
2023-03-03 16:35:30,045:INFO:               numba: 0.55.1
2023-03-03 16:35:30,045:INFO:            requests: 2.28.1
2023-03-03 16:35:30,045:INFO:          matplotlib: 3.5.2
2023-03-03 16:35:30,045:INFO:          scikitplot: 0.3.7
2023-03-03 16:35:30,045:INFO:         yellowbrick: 1.5
2023-03-03 16:35:30,046:INFO:              plotly: 5.9.0
2023-03-03 16:35:30,046:INFO:             kaleido: 0.2.1
2023-03-03 16:35:30,046:INFO:         statsmodels: 0.13.2
2023-03-03 16:35:30,046:INFO:              sktime: 0.16.1
2023-03-03 16:35:30,046:INFO:               tbats: 1.1.2
2023-03-03 16:35:30,046:INFO:            pmdarima: 2.0.2
2023-03-03 16:35:30,047:INFO:              psutil: 5.9.0
2023-03-03 16:35:30,047:INFO:PyCaret optional dependencies:
2023-03-03 16:35:30,047:INFO:                shap: Not installed
2023-03-03 16:35:30,047:INFO:           interpret: Not installed
2023-03-03 16:35:30,047:INFO:                umap: Not installed
2023-03-03 16:35:30,047:INFO:    pandas_profiling: Not installed
2023-03-03 16:35:30,048:INFO:  explainerdashboard: Not installed
2023-03-03 16:35:30,048:INFO:             autoviz: Not installed
2023-03-03 16:35:30,048:INFO:           fairlearn: Not installed
2023-03-03 16:35:30,048:INFO:             xgboost: Not installed
2023-03-03 16:35:30,048:INFO:            catboost: Not installed
2023-03-03 16:35:30,048:INFO:              kmodes: Not installed
2023-03-03 16:35:30,048:INFO:             mlxtend: Not installed
2023-03-03 16:35:30,049:INFO:       statsforecast: Not installed
2023-03-03 16:35:30,049:INFO:        tune_sklearn: Not installed
2023-03-03 16:35:30,049:INFO:                 ray: Not installed
2023-03-03 16:35:30,049:INFO:            hyperopt: Not installed
2023-03-03 16:35:30,049:INFO:              optuna: Not installed
2023-03-03 16:35:30,049:INFO:               skopt: Not installed
2023-03-03 16:35:30,049:INFO:              mlflow: Not installed
2023-03-03 16:35:30,049:INFO:              gradio: Not installed
2023-03-03 16:35:30,049:INFO:             fastapi: Not installed
2023-03-03 16:35:30,050:INFO:             uvicorn: Not installed
2023-03-03 16:35:30,050:INFO:              m2cgen: Not installed
2023-03-03 16:35:30,050:INFO:           evidently: Not installed
2023-03-03 16:35:30,050:INFO:               fugue: Not installed
2023-03-03 16:35:30,050:INFO:           streamlit: Not installed
2023-03-03 16:35:30,050:INFO:             prophet: Not installed
2023-03-03 16:35:30,051:INFO:None
2023-03-03 16:35:30,051:INFO:Set up data.
2023-03-03 16:35:30,071:INFO:Set up train/test split.
2023-03-03 17:01:04,241:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-03 17:01:04,241:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-03 17:01:04,242:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-03 17:01:04,242:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-03 17:01:05,664:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-03-03 17:01:33,277:INFO:PyCaret ClassificationExperiment
2023-03-03 17:01:33,278:INFO:Logging name: clf-default-name
2023-03-03 17:01:33,278:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-03-03 17:01:33,278:INFO:version 3.0.0.rc9
2023-03-03 17:01:33,278:INFO:Initializing setup()
2023-03-03 17:01:33,278:INFO:self.USI: 0480
2023-03-03 17:01:33,278:INFO:self._variable_keys: {'logging_param', 'exp_name_log', 'exp_id', 'gpu_n_jobs_param', 'pipeline', 'html_param', 'idx', 'y', 'target_param', 'fold_groups_param', 'X_test', 'y_test', 'memory', 'data', 'log_plots_param', 'gpu_param', 'X', 'y_train', 'USI', 'seed', 'X_train', 'is_multiclass', '_available_plots', 'fold_generator', '_ml_usecase', 'fold_shuffle_param', 'fix_imbalance', 'n_jobs_param'}
2023-03-03 17:01:33,278:INFO:Checking environment
2023-03-03 17:01:33,278:INFO:python_version: 3.9.13
2023-03-03 17:01:33,278:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-03-03 17:01:33,278:INFO:machine: AMD64
2023-03-03 17:01:33,278:INFO:platform: Windows-10-10.0.19044-SP0
2023-03-03 17:01:33,278:INFO:Memory: svmem(total=17009516544, available=4332417024, percent=74.5, used=12677099520, free=4332417024)
2023-03-03 17:01:33,279:INFO:Physical Core: 4
2023-03-03 17:01:33,279:INFO:Logical Core: 8
2023-03-03 17:01:33,279:INFO:Checking libraries
2023-03-03 17:01:33,279:INFO:System:
2023-03-03 17:01:33,279:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-03-03 17:01:33,279:INFO:executable: C:\Users\Alvaro\anaconda3\python.exe
2023-03-03 17:01:33,279:INFO:   machine: Windows-10-10.0.19044-SP0
2023-03-03 17:01:33,279:INFO:PyCaret required dependencies:
2023-03-03 17:01:33,279:INFO:                 pip: 22.2.2
2023-03-03 17:01:33,279:INFO:          setuptools: 63.4.1
2023-03-03 17:01:33,279:INFO:             pycaret: 3.0.0rc9
2023-03-03 17:01:33,279:INFO:             IPython: 7.31.1
2023-03-03 17:01:33,279:INFO:          ipywidgets: 7.6.5
2023-03-03 17:01:33,279:INFO:                tqdm: 4.64.1
2023-03-03 17:01:33,279:INFO:               numpy: 1.21.5
2023-03-03 17:01:33,279:INFO:              pandas: 1.4.4
2023-03-03 17:01:33,279:INFO:              jinja2: 2.11.3
2023-03-03 17:01:33,279:INFO:               scipy: 1.9.1
2023-03-03 17:01:33,279:INFO:              joblib: 1.2.0
2023-03-03 17:01:33,279:INFO:             sklearn: 1.0.2
2023-03-03 17:01:33,280:INFO:                pyod: 1.0.7
2023-03-03 17:01:33,280:INFO:            imblearn: 0.10.1
2023-03-03 17:01:33,280:INFO:   category_encoders: 2.6.0
2023-03-03 17:01:33,280:INFO:            lightgbm: 3.3.5
2023-03-03 17:01:33,280:INFO:               numba: 0.55.1
2023-03-03 17:01:33,280:INFO:            requests: 2.28.1
2023-03-03 17:01:33,280:INFO:          matplotlib: 3.5.2
2023-03-03 17:01:33,280:INFO:          scikitplot: 0.3.7
2023-03-03 17:01:33,280:INFO:         yellowbrick: 1.5
2023-03-03 17:01:33,280:INFO:              plotly: 5.9.0
2023-03-03 17:01:33,280:INFO:             kaleido: 0.2.1
2023-03-03 17:01:33,280:INFO:         statsmodels: 0.13.2
2023-03-03 17:01:33,280:INFO:              sktime: 0.16.1
2023-03-03 17:01:33,280:INFO:               tbats: 1.1.2
2023-03-03 17:01:33,280:INFO:            pmdarima: 2.0.2
2023-03-03 17:01:33,280:INFO:              psutil: 5.9.0
2023-03-03 17:01:33,280:INFO:PyCaret optional dependencies:
2023-03-03 17:01:33,301:INFO:                shap: Not installed
2023-03-03 17:01:33,302:INFO:           interpret: Not installed
2023-03-03 17:01:33,302:INFO:                umap: Not installed
2023-03-03 17:01:33,302:INFO:    pandas_profiling: Not installed
2023-03-03 17:01:33,302:INFO:  explainerdashboard: Not installed
2023-03-03 17:01:33,302:INFO:             autoviz: Not installed
2023-03-03 17:01:33,302:INFO:           fairlearn: Not installed
2023-03-03 17:01:33,302:INFO:             xgboost: 1.7.4
2023-03-03 17:01:33,302:INFO:            catboost: Not installed
2023-03-03 17:01:33,302:INFO:              kmodes: Not installed
2023-03-03 17:01:33,302:INFO:             mlxtend: Not installed
2023-03-03 17:01:33,302:INFO:       statsforecast: Not installed
2023-03-03 17:01:33,302:INFO:        tune_sklearn: Not installed
2023-03-03 17:01:33,302:INFO:                 ray: Not installed
2023-03-03 17:01:33,302:INFO:            hyperopt: Not installed
2023-03-03 17:01:33,302:INFO:              optuna: Not installed
2023-03-03 17:01:33,302:INFO:               skopt: Not installed
2023-03-03 17:01:33,302:INFO:              mlflow: Not installed
2023-03-03 17:01:33,302:INFO:              gradio: Not installed
2023-03-03 17:01:33,302:INFO:             fastapi: Not installed
2023-03-03 17:01:33,302:INFO:             uvicorn: Not installed
2023-03-03 17:01:33,303:INFO:              m2cgen: Not installed
2023-03-03 17:01:33,303:INFO:           evidently: Not installed
2023-03-03 17:01:33,303:INFO:               fugue: Not installed
2023-03-03 17:01:33,303:INFO:           streamlit: Not installed
2023-03-03 17:01:33,303:INFO:             prophet: Not installed
2023-03-03 17:01:33,303:INFO:None
2023-03-03 17:01:33,303:INFO:Set up data.
2023-03-03 17:01:33,311:INFO:Set up train/test split.
2023-03-03 22:06:22,524:INFO:PyCaret ClassificationExperiment
2023-03-03 22:06:22,528:INFO:Logging name: clf-default-name
2023-03-03 22:06:22,528:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-03-03 22:06:22,529:INFO:version 3.0.0.rc9
2023-03-03 22:06:22,529:INFO:Initializing setup()
2023-03-03 22:06:22,529:INFO:self.USI: b156
2023-03-03 22:06:22,530:INFO:self._variable_keys: {'logging_param', 'exp_name_log', 'exp_id', 'gpu_n_jobs_param', 'pipeline', 'html_param', 'idx', 'y', 'target_param', 'fold_groups_param', 'X_test', 'y_test', 'memory', 'data', 'log_plots_param', 'gpu_param', 'X', 'y_train', 'USI', 'seed', 'X_train', 'is_multiclass', '_available_plots', 'fold_generator', '_ml_usecase', 'fold_shuffle_param', 'fix_imbalance', 'n_jobs_param'}
2023-03-03 22:06:22,531:INFO:Checking environment
2023-03-03 22:06:22,531:INFO:python_version: 3.9.13
2023-03-03 22:06:22,531:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-03-03 22:06:22,532:INFO:machine: AMD64
2023-03-03 22:06:22,532:INFO:platform: Windows-10-10.0.19044-SP0
2023-03-03 22:06:22,534:INFO:Memory: svmem(total=17009516544, available=5290860544, percent=68.9, used=11718656000, free=5290860544)
2023-03-03 22:06:22,534:INFO:Physical Core: 4
2023-03-03 22:06:22,534:INFO:Logical Core: 8
2023-03-03 22:06:22,534:INFO:Checking libraries
2023-03-03 22:06:22,535:INFO:System:
2023-03-03 22:06:22,535:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-03-03 22:06:22,536:INFO:executable: C:\Users\Alvaro\anaconda3\python.exe
2023-03-03 22:06:22,536:INFO:   machine: Windows-10-10.0.19044-SP0
2023-03-03 22:06:22,536:INFO:PyCaret required dependencies:
2023-03-03 22:06:22,540:INFO:                 pip: 22.2.2
2023-03-03 22:06:22,540:INFO:          setuptools: 63.4.1
2023-03-03 22:06:22,540:INFO:             pycaret: 3.0.0rc9
2023-03-03 22:06:22,540:INFO:             IPython: 7.31.1
2023-03-03 22:06:22,540:INFO:          ipywidgets: 7.6.5
2023-03-03 22:06:22,541:INFO:                tqdm: 4.64.1
2023-03-03 22:06:22,541:INFO:               numpy: 1.21.5
2023-03-03 22:06:22,541:INFO:              pandas: 1.4.4
2023-03-03 22:06:22,541:INFO:              jinja2: 2.11.3
2023-03-03 22:06:22,541:INFO:               scipy: 1.9.1
2023-03-03 22:06:22,541:INFO:              joblib: 1.2.0
2023-03-03 22:06:22,541:INFO:             sklearn: 1.0.2
2023-03-03 22:06:22,541:INFO:                pyod: 1.0.7
2023-03-03 22:06:22,541:INFO:            imblearn: 0.10.1
2023-03-03 22:06:22,542:INFO:   category_encoders: 2.6.0
2023-03-03 22:06:22,542:INFO:            lightgbm: 3.3.5
2023-03-03 22:06:22,542:INFO:               numba: 0.55.1
2023-03-03 22:06:22,542:INFO:            requests: 2.28.1
2023-03-03 22:06:22,542:INFO:          matplotlib: 3.5.2
2023-03-03 22:06:22,543:INFO:          scikitplot: 0.3.7
2023-03-03 22:06:22,543:INFO:         yellowbrick: 1.5
2023-03-03 22:06:22,543:INFO:              plotly: 5.9.0
2023-03-03 22:06:22,543:INFO:             kaleido: 0.2.1
2023-03-03 22:06:22,543:INFO:         statsmodels: 0.13.2
2023-03-03 22:06:22,544:INFO:              sktime: 0.16.1
2023-03-03 22:06:22,544:INFO:               tbats: 1.1.2
2023-03-03 22:06:22,544:INFO:            pmdarima: 2.0.2
2023-03-03 22:06:22,544:INFO:              psutil: 5.9.0
2023-03-03 22:06:22,544:INFO:PyCaret optional dependencies:
2023-03-03 22:06:22,545:INFO:                shap: Not installed
2023-03-03 22:06:22,545:INFO:           interpret: Not installed
2023-03-03 22:06:22,545:INFO:                umap: Not installed
2023-03-03 22:06:22,545:INFO:    pandas_profiling: Not installed
2023-03-03 22:06:22,545:INFO:  explainerdashboard: Not installed
2023-03-03 22:06:22,546:INFO:             autoviz: Not installed
2023-03-03 22:06:22,546:INFO:           fairlearn: Not installed
2023-03-03 22:06:22,546:INFO:             xgboost: 1.7.4
2023-03-03 22:06:22,546:INFO:            catboost: Not installed
2023-03-03 22:06:22,546:INFO:              kmodes: Not installed
2023-03-03 22:06:22,546:INFO:             mlxtend: Not installed
2023-03-03 22:06:22,546:INFO:       statsforecast: Not installed
2023-03-03 22:06:22,547:INFO:        tune_sklearn: Not installed
2023-03-03 22:06:22,547:INFO:                 ray: Not installed
2023-03-03 22:06:22,547:INFO:            hyperopt: Not installed
2023-03-03 22:06:22,547:INFO:              optuna: Not installed
2023-03-03 22:06:22,547:INFO:               skopt: Not installed
2023-03-03 22:06:22,547:INFO:              mlflow: Not installed
2023-03-03 22:06:22,547:INFO:              gradio: Not installed
2023-03-03 22:06:22,547:INFO:             fastapi: Not installed
2023-03-03 22:06:22,548:INFO:             uvicorn: Not installed
2023-03-03 22:06:22,548:INFO:              m2cgen: Not installed
2023-03-03 22:06:22,548:INFO:           evidently: Not installed
2023-03-03 22:06:22,548:INFO:               fugue: Not installed
2023-03-03 22:06:22,548:INFO:           streamlit: Not installed
2023-03-03 22:06:22,548:INFO:             prophet: Not installed
2023-03-03 22:06:22,548:INFO:None
2023-03-03 22:06:22,549:INFO:Set up data.
2023-03-03 22:06:22,643:INFO:Set up train/test split.
2023-03-03 22:06:55,215:INFO:PyCaret ClassificationExperiment
2023-03-03 22:06:55,216:INFO:Logging name: clf-default-name
2023-03-03 22:06:55,216:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-03-03 22:06:55,216:INFO:version 3.0.0.rc9
2023-03-03 22:06:55,216:INFO:Initializing setup()
2023-03-03 22:06:55,217:INFO:self.USI: 6933
2023-03-03 22:06:55,217:INFO:self._variable_keys: {'logging_param', 'exp_name_log', 'exp_id', 'gpu_n_jobs_param', 'pipeline', 'html_param', 'idx', 'y', 'target_param', 'fold_groups_param', 'X_test', 'y_test', 'memory', 'data', 'log_plots_param', 'gpu_param', 'X', 'y_train', 'USI', 'seed', 'X_train', 'is_multiclass', '_available_plots', 'fold_generator', '_ml_usecase', 'fold_shuffle_param', 'fix_imbalance', 'n_jobs_param'}
2023-03-03 22:06:55,217:INFO:Checking environment
2023-03-03 22:06:55,217:INFO:python_version: 3.9.13
2023-03-03 22:06:55,217:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-03-03 22:06:55,218:INFO:machine: AMD64
2023-03-03 22:06:55,218:INFO:platform: Windows-10-10.0.19044-SP0
2023-03-03 22:06:55,218:INFO:Memory: svmem(total=17009516544, available=4864712704, percent=71.4, used=12144803840, free=4864712704)
2023-03-03 22:06:55,218:INFO:Physical Core: 4
2023-03-03 22:06:55,218:INFO:Logical Core: 8
2023-03-03 22:06:55,218:INFO:Checking libraries
2023-03-03 22:06:55,219:INFO:System:
2023-03-03 22:06:55,219:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-03-03 22:06:55,219:INFO:executable: C:\Users\Alvaro\anaconda3\python.exe
2023-03-03 22:06:55,219:INFO:   machine: Windows-10-10.0.19044-SP0
2023-03-03 22:06:55,219:INFO:PyCaret required dependencies:
2023-03-03 22:06:55,220:INFO:                 pip: 22.2.2
2023-03-03 22:06:55,220:INFO:          setuptools: 63.4.1
2023-03-03 22:06:55,220:INFO:             pycaret: 3.0.0rc9
2023-03-03 22:06:55,220:INFO:             IPython: 7.31.1
2023-03-03 22:06:55,220:INFO:          ipywidgets: 7.6.5
2023-03-03 22:06:55,220:INFO:                tqdm: 4.64.1
2023-03-03 22:06:55,220:INFO:               numpy: 1.21.5
2023-03-03 22:06:55,221:INFO:              pandas: 1.4.4
2023-03-03 22:06:55,221:INFO:              jinja2: 2.11.3
2023-03-03 22:06:55,221:INFO:               scipy: 1.9.1
2023-03-03 22:06:55,221:INFO:              joblib: 1.2.0
2023-03-03 22:06:55,221:INFO:             sklearn: 1.0.2
2023-03-03 22:06:55,221:INFO:                pyod: 1.0.7
2023-03-03 22:06:55,221:INFO:            imblearn: 0.10.1
2023-03-03 22:06:55,222:INFO:   category_encoders: 2.6.0
2023-03-03 22:06:55,222:INFO:            lightgbm: 3.3.5
2023-03-03 22:06:55,222:INFO:               numba: 0.55.1
2023-03-03 22:06:55,223:INFO:            requests: 2.28.1
2023-03-03 22:06:55,223:INFO:          matplotlib: 3.5.2
2023-03-03 22:06:55,223:INFO:          scikitplot: 0.3.7
2023-03-03 22:06:55,223:INFO:         yellowbrick: 1.5
2023-03-03 22:06:55,223:INFO:              plotly: 5.9.0
2023-03-03 22:06:55,223:INFO:             kaleido: 0.2.1
2023-03-03 22:06:55,223:INFO:         statsmodels: 0.13.2
2023-03-03 22:06:55,223:INFO:              sktime: 0.16.1
2023-03-03 22:06:55,223:INFO:               tbats: 1.1.2
2023-03-03 22:06:55,223:INFO:            pmdarima: 2.0.2
2023-03-03 22:06:55,223:INFO:              psutil: 5.9.0
2023-03-03 22:06:55,223:INFO:PyCaret optional dependencies:
2023-03-03 22:06:55,224:INFO:                shap: Not installed
2023-03-03 22:06:55,224:INFO:           interpret: Not installed
2023-03-03 22:06:55,224:INFO:                umap: Not installed
2023-03-03 22:06:55,224:INFO:    pandas_profiling: Not installed
2023-03-03 22:06:55,224:INFO:  explainerdashboard: Not installed
2023-03-03 22:06:55,224:INFO:             autoviz: Not installed
2023-03-03 22:06:55,225:INFO:           fairlearn: Not installed
2023-03-03 22:06:55,225:INFO:             xgboost: 1.7.4
2023-03-03 22:06:55,225:INFO:            catboost: Not installed
2023-03-03 22:06:55,225:INFO:              kmodes: Not installed
2023-03-03 22:06:55,226:INFO:             mlxtend: Not installed
2023-03-03 22:06:55,226:INFO:       statsforecast: Not installed
2023-03-03 22:06:55,226:INFO:        tune_sklearn: Not installed
2023-03-03 22:06:55,226:INFO:                 ray: Not installed
2023-03-03 22:06:55,226:INFO:            hyperopt: Not installed
2023-03-03 22:06:55,226:INFO:              optuna: Not installed
2023-03-03 22:06:55,226:INFO:               skopt: Not installed
2023-03-03 22:06:55,226:INFO:              mlflow: Not installed
2023-03-03 22:06:55,226:INFO:              gradio: Not installed
2023-03-03 22:06:55,227:INFO:             fastapi: Not installed
2023-03-03 22:06:55,227:INFO:             uvicorn: Not installed
2023-03-03 22:06:55,227:INFO:              m2cgen: Not installed
2023-03-03 22:06:55,227:INFO:           evidently: Not installed
2023-03-03 22:06:55,227:INFO:               fugue: Not installed
2023-03-03 22:06:55,227:INFO:           streamlit: Not installed
2023-03-03 22:06:55,227:INFO:             prophet: Not installed
2023-03-03 22:06:55,227:INFO:None
2023-03-03 22:06:55,228:INFO:Set up data.
2023-03-03 22:06:55,248:INFO:Set up train/test split.
2023-03-04 16:58:50,558:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-04 16:58:50,559:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-04 16:58:50,559:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-04 16:58:50,559:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-04 16:58:52,426:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-03-04 16:58:52,827:INFO:PyCaret RegressionExperiment
2023-03-04 16:58:52,828:INFO:Logging name: reg-default-name
2023-03-04 16:58:52,828:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-04 16:58:52,828:INFO:version 3.0.0.rc9
2023-03-04 16:58:52,828:INFO:Initializing setup()
2023-03-04 16:58:52,828:INFO:self.USI: e2de
2023-03-04 16:58:52,828:INFO:self._variable_keys: {'log_plots_param', 'pipeline', 'fold_generator', 'target_param', 'X_test', 'y_train', 'idx', 'data', 'seed', 'USI', 'n_jobs_param', 'gpu_n_jobs_param', 'exp_id', '_ml_usecase', 'gpu_param', '_available_plots', 'y_test', 'fold_groups_param', 'logging_param', 'y', 'html_param', 'fold_shuffle_param', 'transform_target_param', 'exp_name_log', 'X_train', 'memory', 'X'}
2023-03-04 16:58:52,828:INFO:Checking environment
2023-03-04 16:58:52,828:INFO:python_version: 3.9.13
2023-03-04 16:58:52,828:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-03-04 16:58:52,828:INFO:machine: AMD64
2023-03-04 16:58:52,828:INFO:platform: Windows-10-10.0.19044-SP0
2023-03-04 16:58:52,828:INFO:Memory: svmem(total=17009516544, available=3945406464, percent=76.8, used=13064110080, free=3945406464)
2023-03-04 16:58:52,828:INFO:Physical Core: 4
2023-03-04 16:58:52,829:INFO:Logical Core: 8
2023-03-04 16:58:52,829:INFO:Checking libraries
2023-03-04 16:58:52,829:INFO:System:
2023-03-04 16:58:52,829:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-03-04 16:58:52,829:INFO:executable: C:\Users\Alvaro\anaconda3\python.exe
2023-03-04 16:58:52,829:INFO:   machine: Windows-10-10.0.19044-SP0
2023-03-04 16:58:52,829:INFO:PyCaret required dependencies:
2023-03-04 16:58:52,829:INFO:                 pip: 22.2.2
2023-03-04 16:58:52,829:INFO:          setuptools: 63.4.1
2023-03-04 16:58:52,829:INFO:             pycaret: 3.0.0rc9
2023-03-04 16:58:52,829:INFO:             IPython: 7.31.1
2023-03-04 16:58:52,829:INFO:          ipywidgets: 7.6.5
2023-03-04 16:58:52,829:INFO:                tqdm: 4.64.1
2023-03-04 16:58:52,829:INFO:               numpy: 1.21.5
2023-03-04 16:58:52,829:INFO:              pandas: 1.4.4
2023-03-04 16:58:52,829:INFO:              jinja2: 2.11.3
2023-03-04 16:58:52,830:INFO:               scipy: 1.9.1
2023-03-04 16:58:52,830:INFO:              joblib: 1.2.0
2023-03-04 16:58:52,830:INFO:             sklearn: 1.0.2
2023-03-04 16:58:52,830:INFO:                pyod: 1.0.7
2023-03-04 16:58:52,830:INFO:            imblearn: 0.10.1
2023-03-04 16:58:52,830:INFO:   category_encoders: 2.6.0
2023-03-04 16:58:52,830:INFO:            lightgbm: 3.3.5
2023-03-04 16:58:52,830:INFO:               numba: 0.55.1
2023-03-04 16:58:52,830:INFO:            requests: 2.28.1
2023-03-04 16:58:52,830:INFO:          matplotlib: 3.5.2
2023-03-04 16:58:52,830:INFO:          scikitplot: 0.3.7
2023-03-04 16:58:52,830:INFO:         yellowbrick: 1.5
2023-03-04 16:58:52,830:INFO:              plotly: 5.9.0
2023-03-04 16:58:52,830:INFO:             kaleido: 0.2.1
2023-03-04 16:58:52,830:INFO:         statsmodels: 0.13.2
2023-03-04 16:58:52,830:INFO:              sktime: 0.16.1
2023-03-04 16:58:52,830:INFO:               tbats: 1.1.2
2023-03-04 16:58:52,830:INFO:            pmdarima: 2.0.2
2023-03-04 16:58:52,830:INFO:              psutil: 5.9.0
2023-03-04 16:58:52,831:INFO:PyCaret optional dependencies:
2023-03-04 16:58:52,855:INFO:                shap: Not installed
2023-03-04 16:58:52,855:INFO:           interpret: Not installed
2023-03-04 16:58:52,855:INFO:                umap: Not installed
2023-03-04 16:58:52,855:INFO:    pandas_profiling: Not installed
2023-03-04 16:58:52,856:INFO:  explainerdashboard: Not installed
2023-03-04 16:58:52,856:INFO:             autoviz: Not installed
2023-03-04 16:58:52,856:INFO:           fairlearn: Not installed
2023-03-04 16:58:52,856:INFO:             xgboost: 1.7.4
2023-03-04 16:58:52,856:INFO:            catboost: Not installed
2023-03-04 16:58:52,856:INFO:              kmodes: Not installed
2023-03-04 16:58:52,856:INFO:             mlxtend: Not installed
2023-03-04 16:58:52,856:INFO:       statsforecast: Not installed
2023-03-04 16:58:52,856:INFO:        tune_sklearn: Not installed
2023-03-04 16:58:52,856:INFO:                 ray: Not installed
2023-03-04 16:58:52,856:INFO:            hyperopt: Not installed
2023-03-04 16:58:52,856:INFO:              optuna: Not installed
2023-03-04 16:58:52,856:INFO:               skopt: Not installed
2023-03-04 16:58:52,856:INFO:              mlflow: Not installed
2023-03-04 16:58:52,856:INFO:              gradio: Not installed
2023-03-04 16:58:52,856:INFO:             fastapi: Not installed
2023-03-04 16:58:52,856:INFO:             uvicorn: Not installed
2023-03-04 16:58:52,856:INFO:              m2cgen: Not installed
2023-03-04 16:58:52,857:INFO:           evidently: Not installed
2023-03-04 16:58:52,857:INFO:               fugue: Not installed
2023-03-04 16:58:52,857:INFO:           streamlit: Not installed
2023-03-04 16:58:52,857:INFO:             prophet: Not installed
2023-03-04 16:58:52,857:INFO:None
2023-03-04 16:58:52,857:INFO:Set up data.
2023-03-04 17:00:02,946:INFO:PyCaret RegressionExperiment
2023-03-04 17:00:02,946:INFO:Logging name: reg-default-name
2023-03-04 17:00:02,946:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-04 17:00:02,946:INFO:version 3.0.0.rc9
2023-03-04 17:00:02,946:INFO:Initializing setup()
2023-03-04 17:00:02,946:INFO:self.USI: 7e8c
2023-03-04 17:00:02,946:INFO:self._variable_keys: {'log_plots_param', 'pipeline', 'fold_generator', 'target_param', 'X_test', 'y_train', 'idx', 'data', 'seed', 'USI', 'n_jobs_param', 'gpu_n_jobs_param', 'exp_id', '_ml_usecase', 'gpu_param', '_available_plots', 'y_test', 'fold_groups_param', 'logging_param', 'y', 'html_param', 'fold_shuffle_param', 'transform_target_param', 'exp_name_log', 'X_train', 'memory', 'X'}
2023-03-04 17:00:02,947:INFO:Checking environment
2023-03-04 17:00:02,947:INFO:python_version: 3.9.13
2023-03-04 17:00:02,947:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-03-04 17:00:02,947:INFO:machine: AMD64
2023-03-04 17:00:02,947:INFO:platform: Windows-10-10.0.19044-SP0
2023-03-04 17:00:02,947:INFO:Memory: svmem(total=17009516544, available=3673432064, percent=78.4, used=13336084480, free=3673432064)
2023-03-04 17:00:02,947:INFO:Physical Core: 4
2023-03-04 17:00:02,947:INFO:Logical Core: 8
2023-03-04 17:00:02,947:INFO:Checking libraries
2023-03-04 17:00:02,947:INFO:System:
2023-03-04 17:00:02,947:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-03-04 17:00:02,947:INFO:executable: C:\Users\Alvaro\anaconda3\python.exe
2023-03-04 17:00:02,947:INFO:   machine: Windows-10-10.0.19044-SP0
2023-03-04 17:00:02,947:INFO:PyCaret required dependencies:
2023-03-04 17:00:02,947:INFO:                 pip: 22.2.2
2023-03-04 17:00:02,948:INFO:          setuptools: 63.4.1
2023-03-04 17:00:02,948:INFO:             pycaret: 3.0.0rc9
2023-03-04 17:00:02,948:INFO:             IPython: 7.31.1
2023-03-04 17:00:02,948:INFO:          ipywidgets: 7.6.5
2023-03-04 17:00:02,948:INFO:                tqdm: 4.64.1
2023-03-04 17:00:02,948:INFO:               numpy: 1.21.5
2023-03-04 17:00:02,948:INFO:              pandas: 1.4.4
2023-03-04 17:00:02,948:INFO:              jinja2: 2.11.3
2023-03-04 17:00:02,948:INFO:               scipy: 1.9.1
2023-03-04 17:00:02,948:INFO:              joblib: 1.2.0
2023-03-04 17:00:02,948:INFO:             sklearn: 1.0.2
2023-03-04 17:00:02,948:INFO:                pyod: 1.0.7
2023-03-04 17:00:02,948:INFO:            imblearn: 0.10.1
2023-03-04 17:00:02,948:INFO:   category_encoders: 2.6.0
2023-03-04 17:00:02,948:INFO:            lightgbm: 3.3.5
2023-03-04 17:00:02,948:INFO:               numba: 0.55.1
2023-03-04 17:00:02,948:INFO:            requests: 2.28.1
2023-03-04 17:00:02,948:INFO:          matplotlib: 3.5.2
2023-03-04 17:00:02,948:INFO:          scikitplot: 0.3.7
2023-03-04 17:00:02,949:INFO:         yellowbrick: 1.5
2023-03-04 17:00:02,949:INFO:              plotly: 5.9.0
2023-03-04 17:00:02,949:INFO:             kaleido: 0.2.1
2023-03-04 17:00:02,949:INFO:         statsmodels: 0.13.2
2023-03-04 17:00:02,949:INFO:              sktime: 0.16.1
2023-03-04 17:00:02,949:INFO:               tbats: 1.1.2
2023-03-04 17:00:02,949:INFO:            pmdarima: 2.0.2
2023-03-04 17:00:02,949:INFO:              psutil: 5.9.0
2023-03-04 17:00:02,949:INFO:PyCaret optional dependencies:
2023-03-04 17:00:02,949:INFO:                shap: Not installed
2023-03-04 17:00:02,949:INFO:           interpret: Not installed
2023-03-04 17:00:02,949:INFO:                umap: Not installed
2023-03-04 17:00:02,950:INFO:    pandas_profiling: Not installed
2023-03-04 17:00:02,950:INFO:  explainerdashboard: Not installed
2023-03-04 17:00:02,950:INFO:             autoviz: Not installed
2023-03-04 17:00:02,950:INFO:           fairlearn: Not installed
2023-03-04 17:00:02,950:INFO:             xgboost: 1.7.4
2023-03-04 17:00:02,950:INFO:            catboost: Not installed
2023-03-04 17:00:02,950:INFO:              kmodes: Not installed
2023-03-04 17:00:02,950:INFO:             mlxtend: Not installed
2023-03-04 17:00:02,950:INFO:       statsforecast: Not installed
2023-03-04 17:00:02,950:INFO:        tune_sklearn: Not installed
2023-03-04 17:00:02,950:INFO:                 ray: Not installed
2023-03-04 17:00:02,951:INFO:            hyperopt: Not installed
2023-03-04 17:00:02,951:INFO:              optuna: Not installed
2023-03-04 17:00:02,951:INFO:               skopt: Not installed
2023-03-04 17:00:02,951:INFO:              mlflow: Not installed
2023-03-04 17:00:02,951:INFO:              gradio: Not installed
2023-03-04 17:00:02,951:INFO:             fastapi: Not installed
2023-03-04 17:00:02,951:INFO:             uvicorn: Not installed
2023-03-04 17:00:02,951:INFO:              m2cgen: Not installed
2023-03-04 17:00:02,951:INFO:           evidently: Not installed
2023-03-04 17:00:02,951:INFO:               fugue: Not installed
2023-03-04 17:00:02,952:INFO:           streamlit: Not installed
2023-03-04 17:00:02,952:INFO:             prophet: Not installed
2023-03-04 17:00:02,952:INFO:None
2023-03-04 17:00:02,952:INFO:Set up data.
2023-03-04 17:00:03,001:INFO:Set up train/test split.
2023-03-04 17:00:03,010:INFO:Set up index.
2023-03-04 17:00:03,012:INFO:Set up folding strategy.
2023-03-04 17:00:03,012:INFO:Assigning column types.
2023-03-04 17:00:03,017:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-04 17:00:03,017:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-04 17:00:03,023:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-04 17:00:03,029:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-04 17:00:03,099:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:00:03,154:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-04 17:00:03,155:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:00:03,158:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:00:03,158:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-04 17:00:03,164:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-04 17:00:03,169:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-04 17:00:03,235:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:00:03,287:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-04 17:00:03,288:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:00:03,291:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:00:03,291:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-03-04 17:00:03,297:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-04 17:00:03,302:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-04 17:00:03,370:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:00:03,422:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-04 17:00:03,423:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:00:03,426:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:00:03,432:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-04 17:00:03,437:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-04 17:00:03,504:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:00:03,556:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-04 17:00:03,557:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:00:03,560:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:00:03,560:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-03-04 17:00:03,571:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-04 17:00:03,643:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:00:03,694:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-04 17:00:03,695:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:00:03,698:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:00:03,709:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-04 17:00:03,777:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:00:03,828:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-04 17:00:03,829:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:00:03,832:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:00:03,832:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-03-04 17:00:03,912:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:00:03,967:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-04 17:00:03,968:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:00:03,971:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:00:04,057:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:00:04,121:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-04 17:00:04,122:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:00:04,126:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:00:04,126:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-04 17:00:04,209:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:00:04,267:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:00:04,270:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:00:04,357:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:00:04,419:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:00:04,422:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:00:04,422:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-03-04 17:00:04,563:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:00:04,567:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:00:04,699:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:00:04,702:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:00:04,704:INFO:Preparing preprocessing pipeline...
2023-03-04 17:00:04,705:INFO:Set up column name cleaning.
2023-03-04 17:00:04,705:INFO:Set up simple imputation.
2023-03-04 17:00:04,749:INFO:Finished creating preprocessing pipeline.
2023-03-04 17:00:04,756:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Alvaro\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('numerical_imputer',
                 TransformerWrapper(include=['work_year', 'remote_ratio',
                                             'experience_level_EX',
                                             'experience_level_MI',
                                             'experience_level_SE',
                                             'employment_type_FL',
                                             'employment_type_FT',
                                             'emplo...
                                             'job_title_Data Architect',
                                             'job_title_Data Engineer',
                                             'job_title_Data Engineering '
                                             'Manager',
                                             'job_title_Data Science '
                                             'Consultant',
                                             'job_title_Data Science Engineer',
                                             'job_title_Data Science Manager',
                                             'job_title_Data Scientist', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent')))])
2023-03-04 17:00:04,756:INFO:Creating final display dataframe.
2023-03-04 17:00:04,997:INFO:Setup _display_container:                     Description             Value
0                    Session id              6865
1                        Target     salary_in_usd
2                   Target type        Regression
3           Original data shape        (499, 165)
4        Transformed data shape        (499, 165)
5   Transformed train set shape        (349, 165)
6    Transformed test set shape        (150, 165)
7              Numeric features               164
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator             KFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  reg-default-name
18                          USI              7e8c
2023-03-04 17:00:05,154:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:00:05,157:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:00:05,295:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:00:05,298:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:00:05,299:INFO:setup() successfully completed in 2.36s...............
2023-03-04 17:00:25,224:INFO:Initializing compare_models()
2023-03-04 17:00:25,225:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC6DA9730>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000011BC6DA9730>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-03-04 17:00:25,225:INFO:Checking exceptions
2023-03-04 17:00:25,229:INFO:Preparing display monitor
2023-03-04 17:00:25,279:INFO:Initializing Linear Regression
2023-03-04 17:00:25,280:INFO:Total runtime is 1.6729036966959637e-05 minutes
2023-03-04 17:00:25,286:INFO:SubProcess create_model() called ==================================
2023-03-04 17:00:25,287:INFO:Initializing create_model()
2023-03-04 17:00:25,287:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC6DA9730>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BC65C9820>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:00:25,287:INFO:Checking exceptions
2023-03-04 17:00:25,288:INFO:Importing libraries
2023-03-04 17:00:25,288:INFO:Copying training dataset
2023-03-04 17:00:25,297:INFO:Defining folds
2023-03-04 17:00:25,298:INFO:Declaring metric variables
2023-03-04 17:00:25,301:INFO:Importing untrained model
2023-03-04 17:00:25,304:INFO:Linear Regression Imported successfully
2023-03-04 17:00:25,318:INFO:Starting cross validation
2023-03-04 17:00:25,321:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:00:38,401:INFO:Calculating mean and std
2023-03-04 17:00:38,404:INFO:Creating metrics dataframe
2023-03-04 17:00:38,408:INFO:Uploading results into container
2023-03-04 17:00:38,409:INFO:Uploading model into container now
2023-03-04 17:00:38,410:INFO:_master_model_container: 1
2023-03-04 17:00:38,410:INFO:_display_container: 2
2023-03-04 17:00:38,411:INFO:LinearRegression(n_jobs=-1)
2023-03-04 17:00:38,411:INFO:create_model() successfully completed......................................
2023-03-04 17:00:38,547:INFO:SubProcess create_model() end ==================================
2023-03-04 17:00:38,547:INFO:Creating metrics dataframe
2023-03-04 17:00:38,558:INFO:Initializing Lasso Regression
2023-03-04 17:00:38,558:INFO:Total runtime is 0.22130781014760334 minutes
2023-03-04 17:00:38,562:INFO:SubProcess create_model() called ==================================
2023-03-04 17:00:38,563:INFO:Initializing create_model()
2023-03-04 17:00:38,563:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC6DA9730>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BC65C9820>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:00:38,563:INFO:Checking exceptions
2023-03-04 17:00:38,563:INFO:Importing libraries
2023-03-04 17:00:38,563:INFO:Copying training dataset
2023-03-04 17:00:38,573:INFO:Defining folds
2023-03-04 17:00:38,574:INFO:Declaring metric variables
2023-03-04 17:00:38,580:INFO:Importing untrained model
2023-03-04 17:00:38,587:INFO:Lasso Regression Imported successfully
2023-03-04 17:00:38,602:INFO:Starting cross validation
2023-03-04 17:00:38,605:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:00:38,793:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.118e+10, tolerance: 1.627e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-04 17:00:38,842:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.977e+09, tolerance: 1.571e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-04 17:00:38,875:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.677e+10, tolerance: 1.616e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-04 17:00:38,893:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.409e+10, tolerance: 1.538e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-04 17:00:38,899:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.294e+10, tolerance: 1.572e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-04 17:00:38,912:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.013e+10, tolerance: 1.577e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-04 17:00:38,924:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.005e+10, tolerance: 1.423e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-04 17:00:38,955:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.577e+10, tolerance: 1.521e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-04 17:00:39,075:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.795e+10, tolerance: 1.481e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-04 17:00:39,109:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.066e+10, tolerance: 1.528e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-04 17:00:39,274:INFO:Calculating mean and std
2023-03-04 17:00:39,276:INFO:Creating metrics dataframe
2023-03-04 17:00:39,281:INFO:Uploading results into container
2023-03-04 17:00:39,282:INFO:Uploading model into container now
2023-03-04 17:00:39,282:INFO:_master_model_container: 2
2023-03-04 17:00:39,283:INFO:_display_container: 2
2023-03-04 17:00:39,283:INFO:Lasso(random_state=6865)
2023-03-04 17:00:39,283:INFO:create_model() successfully completed......................................
2023-03-04 17:00:39,418:INFO:SubProcess create_model() end ==================================
2023-03-04 17:00:39,418:INFO:Creating metrics dataframe
2023-03-04 17:00:39,429:INFO:Initializing Ridge Regression
2023-03-04 17:00:39,429:INFO:Total runtime is 0.23583558400472004 minutes
2023-03-04 17:00:39,433:INFO:SubProcess create_model() called ==================================
2023-03-04 17:00:39,434:INFO:Initializing create_model()
2023-03-04 17:00:39,434:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC6DA9730>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BC65C9820>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:00:39,434:INFO:Checking exceptions
2023-03-04 17:00:39,434:INFO:Importing libraries
2023-03-04 17:00:39,435:INFO:Copying training dataset
2023-03-04 17:00:39,445:INFO:Defining folds
2023-03-04 17:00:39,445:INFO:Declaring metric variables
2023-03-04 17:00:39,452:INFO:Importing untrained model
2023-03-04 17:00:39,459:INFO:Ridge Regression Imported successfully
2023-03-04 17:00:39,475:INFO:Starting cross validation
2023-03-04 17:00:39,479:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:00:39,840:INFO:Calculating mean and std
2023-03-04 17:00:39,842:INFO:Creating metrics dataframe
2023-03-04 17:00:39,846:INFO:Uploading results into container
2023-03-04 17:00:39,847:INFO:Uploading model into container now
2023-03-04 17:00:39,848:INFO:_master_model_container: 3
2023-03-04 17:00:39,848:INFO:_display_container: 2
2023-03-04 17:00:39,849:INFO:Ridge(random_state=6865)
2023-03-04 17:00:39,849:INFO:create_model() successfully completed......................................
2023-03-04 17:00:39,981:INFO:SubProcess create_model() end ==================================
2023-03-04 17:00:39,981:INFO:Creating metrics dataframe
2023-03-04 17:00:39,994:INFO:Initializing Elastic Net
2023-03-04 17:00:39,994:INFO:Total runtime is 0.24524393081665039 minutes
2023-03-04 17:00:39,998:INFO:SubProcess create_model() called ==================================
2023-03-04 17:00:39,998:INFO:Initializing create_model()
2023-03-04 17:00:39,999:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC6DA9730>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BC65C9820>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:00:39,999:INFO:Checking exceptions
2023-03-04 17:00:39,999:INFO:Importing libraries
2023-03-04 17:00:40,000:INFO:Copying training dataset
2023-03-04 17:00:40,009:INFO:Defining folds
2023-03-04 17:00:40,010:INFO:Declaring metric variables
2023-03-04 17:00:40,016:INFO:Importing untrained model
2023-03-04 17:00:40,023:INFO:Elastic Net Imported successfully
2023-03-04 17:00:40,037:INFO:Starting cross validation
2023-03-04 17:00:40,039:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:00:40,420:INFO:Calculating mean and std
2023-03-04 17:00:40,422:INFO:Creating metrics dataframe
2023-03-04 17:00:40,427:INFO:Uploading results into container
2023-03-04 17:00:40,427:INFO:Uploading model into container now
2023-03-04 17:00:40,428:INFO:_master_model_container: 4
2023-03-04 17:00:40,428:INFO:_display_container: 2
2023-03-04 17:00:40,429:INFO:ElasticNet(random_state=6865)
2023-03-04 17:00:40,429:INFO:create_model() successfully completed......................................
2023-03-04 17:00:40,562:INFO:SubProcess create_model() end ==================================
2023-03-04 17:00:40,563:INFO:Creating metrics dataframe
2023-03-04 17:00:40,575:INFO:Initializing Least Angle Regression
2023-03-04 17:00:40,575:INFO:Total runtime is 0.2549347956975301 minutes
2023-03-04 17:00:40,580:INFO:SubProcess create_model() called ==================================
2023-03-04 17:00:40,580:INFO:Initializing create_model()
2023-03-04 17:00:40,581:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC6DA9730>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BC65C9820>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:00:40,581:INFO:Checking exceptions
2023-03-04 17:00:40,581:INFO:Importing libraries
2023-03-04 17:00:40,581:INFO:Copying training dataset
2023-03-04 17:00:40,593:INFO:Defining folds
2023-03-04 17:00:40,593:INFO:Declaring metric variables
2023-03-04 17:00:40,600:INFO:Importing untrained model
2023-03-04 17:00:40,607:INFO:Least Angle Regression Imported successfully
2023-03-04 17:00:40,620:INFO:Starting cross validation
2023-03-04 17:00:40,623:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:00:40,782:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:00:40,782:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:00:40,799:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=2.261e+02, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,803:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.718e+02, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,805:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.546e+02, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,805:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.546e+02, with an active set of 33 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,812:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.399e+02, with an active set of 34 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,814:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:00:40,815:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.269e+02, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,816:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=8.018e+01, with an active set of 60 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,817:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=8.018e+01, with an active set of 60 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,817:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=7.844e+01, with an active set of 60 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

odel = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:00:40,818:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=7.844e+01, with an active set of 60 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,819:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=7.844e+01, with an active set of 60 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,819:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=7.515e+01, with an active set of 60 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,822:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=5.597e+01, with an active set of 64 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,822:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=7.032e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,822:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=7.032e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,823:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=5.242e+01, with an active set of 66 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,823:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=6.996e+01, with an active set of 53 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,824:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=6.996e+01, with an active set of 53 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,825:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=5.119e+01, with an active set of 68 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,826:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=6.788e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,826:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=4.771e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,827:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=4.771e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,827:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=4.422e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,828:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=4.422e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,828:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=6.214e+01, with an active set of 59 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,833:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.625e+02, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,834:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.701e+02, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,834:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.701e+02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,836:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:00:40,837:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.796e+02, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,837:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.506e+02, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,838:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.506e+02, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,838:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=3.611e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,838:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:00:40,838:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=3.611e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,839:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=3.575e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,839:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=3.575e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,841:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.219e+02, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,842:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=3.370e+01, with an active set of 79 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,844:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=9.344e+01, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,844:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=9.805e+01, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,844:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=4.546e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,845:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=4.140e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,845:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=3.959e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,846:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=8.726e+01, with an active set of 46 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,846:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=3.959e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,846:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=8.726e+01, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,846:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=3.959e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,847:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=8.337e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,848:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=3.664e+01, with an active set of 74 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,848:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=3.203e+01, with an active set of 81 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,849:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=7.323e+01, with an active set of 50 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,849:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=7.323e+01, with an active set of 50 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,850:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=2.965e+01, with an active set of 83 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,850:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.718e+02, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,851:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=2.965e+01, with an active set of 83 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,851:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=6.302e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,851:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=7.189e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,851:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=6.302e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,852:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=2.859e+01, with an active set of 84 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,852:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=2.859e+01, with an active set of 84 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,852:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=5.965e+01, with an active set of 53 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,852:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.641e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,853:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=2.793e+01, with an active set of 84 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,853:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=3.050e+01, with an active set of 81 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,853:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.641e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,853:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.566e+02, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,853:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=3.050e+01, with an active set of 81 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,853:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.641e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,854:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=2.957e+01, with an active set of 81 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,854:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=2.368e+01, with an active set of 86 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,854:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=2.443e+02, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,854:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=5.894e+01, with an active set of 61 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,854:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=5.734e+01, with an active set of 57 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,855:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=2.702e+01, with an active set of 81 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,855:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=5.894e+01, with an active set of 61 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,855:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=2.702e+01, with an active set of 81 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,855:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=2.633e+01, with an active set of 81 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,856:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 91 iterations, i.e. alpha=1.911e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,858:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=9.791e+01, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,858:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.455e+02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,858:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=9.791e+01, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,859:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 91 iterations, i.e. alpha=2.371e+01, with an active set of 84 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,859:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=4.529e+01, with an active set of 66 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,860:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 93 iterations, i.e. alpha=2.343e+01, with an active set of 86 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,860:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=4.272e+01, with an active set of 67 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,862:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=4.166e+01, with an active set of 68 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,862:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=7.914e+01, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,862:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=7.914e+01, with an active set of 46 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,863:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=7.914e+01, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,863:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=3.919e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,863:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=1.176e+02, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,863:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=7.914e+01, with an active set of 46 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,864:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=1.176e+02, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,864:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:00:40,865:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=3.560e+01, with an active set of 72 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,865:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=4.742e+01, with an active set of 69 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,865:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=1.812e+01, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,865:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=4.683e+01, with an active set of 69 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,866:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.001e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,866:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=3.267e+01, with an active set of 72 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,866:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=1.807e+01, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,867:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=4.310e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,867:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.001e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,867:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=3.267e+01, with an active set of 72 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,867:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:00:40,868:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=2.832e+01, with an active set of 74 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,868:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 98 iterations, i.e. alpha=1.477e+01, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,869:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=2.832e+01, with an active set of 74 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,869:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 98 iterations, i.e. alpha=1.477e+01, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,870:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=2.494e+01, with an active set of 75 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,870:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 98 iterations, i.e. alpha=1.458e+01, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,871:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=2.319e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,871:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=7.437e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,872:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=2.245e+02, with an active set of 76 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,872:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=7.658e+01, with an active set of 53 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,872:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=7.658e+01, with an active set of 53 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,873:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=1.432e+01, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,873:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=2.165e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,873:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=1.023e+02, with an active set of 78 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,874:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=7.203e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,874:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=2.646e+01, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,874:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 89 iterations, i.e. alpha=9.660e+01, with an active set of 79 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,875:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=2.646e+01, with an active set of 94 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,875:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.549e+02, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,875:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=2.359e+01, with an active set of 94 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,875:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=7.902e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,876:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=2.348e+01, with an active set of 94 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,876:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=7.902e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,877:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 91 iterations, i.e. alpha=6.319e+01, with an active set of 81 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,877:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 91 iterations, i.e. alpha=1.822e+01, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,877:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.948e+01, with an active set of 63 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,877:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 91 iterations, i.e. alpha=1.822e+01, with an active set of 85 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,878:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.948e+01, with an active set of 63 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,878:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 92 iterations, i.e. alpha=6.167e+01, with an active set of 82 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,878:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=2.418e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,878:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.948e+01, with an active set of 63 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,878:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 92 iterations, i.e. alpha=6.167e+01, with an active set of 82 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,878:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=2.125e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,878:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 92 iterations, i.e. alpha=1.698e+01, with an active set of 86 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,879:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=2.124e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,879:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 93 iterations, i.e. alpha=5.837e+01, with an active set of 83 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,879:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=2.119e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,879:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 93 iterations, i.e. alpha=1.535e+01, with an active set of 87 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,879:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=2.118e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,880:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 93 iterations, i.e. alpha=1.453e+01, with an active set of 87 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,880:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=5.509e+01, with an active set of 84 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,880:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=2.115e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,880:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=5.509e+01, with an active set of 84 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,880:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=2.108e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,880:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=1.370e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,881:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=2.102e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,881:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=2.099e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,881:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=5.398e+01, with an active set of 85 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,882:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=2.091e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,882:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=5.398e+01, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,882:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=4.650e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-04 17:00:40,882:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.561e+02, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,882:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=1.195e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,882:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=1.450e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,883:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=1.190e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,883:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=1.349e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,883:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.705e+06, with an active set of 105 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,883:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 96 iterations, i.e. alpha=5.250e+01, with an active set of 86 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,883:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.583e+06, with an active set of 105 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,883:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 96 iterations, i.e. alpha=1.155e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,884:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 96 iterations, i.e. alpha=4.475e+01, with an active set of 86 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,884:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.361e+02, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-04 17:00:40,884:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 96 iterations, i.e. alpha=1.155e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,884:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 96 iterations, i.e. alpha=1.066e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-04 17:00:40,885:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=6.924e+01, with an active set of 57 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,885:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 96 iterations, i.e. alpha=3.926e+01, with an active set of 86 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-04 17:00:40,885:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=1.725e+01, with an active set of 98 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-04 17:00:40,886:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=6.855e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,886:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.295e+06, with an active set of 105 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,886:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=6.855e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,886:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.276e+06, with an active set of 105 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,886:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 97 iterations, i.e. alpha=3.408e+01, with an active set of 87 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,887:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=1.010e+01, with an active set of 98 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-04 17:00:40,887:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 97 iterations, i.e. alpha=3.408e+01, with an active set of 87 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,887:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 98 iterations, i.e. alpha=8.176e+00, with an active set of 92 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,887:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.228e+06, with an active set of 105 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,888:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.226e+06, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,888:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 98 iterations, i.e. alpha=8.176e+00, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,888:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 98 iterations, i.e. alpha=2.680e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,888:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.166e+06, with an active set of 105 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,888:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=3.993e+01, with an active set of 79 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,888:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=9.469e+00, with an active set of 98 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,888:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.151e+06, with an active set of 105 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,888:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=3.993e+01, with an active set of 79 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,889:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=3.993e+01, with an active set of 79 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-04 17:00:40,889:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 100 iterations, i.e. alpha=7.677e+00, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,889:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=5.249e+01, with an active set of 64 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,889:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.121e+06, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,889:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 99 iterations, i.e. alpha=2.556e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-04 17:00:40,890:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=3.990e+01, with an active set of 79 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,890:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=5.249e+01, with an active set of 64 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,890:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.108e+06, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,890:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=8.619e+00, with an active set of 99 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,890:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.018e+06, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,890:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=7.938e+00, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,890:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 100 iterations, i.e. alpha=6.421e+00, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,891:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.980e+06, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,891:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=2.364e+01, with an active set of 91 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,891:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=3.740e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,891:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.917e+06, with an active set of 105 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,891:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=1.952e+01, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,892:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=3.648e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,892:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.899e+06, with an active set of 105 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,892:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 102 iterations, i.e. alpha=5.575e+00, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-04 17:00:40,893:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.674e+06, with an active set of 105 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,893:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=1.911e+01, with an active set of 93 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,893:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=8.784e+04, with an active set of 100 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,893:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=4.196e+01, with an active set of 67 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-04 17:00:40,893:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=1.911e+01, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-04 17:00:40,894:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.015e+02, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,894:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.043e+06, with an active set of 105 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,894:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=5.263e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,894:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=3.123e+03, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,894:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.408e+05, with an active set of 105 regressors, and the smallest cholesky pivot element being 7.068e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,894:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=2.950e+03, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,895:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=9.551e+01, with an active set of 49 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,895:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=2.921e+03, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,896:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=2.796e+03, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,896:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=2.744e+03, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,896:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=4.184e+00, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,896:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=1.752e+01, with an active set of 95 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(
2023-03-04 17:00:40,896:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=3.342e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,897:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=2.733e+03, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,897:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=4.011e+00, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,897:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=8.386e+01, with an active set of 50 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,897:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=2.599e+03, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,897:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=4.010e+00, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,897:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=2.582e+03, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,897:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=3.931e+00, with an active set of 99 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,898:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=2.565e+03, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,898:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=3.171e+00, with an active set of 99 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,898:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=3.040e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,898:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=3.156e+00, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.wwarn(

2023-03-04 17:00:40,898:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=3.040e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,899:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=3.155e+00, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-04 17:00:40,899:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=1.665e+01, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,899:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=3.153e+00, with an active set of 99 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,899:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=2.028e+03, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,899:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=3.152e+00, with an active set of 99 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,899:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=2.017e+03, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,900:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=2.681e+01, with an active set of 74 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,900:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=6.509e+00, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,900:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=1.972e+03, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,900:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=7.389e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,900:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=2.638e+01, with an active set of 74 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,900:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=1.723e+03, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,900:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=7.389e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,901:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=2.769e+00, with an active set of 99 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,901:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=1.714e+03, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,901:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=4.582e+00, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-04 17:00:40,901:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=2.599e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,902:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=9.550e+02, with an active set of 100 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,902:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=1.631e+00, with an active set of 99 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,902:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=2.599e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,902:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=8.713e+02, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,902:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=1.575e+00, with an active set of 99 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,902:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=2.882e+04, with an active set of 89 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,902:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=8.564e+02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,902:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=1.572e+00, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,903:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=6.672e+01, with an active set of 59 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,903:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=8.466e+02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,903:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=1.572e+00, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,903:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=2.066e+01, with an active set of 78 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,903:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.810e+02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,903:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=1.549e+00, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,904:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=6.672e+01, with an active set of 59 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,904:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=1.816e+02, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,904:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=6.672e+01, with an active set of 59 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,904:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=8.948e+01, with an active set of 50 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,904:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.375e+06, with an active set of 97 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,905:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=1.887e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,905:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=1.305e+00, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,906:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=1.887e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,906:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=6.264e+01, with an active set of 62 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,906:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=1.887e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,907:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=1.838e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,907:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=1.098e+00, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,907:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=9.866e-01, with an active set of 102 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,908:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=9.081e-01, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,908:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=1.854e+08, with an active set of 100 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,908:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=5.342e-01, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,908:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=2.267e+07, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,908:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=4.111e-01, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,908:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=1.690e+01, with an active set of 82 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,909:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=2.038e+07, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,909:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=4.090e-01, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,909:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=1.989e+07, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,909:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=4.074e-01, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,910:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=1.923e+07, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,910:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=4.072e-01, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,910:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=4.932e+01, with an active set of 67 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,910:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=1.517e+01, with an active set of 84 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,910:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=1.472e+07, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,910:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=4.058e-01, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,910:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=4.932e+01, with an active set of 67 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,910:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=1.517e+01, with an active set of 84 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,910:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=1.343e+07, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,911:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=4.056e-01, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,911:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=4.054e-01, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,911:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=1.517e+01, with an active set of 84 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,911:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=7.629e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,911:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=4.052e-01, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,912:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=1.403e+01, with an active set of 84 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,912:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=4.242e+01, with an active set of 69 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,912:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=1.231e+07, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,912:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=4.048e-01, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,912:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=1.038e+07, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,912:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=4.047e-01, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,913:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=4.195e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-04 17:00:40,913:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=4.036e-01, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,913:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=6.829e+01, with an active set of 57 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,913:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=1.021e+07, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,913:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=4.195e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,913:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=4.032e-01, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,914:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=8.402e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,914:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=4.024e-01, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,914:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=6.759e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-04 17:00:40,914:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=4.006e-01, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,914:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=4.107e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,915:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=6.630e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,915:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=3.994e-01, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,915:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=5.502e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,915:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=3.990e-01, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,915:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=5.395e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,916:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=3.930e-01, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,916:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=2.248e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,916:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=1.536e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,912:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 89 iterations, i.e. alpha=1.319e+01, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,917:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=2.618e-01, with an active set of 103 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,917:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=3.356e+05, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,917:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=3.678e+01, with an active set of 74 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,917:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 89 iterations, i.e. alpha=1.319e+01, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,917:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=2.579e-01, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,918:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=1.831e-01, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,918:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=1.010e-01, with an active set of 103 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,920:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=3.263e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,920:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=3.263e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,920:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=9.905e-02, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,921:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=3.263e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,921:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=9.709e-02, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,921:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=3.192e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-04 17:00:40,921:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=4.782e+01, with an active set of 66 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,921:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=2.877e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,921:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.937e+06, with an active set of 97 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,921:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=9.660e-02, with an active set of 103 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,922:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=9.611e-02, with an active set of 103 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,922:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=9.611e-02, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,922:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=9.562e-02, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,923:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=9.513e-02, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,923:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=1.056e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,923:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=9.439e-02, with an active set of 103 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,923:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=1.031e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,923:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=9.427e-02, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,924:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=1.031e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,924:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=9.415e-02, with an active set of 103 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,924:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=9.317e-02, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,924:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=9.403e+00, with an active set of 88 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,924:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=4.388e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,923:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=2.532e+01, with an active set of 81 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-04 17:00:40,925:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=9.095e+00, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,925:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=4.361e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,925:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=1.837e+07, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,926:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=9.219e-02, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,926:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=4.361e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,926:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=1.837e+07, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,926:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=9.219e-02, with an active set of 103 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,926:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=1.739e+07, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,927:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=9.120e-02, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,927:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 96 iterations, i.e. alpha=8.609e+00, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,928:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=9.022e-02, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,927:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=4.209e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,928:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=2.281e+01, with an active set of 83 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,928:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=5.030e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,929:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=2.693e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,929:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=2.259e+01, with an active set of 84 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-04 17:00:40,929:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 100 iterations, i.e. alpha=7.499e+00, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,930:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 100 iterations, i.e. alpha=7.499e+00, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,930:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=2.488e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,930:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=2.461e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,931:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 89 iterations, i.e. alpha=2.189e+01, with an active set of 85 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,931:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=2.350e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,931:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=6.400e+00, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,931:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 89 iterations, i.e. alpha=2.177e+01, with an active set of 85 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,932:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=6.399e+00, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-04 17:00:40,932:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=5.848e+00, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,932:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=2.190e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,932:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=1.840e+01, with an active set of 86 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,933:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=2.146e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,933:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=1.840e+01, with an active set of 86 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,933:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 102 iterations, i.e. alpha=4.926e+00, with an active set of 94 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,933:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=2.033e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,934:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 102 iterations, i.e. alpha=4.319e+00, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,934:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=1.976e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,934:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 102 iterations, i.e. alpha=4.191e+00, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,934:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=1.942e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,934:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 102 iterations, i.e. alpha=3.917e+00, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,935:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=1.833e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,935:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 102 iterations, i.e. alpha=3.917e+00, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,935:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=1.833e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,935:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 93 iterations, i.e. alpha=1.595e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,936:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 102 iterations, i.e. alpha=3.917e+00, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,936:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=1.807e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,936:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 93 iterations, i.e. alpha=1.595e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,936:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=1.765e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,936:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 102 iterations, i.e. alpha=3.692e+00, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,936:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 93 iterations, i.e. alpha=1.595e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,937:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=1.259e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,937:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 102 iterations, i.e. alpha=3.692e+00, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,937:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=4.071e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,937:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=1.028e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,937:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=3.571e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,938:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 93 iterations, i.e. alpha=1.501e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,938:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=7.966e+05, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,938:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=2.586e+00, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,938:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=6.264e+05, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,939:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=2.177e+00, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,939:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=5.838e+05, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,939:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=1.336e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,939:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=2.177e+00, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,939:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=4.448e+05, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,940:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=1.263e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,940:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=2.177e+00, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,940:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=2.177e+00, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,941:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=1.263e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,941:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=2.061e+00, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,941:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=1.262e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,941:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=1.267e+00, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,942:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=8.553e-01, with an active set of 96 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,942:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=6.887e-01, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,943:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.493e-01, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,943:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=3.099e+01, with an active set of 79 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,943:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.488e-01, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,944:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=3.193e-01, with an active set of 96 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,944:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 99 iterations, i.e. alpha=1.139e+01, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,944:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=2.272e-01, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,945:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=1.578e-01, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,945:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=1.577e-01, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,945:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=3.032e+01, with an active set of 81 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,946:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=1.334e-01, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,946:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=1.331e-01, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,946:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=2.962e+01, with an active set of 81 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,946:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=1.331e-01, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,947:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=1.330e-01, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,947:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=1.330e-01, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,948:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=1.329e-01, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,948:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=2.727e+01, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,948:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=1.329e-01, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,949:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=1.328e-01, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,949:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=1.326e-01, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,949:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=1.145e+01, with an active set of 96 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,950:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=1.145e+01, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,951:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=1.033e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,952:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=9.641e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,952:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=9.444e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,953:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=2.467e+01, with an active set of 84 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,954:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=9.628e+00, with an active set of 98 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,955:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 89 iterations, i.e. alpha=2.278e+01, with an active set of 86 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,955:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=9.588e+00, with an active set of 98 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,955:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=8.628e+00, with an active set of 98 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,956:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 89 iterations, i.e. alpha=1.714e+01, with an active set of 86 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,956:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=8.628e+00, with an active set of 98 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,956:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=6.797e+00, with an active set of 98 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,957:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=6.797e+00, with an active set of 98 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,957:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=5.836e+00, with an active set of 98 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,958:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=5.520e+00, with an active set of 98 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,958:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 93 iterations, i.e. alpha=1.639e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,959:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=5.463e+00, with an active set of 98 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,960:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=4.823e+00, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,960:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=4.500e+00, with an active set of 99 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,961:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=4.355e+00, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,961:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=4.355e+00, with an active set of 99 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,962:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=4.155e+00, with an active set of 99 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,964:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=1.577e+01, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,965:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=1.577e+01, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,965:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=3.927e+00, with an active set of 99 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,965:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=1.521e+01, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,966:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=3.644e+00, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,966:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=1.470e+01, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,966:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=1.423e+01, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,967:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=1.423e+01, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,968:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=6.364e+02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,968:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=1.115e+01, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,969:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.185e+02, with an active set of 100 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,969:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=4.292e+01, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,970:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=4.285e+01, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,970:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=4.204e+01, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,971:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=4.078e+01, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,971:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=4.021e+01, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,972:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=4.015e+01, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,972:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=4.014e+01, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,973:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=4.008e+01, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,973:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=1.037e+01, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,973:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=3.986e+01, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,974:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=3.979e+01, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,974:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=1.002e+01, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,974:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=3.978e+01, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,975:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=3.970e+01, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,975:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=3.964e+01, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,974:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=9.915e+00, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,976:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=3.958e+01, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,976:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=9.863e+00, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,976:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=3.944e+01, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,977:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=9.782e+00, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,977:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=3.942e+01, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,977:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=9.780e+00, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,978:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=3.925e+01, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,978:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=7.267e+00, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,978:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=3.902e+01, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,978:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=6.087e+00, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,978:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=3.880e+01, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,979:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=4.547e+00, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,979:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=3.845e+01, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,979:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=3.845e+01, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,980:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=3.806e+01, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,980:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=3.691e+01, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,980:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=4.170e+00, with an active set of 103 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,982:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=3.425e+00, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,983:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=3.412e+00, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,983:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=2.886e+00, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,984:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=2.865e+00, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,984:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=2.844e+00, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,984:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=2.842e+00, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,985:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=2.841e+00, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,985:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=2.743e+00, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,986:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=2.659e+00, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,987:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=2.397e+00, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,987:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=2.152e+00, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,988:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.967e+00, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,989:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.182e+00, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,989:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.116e+00, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,992:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.052e+00, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,992:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.041e+00, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,993:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.036e+00, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,994:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.022e+00, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,994:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.019e+00, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:40,999:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=9.648e-01, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,000:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=8.375e-01, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,000:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=6.248e-01, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,001:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=2.814e-01, with an active set of 104 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,447:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-04 17:00:41,497:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-04 17:00:41,637:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:00:41,642:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:00:41,643:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.278e+02, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,649:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.478e+02, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,650:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.893e+02, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,652:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=8.304e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,652:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=8.304e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,653:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=7.634e+01, with an active set of 49 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,654:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.246e+02, with an active set of 39 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,654:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=6.720e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,655:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=6.720e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,656:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.463e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,656:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.463e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,656:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=5.781e+01, with an active set of 61 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,656:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.463e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,657:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.463e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,659:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=8.194e+01, with an active set of 53 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,659:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=5.276e+01, with an active set of 68 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,659:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=5.057e+01, with an active set of 69 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,659:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=5.057e+01, with an active set of 69 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,660:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=7.522e+01, with an active set of 57 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,660:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=7.522e+01, with an active set of 57 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,660:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=4.918e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,661:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=7.134e+01, with an active set of 59 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,661:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=7.134e+01, with an active set of 59 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,661:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=4.222e+01, with an active set of 74 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,662:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=4.222e+01, with an active set of 74 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,663:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=3.879e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,663:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=3.755e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,663:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=5.820e+01, with an active set of 66 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,663:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=5.820e+01, with an active set of 66 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,664:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=3.325e+01, with an active set of 79 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,664:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=3.325e+01, with an active set of 79 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,664:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=3.163e+01, with an active set of 79 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,665:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=4.991e+01, with an active set of 72 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,666:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=4.991e+01, with an active set of 72 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,666:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=4.991e+01, with an active set of 72 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,666:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=2.686e+01, with an active set of 84 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,666:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=4.991e+01, with an active set of 72 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,666:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=2.686e+01, with an active set of 84 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,667:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=2.278e+01, with an active set of 85 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,667:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=2.059e+01, with an active set of 85 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,667:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=4.455e+01, with an active set of 74 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,667:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=2.059e+01, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,667:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=4.455e+01, with an active set of 74 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,667:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=2.019e+01, with an active set of 85 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,668:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=4.234e+01, with an active set of 75 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,668:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=1.977e+01, with an active set of 86 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,668:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=4.038e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,668:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=4.038e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,669:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=3.677e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,669:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=1.882e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,669:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=3.649e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,669:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=3.649e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,671:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=2.747e+01, with an active set of 82 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,671:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=2.747e+01, with an active set of 82 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,672:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=2.747e+01, with an active set of 82 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,672:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=2.747e+01, with an active set of 82 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,673:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=2.028e+01, with an active set of 86 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,674:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=2.840e+06, with an active set of 96 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,674:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 93 iterations, i.e. alpha=1.955e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,675:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 93 iterations, i.e. alpha=1.955e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,675:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 93 iterations, i.e. alpha=1.955e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,675:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 93 iterations, i.e. alpha=1.955e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,675:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 119 iterations, i.e. alpha=2.059e+06, with an active set of 98 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,676:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=1.696e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,676:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=1.694e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,677:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=1.540e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,677:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=1.526e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,677:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=1.519e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,677:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=1.517e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,678:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=2.346e+01, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-04 17:00:41,678:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=1.515e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,678:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=2.317e+01, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,678:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=1.512e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,678:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=2.138e+01, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,678:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=1.503e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,678:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=2.125e+01, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,679:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=1.498e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,679:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=2.099e+01, with an active set of 92 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,679:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=1.489e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,679:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=1.484e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,679:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 102 iterations, i.e. alpha=1.524e+01, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,679:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=1.472e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,680:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 102 iterations, i.e. alpha=1.504e+01, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,680:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=1.470e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,680:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 102 iterations, i.e. alpha=1.478e+01, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,680:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=1.468e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,680:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 102 iterations, i.e. alpha=1.475e+01, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,680:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=1.467e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,680:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 102 iterations, i.e. alpha=1.140e+01, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,680:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=1.448e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,680:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=1.444e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,681:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=1.038e+01, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,681:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=1.161e+06, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,681:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=1.013e+01, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,681:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=1.159e+06, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,681:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=9.810e+00, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,682:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=9.609e+05, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,682:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=9.635e+00, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,682:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=9.617e+00, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,682:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=9.553e+00, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,682:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=9.223e+00, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,683:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=8.946e+00, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,684:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=7.122e+00, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,684:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=5.364e+00, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,684:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=4.375e+11, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,685:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=5.279e+00, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,685:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=4.253e+11, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,685:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=5.085e+00, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,685:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=4.220e+11, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,685:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=4.892e+00, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,685:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=4.209e+11, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,685:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=4.882e+00, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,685:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=4.182e+11, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,686:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=4.541e+00, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,686:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=4.158e+11, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,686:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=4.486e+00, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,686:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=4.121e+11, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,686:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=4.423e+00, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,686:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=4.087e+11, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,686:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=4.381e+00, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,686:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=4.080e+11, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,686:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=4.343e+00, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,687:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=4.023e+11, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,687:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=4.307e+00, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,687:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=3.928e+11, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,687:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=3.824e+00, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,687:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=3.922e+11, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,687:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=3.717e+00, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,687:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=3.905e+11, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,687:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=3.610e+00, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,687:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=3.904e+11, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,688:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=3.603e+00, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,688:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=3.881e+11, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,688:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=3.583e+00, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,688:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=3.831e+11, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,688:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=3.583e+00, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,688:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=3.640e+11, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,688:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=3.257e+00, with an active set of 96 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,688:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=3.621e+11, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,688:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=3.197e+00, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,689:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=3.480e+11, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,689:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=1.924e+00, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,689:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=3.478e+11, with an active set of 104 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,689:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=1.446e+00, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,689:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=3.443e+11, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,689:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=9.747e-01, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,689:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=3.442e+11, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,689:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=5.443e-01, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,689:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=3.429e+11, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,690:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=1.175e-01, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:41,749:INFO:Calculating mean and std
2023-03-04 17:00:41,751:INFO:Creating metrics dataframe
2023-03-04 17:00:41,756:INFO:Uploading results into container
2023-03-04 17:00:41,757:INFO:Uploading model into container now
2023-03-04 17:00:41,757:INFO:_master_model_container: 5
2023-03-04 17:00:41,758:INFO:_display_container: 2
2023-03-04 17:00:41,758:INFO:Lars(random_state=6865)
2023-03-04 17:00:41,759:INFO:create_model() successfully completed......................................
2023-03-04 17:00:41,894:INFO:SubProcess create_model() end ==================================
2023-03-04 17:00:41,894:INFO:Creating metrics dataframe
2023-03-04 17:00:41,907:INFO:Initializing Lasso Least Angle Regression
2023-03-04 17:00:41,907:INFO:Total runtime is 0.2771254142125447 minutes
2023-03-04 17:00:41,912:INFO:SubProcess create_model() called ==================================
2023-03-04 17:00:41,912:INFO:Initializing create_model()
2023-03-04 17:00:41,913:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC6DA9730>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BC65C9820>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:00:41,913:INFO:Checking exceptions
2023-03-04 17:00:41,913:INFO:Importing libraries
2023-03-04 17:00:41,913:INFO:Copying training dataset
2023-03-04 17:00:41,923:INFO:Defining folds
2023-03-04 17:00:41,923:INFO:Declaring metric variables
2023-03-04 17:00:41,930:INFO:Importing untrained model
2023-03-04 17:00:41,937:INFO:Lasso Least Angle Regression Imported successfully
2023-03-04 17:00:41,951:INFO:Starting cross validation
2023-03-04 17:00:41,954:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:00:42,106:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-04 17:00:42,125:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-04 17:00:42,128:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-04 17:00:42,134:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.625e+02, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,135:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.399e+02, with an active set of 34 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,137:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.796e+02, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,137:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.269e+02, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,139:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=2.261e+02, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,141:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.219e+02, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,143:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=7.032e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,143:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=9.805e+01, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,144:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=7.032e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,144:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.718e+02, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,145:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=6.996e+01, with an active set of 53 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,145:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=6.996e+01, with an active set of 53 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,146:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.546e+02, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,147:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.546e+02, with an active set of 33 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,149:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=6.214e+01, with an active set of 59 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,150:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=7.189e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,153:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=5.894e+01, with an active set of 61 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,154:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=5.894e+01, with an active set of 61 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,156:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 63 iterations, alpha=5.493e+01, previous alpha=5.332e+01, with an active set of 62 regressors.
  warnings.warn(

2023-03-04 17:00:42,158:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=8.547e+01, with an active set of 56 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,159:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=3.873e+01, with an active set of 72 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,159:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-04 17:00:42,159:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=7.931e+01, with an active set of 59 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,160:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=7.931e+01, with an active set of 59 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,161:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=3.516e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,161:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=7.542e+01, with an active set of 60 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,161:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=3.516e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,162:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=3.480e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,162:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=3.480e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,163:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=3.440e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,165:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=3.054e+01, with an active set of 79 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,166:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=3.043e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,167:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=2.676e+01, with an active set of 82 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,167:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=2.536e+01, with an active set of 82 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,169:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=2.123e+01, with an active set of 84 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,163:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=5.627e+01, with an active set of 64 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,170:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=2.047e+01, with an active set of 86 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,170:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=1.976e+01, with an active set of 86 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,171:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 89 iterations, i.e. alpha=1.818e+01, with an active set of 87 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,171:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=5.312e+01, with an active set of 67 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,172:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.701e+02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,173:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=4.720e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,174:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.506e+02, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,174:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=4.720e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,174:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.506e+02, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,174:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=1.368e+01, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,175:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=1.307e+01, with an active set of 92 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,175:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=1.196e+01, with an active set of 92 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,177:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=4.455e+01, with an active set of 72 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,178:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 97 iterations, alpha=2.598e+01, previous alpha=9.784e+00, with an active set of 92 regressors.
  warnings.warn(

2023-03-04 17:00:42,178:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 75 iterations, alpha=4.799e+01, previous alpha=4.455e+01, with an active set of 72 regressors.
  warnings.warn(

2023-03-04 17:00:42,179:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-04 17:00:42,179:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=9.344e+01, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,181:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=8.726e+01, with an active set of 46 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,182:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=8.726e+01, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,183:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=8.337e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,184:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=7.323e+01, with an active set of 50 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,185:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=7.323e+01, with an active set of 50 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,186:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=6.302e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,186:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=6.302e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,187:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=5.965e+01, with an active set of 53 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,189:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=5.734e+01, with an active set of 57 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,191:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.718e+02, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,191:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 61 iterations, alpha=5.789e+01, previous alpha=5.611e+01, with an active set of 60 regressors.
  warnings.warn(

2023-03-04 17:00:42,193:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.641e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,193:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.641e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,194:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.641e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,196:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-04 17:00:42,198:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=9.791e+01, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,198:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=9.791e+01, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,202:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=7.914e+01, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,202:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=7.914e+01, with an active set of 46 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,203:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=7.914e+01, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,203:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-04 17:00:42,203:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=7.914e+01, with an active set of 46 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,204:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=7.820e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,206:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.566e+02, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,207:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=2.443e+02, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,207:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=7.437e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,210:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.455e+02, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,211:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.455e+02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,211:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-04 17:00:42,212:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.549e+02, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,212:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.948e+01, with an active set of 63 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,212:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.948e+01, with an active set of 63 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,213:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.948e+01, with an active set of 63 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,216:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=1.176e+02, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,216:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=1.176e+02, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,216:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=4.650e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,218:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 72 iterations, alpha=4.639e+01, previous alpha=4.591e+01, with an active set of 71 regressors.
  warnings.warn(

2023-03-04 17:00:42,219:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.561e+02, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,219:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.001e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,219:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.001e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,220:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.361e+02, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,221:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.361e+02, with an active set of 34 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,224:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=7.658e+01, with an active set of 53 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,224:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=7.658e+01, with an active set of 53 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,226:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=7.203e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,227:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=9.551e+01, with an active set of 49 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,227:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=6.924e+01, with an active set of 57 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,228:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=8.386e+01, with an active set of 50 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,228:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=6.855e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,229:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=8.386e+01, with an active set of 50 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,229:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=6.855e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,231:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 53 iterations, alpha=8.180e+01, previous alpha=7.957e+01, with an active set of 52 regressors.
  warnings.warn(

2023-03-04 17:00:42,232:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=5.249e+01, with an active set of 64 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,232:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.015e+02, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,232:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=5.249e+01, with an active set of 64 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,234:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=8.948e+01, with an active set of 50 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,234:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 67 iterations, alpha=5.211e+01, previous alpha=4.391e+01, with an active set of 66 regressors.
  warnings.warn(

2023-03-04 17:00:42,236:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=8.005e+01, with an active set of 53 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,237:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=7.629e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,238:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=6.829e+01, with an active set of 57 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,239:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=6.759e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,243:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=4.782e+01, with an active set of 66 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,246:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 70 iterations, alpha=4.649e+01, previous alpha=4.482e+01, with an active set of 69 regressors.
  warnings.warn(

2023-03-04 17:00:42,365:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-04 17:00:42,371:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.278e+02, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,378:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=8.304e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,378:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=8.304e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,378:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-04 17:00:42,379:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=7.634e+01, with an active set of 49 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,380:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=6.720e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,380:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=6.720e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,382:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=5.781e+01, with an active set of 61 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,382:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=5.781e+01, with an active set of 61 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,383:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.478e+02, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,384:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=5.276e+01, with an active set of 68 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,384:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=5.057e+01, with an active set of 69 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,385:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.893e+02, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,385:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=5.057e+01, with an active set of 69 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,386:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=4.845e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,389:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=4.152e+01, with an active set of 74 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,390:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.246e+02, with an active set of 39 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,391:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 78 iterations, alpha=4.005e+01, previous alpha=3.769e+01, with an active set of 75 regressors.
  warnings.warn(

2023-03-04 17:00:42,394:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.463e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,394:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.463e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,394:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.463e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,395:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.463e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,398:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=8.194e+01, with an active set of 53 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,400:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=7.522e+01, with an active set of 57 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,400:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=7.522e+01, with an active set of 57 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,401:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=7.134e+01, with an active set of 59 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,401:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=7.134e+01, with an active set of 59 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:00:42,403:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 61 iterations, alpha=7.105e+01, previous alpha=6.908e+01, with an active set of 60 regressors.
  warnings.warn(

2023-03-04 17:00:42,456:INFO:Calculating mean and std
2023-03-04 17:00:42,458:INFO:Creating metrics dataframe
2023-03-04 17:00:42,464:INFO:Uploading results into container
2023-03-04 17:00:42,465:INFO:Uploading model into container now
2023-03-04 17:00:42,467:INFO:_master_model_container: 6
2023-03-04 17:00:42,467:INFO:_display_container: 2
2023-03-04 17:00:42,468:INFO:LassoLars(random_state=6865)
2023-03-04 17:00:42,469:INFO:create_model() successfully completed......................................
2023-03-04 17:00:42,594:INFO:SubProcess create_model() end ==================================
2023-03-04 17:00:42,594:INFO:Creating metrics dataframe
2023-03-04 17:00:42,607:INFO:Initializing Orthogonal Matching Pursuit
2023-03-04 17:00:42,607:INFO:Total runtime is 0.2887948393821716 minutes
2023-03-04 17:00:42,613:INFO:SubProcess create_model() called ==================================
2023-03-04 17:00:42,613:INFO:Initializing create_model()
2023-03-04 17:00:42,613:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC6DA9730>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BC65C9820>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:00:42,614:INFO:Checking exceptions
2023-03-04 17:00:42,614:INFO:Importing libraries
2023-03-04 17:00:42,614:INFO:Copying training dataset
2023-03-04 17:00:42,623:INFO:Defining folds
2023-03-04 17:00:42,623:INFO:Declaring metric variables
2023-03-04 17:00:42,628:INFO:Importing untrained model
2023-03-04 17:00:42,636:INFO:Orthogonal Matching Pursuit Imported successfully
2023-03-04 17:00:42,650:INFO:Starting cross validation
2023-03-04 17:00:42,655:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:00:42,774:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:00:42,817:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:00:42,816:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:00:42,841:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:00:42,841:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:00:42,856:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:00:42,874:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:00:42,891:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:00:42,969:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:00:42,999:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:00:43,049:INFO:Calculating mean and std
2023-03-04 17:00:43,051:INFO:Creating metrics dataframe
2023-03-04 17:00:43,055:INFO:Uploading results into container
2023-03-04 17:00:43,055:INFO:Uploading model into container now
2023-03-04 17:00:43,056:INFO:_master_model_container: 7
2023-03-04 17:00:43,056:INFO:_display_container: 2
2023-03-04 17:00:43,057:INFO:OrthogonalMatchingPursuit()
2023-03-04 17:00:43,057:INFO:create_model() successfully completed......................................
2023-03-04 17:00:43,185:INFO:SubProcess create_model() end ==================================
2023-03-04 17:00:43,185:INFO:Creating metrics dataframe
2023-03-04 17:00:43,203:INFO:Initializing Bayesian Ridge
2023-03-04 17:00:43,203:INFO:Total runtime is 0.2987342874209086 minutes
2023-03-04 17:00:43,208:INFO:SubProcess create_model() called ==================================
2023-03-04 17:00:43,208:INFO:Initializing create_model()
2023-03-04 17:00:43,208:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC6DA9730>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BC65C9820>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:00:43,209:INFO:Checking exceptions
2023-03-04 17:00:43,209:INFO:Importing libraries
2023-03-04 17:00:43,210:INFO:Copying training dataset
2023-03-04 17:00:43,218:INFO:Defining folds
2023-03-04 17:00:43,218:INFO:Declaring metric variables
2023-03-04 17:00:43,223:INFO:Importing untrained model
2023-03-04 17:00:43,230:INFO:Bayesian Ridge Imported successfully
2023-03-04 17:00:43,241:INFO:Starting cross validation
2023-03-04 17:00:43,244:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:00:43,664:INFO:Calculating mean and std
2023-03-04 17:00:43,667:INFO:Creating metrics dataframe
2023-03-04 17:00:43,673:INFO:Uploading results into container
2023-03-04 17:00:43,673:INFO:Uploading model into container now
2023-03-04 17:00:43,674:INFO:_master_model_container: 8
2023-03-04 17:00:43,674:INFO:_display_container: 2
2023-03-04 17:00:43,675:INFO:BayesianRidge()
2023-03-04 17:00:43,675:INFO:create_model() successfully completed......................................
2023-03-04 17:00:43,814:INFO:SubProcess create_model() end ==================================
2023-03-04 17:00:43,814:INFO:Creating metrics dataframe
2023-03-04 17:00:43,827:INFO:Initializing Passive Aggressive Regressor
2023-03-04 17:00:43,827:INFO:Total runtime is 0.3091319322586059 minutes
2023-03-04 17:00:43,832:INFO:SubProcess create_model() called ==================================
2023-03-04 17:00:43,833:INFO:Initializing create_model()
2023-03-04 17:00:43,833:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC6DA9730>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BC65C9820>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:00:43,833:INFO:Checking exceptions
2023-03-04 17:00:43,833:INFO:Importing libraries
2023-03-04 17:00:43,834:INFO:Copying training dataset
2023-03-04 17:00:43,846:INFO:Defining folds
2023-03-04 17:00:43,847:INFO:Declaring metric variables
2023-03-04 17:00:43,851:INFO:Importing untrained model
2023-03-04 17:00:43,859:INFO:Passive Aggressive Regressor Imported successfully
2023-03-04 17:00:43,874:INFO:Starting cross validation
2023-03-04 17:00:43,876:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:00:44,280:INFO:Calculating mean and std
2023-03-04 17:00:44,282:INFO:Creating metrics dataframe
2023-03-04 17:00:44,287:INFO:Uploading results into container
2023-03-04 17:00:44,288:INFO:Uploading model into container now
2023-03-04 17:00:44,288:INFO:_master_model_container: 9
2023-03-04 17:00:44,288:INFO:_display_container: 2
2023-03-04 17:00:44,289:INFO:PassiveAggressiveRegressor(random_state=6865)
2023-03-04 17:00:44,289:INFO:create_model() successfully completed......................................
2023-03-04 17:00:44,416:INFO:SubProcess create_model() end ==================================
2023-03-04 17:00:44,416:INFO:Creating metrics dataframe
2023-03-04 17:00:44,430:INFO:Initializing Huber Regressor
2023-03-04 17:00:44,430:INFO:Total runtime is 0.3191883325576782 minutes
2023-03-04 17:00:44,435:INFO:SubProcess create_model() called ==================================
2023-03-04 17:00:44,436:INFO:Initializing create_model()
2023-03-04 17:00:44,436:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC6DA9730>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BC65C9820>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:00:44,437:INFO:Checking exceptions
2023-03-04 17:00:44,437:INFO:Importing libraries
2023-03-04 17:00:44,437:INFO:Copying training dataset
2023-03-04 17:00:44,445:INFO:Defining folds
2023-03-04 17:00:44,446:INFO:Declaring metric variables
2023-03-04 17:00:44,451:INFO:Importing untrained model
2023-03-04 17:00:44,457:INFO:Huber Regressor Imported successfully
2023-03-04 17:00:44,471:INFO:Starting cross validation
2023-03-04 17:00:44,475:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:00:44,799:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-04 17:00:44,854:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-04 17:00:44,875:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-04 17:00:44,881:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-04 17:00:44,929:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-04 17:00:44,935:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-04 17:00:44,949:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-04 17:00:44,962:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-04 17:00:45,412:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-04 17:00:45,491:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-04 17:00:45,656:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-04 17:00:45,693:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-04 17:00:45,958:INFO:Calculating mean and std
2023-03-04 17:00:45,960:INFO:Creating metrics dataframe
2023-03-04 17:00:45,964:INFO:Uploading results into container
2023-03-04 17:00:45,965:INFO:Uploading model into container now
2023-03-04 17:00:45,966:INFO:_master_model_container: 10
2023-03-04 17:00:45,966:INFO:_display_container: 2
2023-03-04 17:00:45,966:INFO:HuberRegressor()
2023-03-04 17:00:45,967:INFO:create_model() successfully completed......................................
2023-03-04 17:00:46,097:INFO:SubProcess create_model() end ==================================
2023-03-04 17:00:46,098:INFO:Creating metrics dataframe
2023-03-04 17:00:46,111:INFO:Initializing K Neighbors Regressor
2023-03-04 17:00:46,112:INFO:Total runtime is 0.347213339805603 minutes
2023-03-04 17:00:46,116:INFO:SubProcess create_model() called ==================================
2023-03-04 17:00:46,117:INFO:Initializing create_model()
2023-03-04 17:00:46,117:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC6DA9730>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BC65C9820>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:00:46,117:INFO:Checking exceptions
2023-03-04 17:00:46,117:INFO:Importing libraries
2023-03-04 17:00:46,117:INFO:Copying training dataset
2023-03-04 17:00:46,127:INFO:Defining folds
2023-03-04 17:00:46,128:INFO:Declaring metric variables
2023-03-04 17:00:46,134:INFO:Importing untrained model
2023-03-04 17:00:46,141:INFO:K Neighbors Regressor Imported successfully
2023-03-04 17:00:46,156:INFO:Starting cross validation
2023-03-04 17:00:46,158:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:00:46,593:INFO:Calculating mean and std
2023-03-04 17:00:46,595:INFO:Creating metrics dataframe
2023-03-04 17:00:46,600:INFO:Uploading results into container
2023-03-04 17:00:46,600:INFO:Uploading model into container now
2023-03-04 17:00:46,601:INFO:_master_model_container: 11
2023-03-04 17:00:46,601:INFO:_display_container: 2
2023-03-04 17:00:46,602:INFO:KNeighborsRegressor(n_jobs=-1)
2023-03-04 17:00:46,602:INFO:create_model() successfully completed......................................
2023-03-04 17:00:46,731:INFO:SubProcess create_model() end ==================================
2023-03-04 17:00:46,731:INFO:Creating metrics dataframe
2023-03-04 17:00:46,746:INFO:Initializing Decision Tree Regressor
2023-03-04 17:00:46,747:INFO:Total runtime is 0.3578017155329386 minutes
2023-03-04 17:00:46,751:INFO:SubProcess create_model() called ==================================
2023-03-04 17:00:46,752:INFO:Initializing create_model()
2023-03-04 17:00:46,752:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC6DA9730>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BC65C9820>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:00:46,753:INFO:Checking exceptions
2023-03-04 17:00:46,753:INFO:Importing libraries
2023-03-04 17:00:46,753:INFO:Copying training dataset
2023-03-04 17:00:46,765:INFO:Defining folds
2023-03-04 17:00:46,765:INFO:Declaring metric variables
2023-03-04 17:00:46,773:INFO:Importing untrained model
2023-03-04 17:00:46,780:INFO:Decision Tree Regressor Imported successfully
2023-03-04 17:00:46,793:INFO:Starting cross validation
2023-03-04 17:00:46,796:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:00:47,261:INFO:Calculating mean and std
2023-03-04 17:00:47,264:INFO:Creating metrics dataframe
2023-03-04 17:00:47,269:INFO:Uploading results into container
2023-03-04 17:00:47,269:INFO:Uploading model into container now
2023-03-04 17:00:47,270:INFO:_master_model_container: 12
2023-03-04 17:00:47,270:INFO:_display_container: 2
2023-03-04 17:00:47,271:INFO:DecisionTreeRegressor(random_state=6865)
2023-03-04 17:00:47,271:INFO:create_model() successfully completed......................................
2023-03-04 17:00:47,402:INFO:SubProcess create_model() end ==================================
2023-03-04 17:00:47,403:INFO:Creating metrics dataframe
2023-03-04 17:00:47,418:INFO:Initializing Random Forest Regressor
2023-03-04 17:00:47,418:INFO:Total runtime is 0.3689811031023661 minutes
2023-03-04 17:00:47,423:INFO:SubProcess create_model() called ==================================
2023-03-04 17:00:47,423:INFO:Initializing create_model()
2023-03-04 17:00:47,423:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC6DA9730>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BC65C9820>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:00:47,423:INFO:Checking exceptions
2023-03-04 17:00:47,424:INFO:Importing libraries
2023-03-04 17:00:47,424:INFO:Copying training dataset
2023-03-04 17:00:47,434:INFO:Defining folds
2023-03-04 17:00:47,434:INFO:Declaring metric variables
2023-03-04 17:00:47,440:INFO:Importing untrained model
2023-03-04 17:00:47,446:INFO:Random Forest Regressor Imported successfully
2023-03-04 17:00:47,459:INFO:Starting cross validation
2023-03-04 17:00:47,463:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:00:48,975:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-04 17:00:49,050:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-04 17:00:49,112:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-04 17:00:49,121:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-04 17:00:49,137:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-04 17:00:49,147:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-04 17:00:49,207:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-04 17:00:49,251:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-04 17:00:50,223:INFO:Calculating mean and std
2023-03-04 17:00:50,226:INFO:Creating metrics dataframe
2023-03-04 17:00:50,230:INFO:Uploading results into container
2023-03-04 17:00:50,231:INFO:Uploading model into container now
2023-03-04 17:00:50,231:INFO:_master_model_container: 13
2023-03-04 17:00:50,232:INFO:_display_container: 2
2023-03-04 17:00:50,232:INFO:RandomForestRegressor(n_jobs=-1, random_state=6865)
2023-03-04 17:00:50,232:INFO:create_model() successfully completed......................................
2023-03-04 17:00:50,375:INFO:SubProcess create_model() end ==================================
2023-03-04 17:00:50,375:INFO:Creating metrics dataframe
2023-03-04 17:00:50,394:INFO:Initializing Extra Trees Regressor
2023-03-04 17:00:50,394:INFO:Total runtime is 0.4185819784800211 minutes
2023-03-04 17:00:50,399:INFO:SubProcess create_model() called ==================================
2023-03-04 17:00:50,400:INFO:Initializing create_model()
2023-03-04 17:00:50,400:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC6DA9730>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BC65C9820>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:00:50,400:INFO:Checking exceptions
2023-03-04 17:00:50,401:INFO:Importing libraries
2023-03-04 17:00:50,401:INFO:Copying training dataset
2023-03-04 17:00:50,413:INFO:Defining folds
2023-03-04 17:00:50,414:INFO:Declaring metric variables
2023-03-04 17:00:50,419:INFO:Importing untrained model
2023-03-04 17:00:50,427:INFO:Extra Trees Regressor Imported successfully
2023-03-04 17:00:50,441:INFO:Starting cross validation
2023-03-04 17:00:50,445:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:00:52,175:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-04 17:00:52,247:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-04 17:00:52,256:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-04 17:00:52,261:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-04 17:00:52,294:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-04 17:00:52,349:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-04 17:00:52,445:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-04 17:00:52,507:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-04 17:00:53,358:INFO:Calculating mean and std
2023-03-04 17:00:53,360:INFO:Creating metrics dataframe
2023-03-04 17:00:53,364:INFO:Uploading results into container
2023-03-04 17:00:53,365:INFO:Uploading model into container now
2023-03-04 17:00:53,365:INFO:_master_model_container: 14
2023-03-04 17:00:53,365:INFO:_display_container: 2
2023-03-04 17:00:53,366:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=6865)
2023-03-04 17:00:53,366:INFO:create_model() successfully completed......................................
2023-03-04 17:00:53,496:INFO:SubProcess create_model() end ==================================
2023-03-04 17:00:53,496:INFO:Creating metrics dataframe
2023-03-04 17:00:53,511:INFO:Initializing AdaBoost Regressor
2023-03-04 17:00:53,511:INFO:Total runtime is 0.47053037087122596 minutes
2023-03-04 17:00:53,515:INFO:SubProcess create_model() called ==================================
2023-03-04 17:00:53,516:INFO:Initializing create_model()
2023-03-04 17:00:53,516:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC6DA9730>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BC65C9820>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:00:53,516:INFO:Checking exceptions
2023-03-04 17:00:53,517:INFO:Importing libraries
2023-03-04 17:00:53,517:INFO:Copying training dataset
2023-03-04 17:00:53,528:INFO:Defining folds
2023-03-04 17:00:53,529:INFO:Declaring metric variables
2023-03-04 17:00:53,534:INFO:Importing untrained model
2023-03-04 17:00:53,541:INFO:AdaBoost Regressor Imported successfully
2023-03-04 17:00:53,552:INFO:Starting cross validation
2023-03-04 17:00:53,555:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:00:54,532:INFO:Calculating mean and std
2023-03-04 17:00:54,534:INFO:Creating metrics dataframe
2023-03-04 17:00:54,539:INFO:Uploading results into container
2023-03-04 17:00:54,540:INFO:Uploading model into container now
2023-03-04 17:00:54,540:INFO:_master_model_container: 15
2023-03-04 17:00:54,541:INFO:_display_container: 2
2023-03-04 17:00:54,541:INFO:AdaBoostRegressor(random_state=6865)
2023-03-04 17:00:54,541:INFO:create_model() successfully completed......................................
2023-03-04 17:00:54,666:INFO:SubProcess create_model() end ==================================
2023-03-04 17:00:54,666:INFO:Creating metrics dataframe
2023-03-04 17:00:54,682:INFO:Initializing Gradient Boosting Regressor
2023-03-04 17:00:54,683:INFO:Total runtime is 0.490044895807902 minutes
2023-03-04 17:00:54,689:INFO:SubProcess create_model() called ==================================
2023-03-04 17:00:54,690:INFO:Initializing create_model()
2023-03-04 17:00:54,690:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC6DA9730>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BC65C9820>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:00:54,691:INFO:Checking exceptions
2023-03-04 17:00:54,691:INFO:Importing libraries
2023-03-04 17:00:54,691:INFO:Copying training dataset
2023-03-04 17:00:54,702:INFO:Defining folds
2023-03-04 17:00:54,702:INFO:Declaring metric variables
2023-03-04 17:00:54,709:INFO:Importing untrained model
2023-03-04 17:00:54,717:INFO:Gradient Boosting Regressor Imported successfully
2023-03-04 17:00:54,729:INFO:Starting cross validation
2023-03-04 17:00:54,732:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:00:55,740:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-04 17:00:55,741:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-04 17:00:55,756:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-04 17:00:55,779:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-04 17:00:55,793:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-04 17:00:55,794:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-04 17:00:55,837:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-04 17:00:55,883:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-04 17:00:56,550:INFO:Calculating mean and std
2023-03-04 17:00:56,552:INFO:Creating metrics dataframe
2023-03-04 17:00:56,556:INFO:Uploading results into container
2023-03-04 17:00:56,557:INFO:Uploading model into container now
2023-03-04 17:00:56,557:INFO:_master_model_container: 16
2023-03-04 17:00:56,557:INFO:_display_container: 2
2023-03-04 17:00:56,558:INFO:GradientBoostingRegressor(random_state=6865)
2023-03-04 17:00:56,558:INFO:create_model() successfully completed......................................
2023-03-04 17:00:56,679:INFO:SubProcess create_model() end ==================================
2023-03-04 17:00:56,679:INFO:Creating metrics dataframe
2023-03-04 17:00:56,700:INFO:Initializing Extreme Gradient Boosting
2023-03-04 17:00:56,701:INFO:Total runtime is 0.523704715569814 minutes
2023-03-04 17:00:56,706:INFO:SubProcess create_model() called ==================================
2023-03-04 17:00:56,706:INFO:Initializing create_model()
2023-03-04 17:00:56,706:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC6DA9730>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BC65C9820>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:00:56,706:INFO:Checking exceptions
2023-03-04 17:00:56,707:INFO:Importing libraries
2023-03-04 17:00:56,707:INFO:Copying training dataset
2023-03-04 17:00:56,718:INFO:Defining folds
2023-03-04 17:00:56,718:INFO:Declaring metric variables
2023-03-04 17:00:56,723:INFO:Importing untrained model
2023-03-04 17:00:56,732:INFO:Extreme Gradient Boosting Imported successfully
2023-03-04 17:00:56,742:INFO:Starting cross validation
2023-03-04 17:00:56,745:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:00:58,636:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-04 17:00:59,455:INFO:Calculating mean and std
2023-03-04 17:00:59,457:INFO:Creating metrics dataframe
2023-03-04 17:00:59,461:INFO:Uploading results into container
2023-03-04 17:00:59,462:INFO:Uploading model into container now
2023-03-04 17:00:59,463:INFO:_master_model_container: 17
2023-03-04 17:00:59,463:INFO:_display_container: 2
2023-03-04 17:00:59,464:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=6865, ...)
2023-03-04 17:00:59,464:INFO:create_model() successfully completed......................................
2023-03-04 17:00:59,584:INFO:SubProcess create_model() end ==================================
2023-03-04 17:00:59,584:INFO:Creating metrics dataframe
2023-03-04 17:00:59,603:INFO:Initializing Light Gradient Boosting Machine
2023-03-04 17:00:59,603:INFO:Total runtime is 0.5720674316088358 minutes
2023-03-04 17:00:59,609:INFO:SubProcess create_model() called ==================================
2023-03-04 17:00:59,609:INFO:Initializing create_model()
2023-03-04 17:00:59,609:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC6DA9730>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BC65C9820>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:00:59,610:INFO:Checking exceptions
2023-03-04 17:00:59,610:INFO:Importing libraries
2023-03-04 17:00:59,611:INFO:Copying training dataset
2023-03-04 17:00:59,622:INFO:Defining folds
2023-03-04 17:00:59,622:INFO:Declaring metric variables
2023-03-04 17:00:59,628:INFO:Importing untrained model
2023-03-04 17:00:59,635:INFO:Light Gradient Boosting Machine Imported successfully
2023-03-04 17:00:59,646:INFO:Starting cross validation
2023-03-04 17:00:59,648:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:01:02,709:INFO:Calculating mean and std
2023-03-04 17:01:02,711:INFO:Creating metrics dataframe
2023-03-04 17:01:02,716:INFO:Uploading results into container
2023-03-04 17:01:02,717:INFO:Uploading model into container now
2023-03-04 17:01:02,717:INFO:_master_model_container: 18
2023-03-04 17:01:02,717:INFO:_display_container: 2
2023-03-04 17:01:02,718:INFO:LGBMRegressor(random_state=6865)
2023-03-04 17:01:02,719:INFO:create_model() successfully completed......................................
2023-03-04 17:01:02,849:INFO:SubProcess create_model() end ==================================
2023-03-04 17:01:02,850:INFO:Creating metrics dataframe
2023-03-04 17:01:02,866:INFO:Initializing Dummy Regressor
2023-03-04 17:01:02,867:INFO:Total runtime is 0.6264638026555378 minutes
2023-03-04 17:01:02,871:INFO:SubProcess create_model() called ==================================
2023-03-04 17:01:02,871:INFO:Initializing create_model()
2023-03-04 17:01:02,872:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC6DA9730>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BC65C9820>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:01:02,872:INFO:Checking exceptions
2023-03-04 17:01:02,872:INFO:Importing libraries
2023-03-04 17:01:02,872:INFO:Copying training dataset
2023-03-04 17:01:02,882:INFO:Defining folds
2023-03-04 17:01:02,882:INFO:Declaring metric variables
2023-03-04 17:01:02,887:INFO:Importing untrained model
2023-03-04 17:01:02,893:INFO:Dummy Regressor Imported successfully
2023-03-04 17:01:02,904:INFO:Starting cross validation
2023-03-04 17:01:02,906:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:01:03,304:INFO:Calculating mean and std
2023-03-04 17:01:03,306:INFO:Creating metrics dataframe
2023-03-04 17:01:03,311:INFO:Uploading results into container
2023-03-04 17:01:03,311:INFO:Uploading model into container now
2023-03-04 17:01:03,312:INFO:_master_model_container: 19
2023-03-04 17:01:03,312:INFO:_display_container: 2
2023-03-04 17:01:03,312:INFO:DummyRegressor()
2023-03-04 17:01:03,312:INFO:create_model() successfully completed......................................
2023-03-04 17:01:03,443:INFO:SubProcess create_model() end ==================================
2023-03-04 17:01:03,443:INFO:Creating metrics dataframe
2023-03-04 17:01:03,478:INFO:Initializing create_model()
2023-03-04 17:01:03,478:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC6DA9730>, estimator=HuberRegressor(), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:01:03,478:INFO:Checking exceptions
2023-03-04 17:01:03,481:INFO:Importing libraries
2023-03-04 17:01:03,482:INFO:Copying training dataset
2023-03-04 17:01:03,490:INFO:Defining folds
2023-03-04 17:01:03,491:INFO:Declaring metric variables
2023-03-04 17:01:03,491:INFO:Importing untrained model
2023-03-04 17:01:03,491:INFO:Declaring custom model
2023-03-04 17:01:03,491:INFO:Huber Regressor Imported successfully
2023-03-04 17:01:03,493:INFO:Cross validation set to False
2023-03-04 17:01:03,493:INFO:Fitting Model
2023-03-04 17:01:03,624:INFO:HuberRegressor()
2023-03-04 17:01:03,624:INFO:create_model() successfully completed......................................
2023-03-04 17:01:03,835:INFO:_master_model_container: 19
2023-03-04 17:01:03,836:INFO:_display_container: 2
2023-03-04 17:01:03,836:INFO:HuberRegressor()
2023-03-04 17:01:03,836:INFO:compare_models() successfully completed......................................
2023-03-04 17:02:04,766:INFO:Initializing evaluate_model()
2023-03-04 17:02:04,767:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC6DA9730>, estimator=HuberRegressor(), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-03-04 17:02:04,799:INFO:Initializing plot_model()
2023-03-04 17:02:04,799:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=HuberRegressor(), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC6DA9730>, system=True)
2023-03-04 17:02:04,800:INFO:Checking exceptions
2023-03-04 17:02:04,803:INFO:Preloading libraries
2023-03-04 17:02:04,803:INFO:Copying training dataset
2023-03-04 17:02:04,803:INFO:Plot type: pipeline
2023-03-04 17:02:05,059:INFO:Visual Rendered Successfully
2023-03-04 17:02:05,172:INFO:plot_model() successfully completed......................................
2023-03-04 17:02:21,679:INFO:Initializing plot_model()
2023-03-04 17:02:21,679:INFO:plot_model(plot=feature, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=HuberRegressor(), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC6DA9730>, system=True)
2023-03-04 17:02:21,680:INFO:Checking exceptions
2023-03-04 17:02:21,684:INFO:Preloading libraries
2023-03-04 17:02:21,684:INFO:Copying training dataset
2023-03-04 17:02:21,684:INFO:Plot type: feature
2023-03-04 17:02:21,920:INFO:Visual Rendered Successfully
2023-03-04 17:02:22,041:INFO:plot_model() successfully completed......................................
2023-03-04 17:02:32,296:INFO:Initializing plot_model()
2023-03-04 17:02:32,297:INFO:plot_model(plot=parameter, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=HuberRegressor(), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC6DA9730>, system=True)
2023-03-04 17:02:32,297:INFO:Checking exceptions
2023-03-04 17:02:32,300:INFO:Preloading libraries
2023-03-04 17:02:32,301:INFO:Copying training dataset
2023-03-04 17:02:32,301:INFO:Plot type: parameter
2023-03-04 17:02:32,305:INFO:Visual Rendered Successfully
2023-03-04 17:02:32,418:INFO:plot_model() successfully completed......................................
2023-03-04 17:02:38,777:INFO:Initializing plot_model()
2023-03-04 17:02:38,777:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=HuberRegressor(), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC6DA9730>, system=True)
2023-03-04 17:02:38,778:INFO:Checking exceptions
2023-03-04 17:02:38,781:INFO:Preloading libraries
2023-03-04 17:02:38,781:INFO:Copying training dataset
2023-03-04 17:02:38,781:INFO:Plot type: pipeline
2023-03-04 17:02:38,854:INFO:Visual Rendered Successfully
2023-03-04 17:02:38,960:INFO:plot_model() successfully completed......................................
2023-03-04 17:04:23,936:INFO:Initializing predict_model()
2023-03-04 17:04:23,937:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC6DA9730>, estimator=HuberRegressor(), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x0000011BD294FF70>)
2023-03-04 17:04:23,937:INFO:Checking exceptions
2023-03-04 17:04:23,937:INFO:Preloading libraries
2023-03-04 17:04:23,939:INFO:Set up data.
2023-03-04 17:06:58,265:INFO:PyCaret RegressionExperiment
2023-03-04 17:06:58,265:INFO:Logging name: reg-default-name
2023-03-04 17:06:58,265:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-04 17:06:58,265:INFO:version 3.0.0.rc9
2023-03-04 17:06:58,266:INFO:Initializing setup()
2023-03-04 17:06:58,266:INFO:self.USI: aa84
2023-03-04 17:06:58,266:INFO:self._variable_keys: {'log_plots_param', 'pipeline', 'fold_generator', 'target_param', 'X_test', 'y_train', 'idx', 'data', 'seed', 'USI', 'n_jobs_param', 'gpu_n_jobs_param', 'exp_id', '_ml_usecase', 'gpu_param', '_available_plots', 'y_test', 'fold_groups_param', 'logging_param', 'y', 'html_param', 'fold_shuffle_param', 'transform_target_param', 'exp_name_log', 'X_train', 'memory', 'X'}
2023-03-04 17:06:58,266:INFO:Checking environment
2023-03-04 17:06:58,266:INFO:python_version: 3.9.13
2023-03-04 17:06:58,266:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-03-04 17:06:58,266:INFO:machine: AMD64
2023-03-04 17:06:58,266:INFO:platform: Windows-10-10.0.19044-SP0
2023-03-04 17:06:58,266:INFO:Memory: svmem(total=17009516544, available=3402575872, percent=80.0, used=13606940672, free=3402575872)
2023-03-04 17:06:58,266:INFO:Physical Core: 4
2023-03-04 17:06:58,266:INFO:Logical Core: 8
2023-03-04 17:06:58,266:INFO:Checking libraries
2023-03-04 17:06:58,266:INFO:System:
2023-03-04 17:06:58,266:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-03-04 17:06:58,266:INFO:executable: C:\Users\Alvaro\anaconda3\python.exe
2023-03-04 17:06:58,266:INFO:   machine: Windows-10-10.0.19044-SP0
2023-03-04 17:06:58,266:INFO:PyCaret required dependencies:
2023-03-04 17:06:58,266:INFO:                 pip: 22.2.2
2023-03-04 17:06:58,267:INFO:          setuptools: 63.4.1
2023-03-04 17:06:58,267:INFO:             pycaret: 3.0.0rc9
2023-03-04 17:06:58,267:INFO:             IPython: 7.31.1
2023-03-04 17:06:58,267:INFO:          ipywidgets: 7.6.5
2023-03-04 17:06:58,267:INFO:                tqdm: 4.64.1
2023-03-04 17:06:58,267:INFO:               numpy: 1.21.5
2023-03-04 17:06:58,267:INFO:              pandas: 1.4.4
2023-03-04 17:06:58,267:INFO:              jinja2: 2.11.3
2023-03-04 17:06:58,267:INFO:               scipy: 1.9.1
2023-03-04 17:06:58,267:INFO:              joblib: 1.2.0
2023-03-04 17:06:58,267:INFO:             sklearn: 1.0.2
2023-03-04 17:06:58,267:INFO:                pyod: 1.0.7
2023-03-04 17:06:58,267:INFO:            imblearn: 0.10.1
2023-03-04 17:06:58,267:INFO:   category_encoders: 2.6.0
2023-03-04 17:06:58,267:INFO:            lightgbm: 3.3.5
2023-03-04 17:06:58,267:INFO:               numba: 0.55.1
2023-03-04 17:06:58,267:INFO:            requests: 2.28.1
2023-03-04 17:06:58,267:INFO:          matplotlib: 3.5.2
2023-03-04 17:06:58,267:INFO:          scikitplot: 0.3.7
2023-03-04 17:06:58,267:INFO:         yellowbrick: 1.5
2023-03-04 17:06:58,267:INFO:              plotly: 5.9.0
2023-03-04 17:06:58,267:INFO:             kaleido: 0.2.1
2023-03-04 17:06:58,267:INFO:         statsmodels: 0.13.2
2023-03-04 17:06:58,267:INFO:              sktime: 0.16.1
2023-03-04 17:06:58,268:INFO:               tbats: 1.1.2
2023-03-04 17:06:58,268:INFO:            pmdarima: 2.0.2
2023-03-04 17:06:58,268:INFO:              psutil: 5.9.0
2023-03-04 17:06:58,268:INFO:PyCaret optional dependencies:
2023-03-04 17:06:58,268:INFO:                shap: Not installed
2023-03-04 17:06:58,268:INFO:           interpret: Not installed
2023-03-04 17:06:58,268:INFO:                umap: Not installed
2023-03-04 17:06:58,268:INFO:    pandas_profiling: Not installed
2023-03-04 17:06:58,268:INFO:  explainerdashboard: Not installed
2023-03-04 17:06:58,268:INFO:             autoviz: Not installed
2023-03-04 17:06:58,268:INFO:           fairlearn: Not installed
2023-03-04 17:06:58,268:INFO:             xgboost: 1.7.4
2023-03-04 17:06:58,268:INFO:            catboost: Not installed
2023-03-04 17:06:58,268:INFO:              kmodes: Not installed
2023-03-04 17:06:58,268:INFO:             mlxtend: Not installed
2023-03-04 17:06:58,268:INFO:       statsforecast: Not installed
2023-03-04 17:06:58,268:INFO:        tune_sklearn: Not installed
2023-03-04 17:06:58,268:INFO:                 ray: Not installed
2023-03-04 17:06:58,268:INFO:            hyperopt: Not installed
2023-03-04 17:06:58,268:INFO:              optuna: Not installed
2023-03-04 17:06:58,268:INFO:               skopt: Not installed
2023-03-04 17:06:58,268:INFO:              mlflow: Not installed
2023-03-04 17:06:58,268:INFO:              gradio: Not installed
2023-03-04 17:06:58,269:INFO:             fastapi: Not installed
2023-03-04 17:06:58,269:INFO:             uvicorn: Not installed
2023-03-04 17:06:58,269:INFO:              m2cgen: Not installed
2023-03-04 17:06:58,269:INFO:           evidently: Not installed
2023-03-04 17:06:58,269:INFO:               fugue: Not installed
2023-03-04 17:06:58,269:INFO:           streamlit: Not installed
2023-03-04 17:06:58,269:INFO:             prophet: Not installed
2023-03-04 17:06:58,269:INFO:None
2023-03-04 17:06:58,269:INFO:Set up data.
2023-03-04 17:06:58,310:INFO:Set up train/test split.
2023-03-04 17:06:58,314:INFO:Set up index.
2023-03-04 17:06:58,314:INFO:Set up folding strategy.
2023-03-04 17:06:58,314:INFO:Assigning column types.
2023-03-04 17:06:58,317:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-04 17:06:58,318:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-04 17:06:58,322:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-04 17:06:58,326:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-04 17:06:58,380:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:06:58,420:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-04 17:06:58,421:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:06:58,423:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:06:58,424:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-04 17:06:58,428:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-04 17:06:58,432:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-04 17:06:58,486:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:06:58,527:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-04 17:06:58,528:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:06:58,530:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:06:58,530:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-03-04 17:06:58,535:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-04 17:06:58,540:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-04 17:06:58,593:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:06:58,633:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-04 17:06:58,634:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:06:58,636:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:06:58,640:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-04 17:06:58,644:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-04 17:06:58,699:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:06:58,739:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-04 17:06:58,740:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:06:58,743:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:06:58,743:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-03-04 17:06:58,751:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-04 17:06:58,803:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:06:58,844:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-04 17:06:58,846:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:06:58,848:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:06:58,857:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-04 17:06:58,909:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:06:58,949:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-04 17:06:58,950:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:06:58,952:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:06:58,952:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-03-04 17:06:59,012:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:06:59,052:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-04 17:06:59,053:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:06:59,055:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:06:59,116:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:06:59,157:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-04 17:06:59,157:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:06:59,159:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:06:59,160:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-04 17:06:59,223:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:06:59,268:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:06:59,271:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:06:59,334:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:06:59,375:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:06:59,378:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:06:59,378:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-03-04 17:06:59,492:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:06:59,495:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:06:59,604:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:06:59,606:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:06:59,607:INFO:Preparing preprocessing pipeline...
2023-03-04 17:06:59,608:INFO:Set up column name cleaning.
2023-03-04 17:06:59,608:INFO:Set up simple imputation.
2023-03-04 17:06:59,637:INFO:Finished creating preprocessing pipeline.
2023-03-04 17:06:59,642:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Alvaro\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('numerical_imputer',
                 TransformerWrapper(include=['work_year', 'remote_ratio',
                                             'experience_level_EX',
                                             'experience_level_MI',
                                             'experience_level_SE',
                                             'employment_type_FL',
                                             'employment_type_FT',
                                             'emplo...
                                             'job_title_Data Architect',
                                             'job_title_Data Engineer',
                                             'job_title_Data Engineering '
                                             'Manager',
                                             'job_title_Data Science '
                                             'Consultant',
                                             'job_title_Data Science Engineer',
                                             'job_title_Data Science Manager',
                                             'job_title_Data Scientist', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent')))])
2023-03-04 17:06:59,642:INFO:Creating final display dataframe.
2023-03-04 17:06:59,820:INFO:Setup _display_container:                     Description             Value
0                    Session id              1873
1                        Target     salary_in_usd
2                   Target type        Regression
3           Original data shape        (499, 165)
4        Transformed data shape        (499, 165)
5   Transformed train set shape        (349, 165)
6    Transformed test set shape        (150, 165)
7              Numeric features               164
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator             KFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  reg-default-name
18                          USI              aa84
2023-03-04 17:06:59,943:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:06:59,945:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:07:00,058:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:07:00,060:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:07:00,061:INFO:setup() successfully completed in 1.8s...............
2023-03-04 17:07:18,277:INFO:PyCaret RegressionExperiment
2023-03-04 17:07:18,277:INFO:Logging name: reg-default-name
2023-03-04 17:07:18,277:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-04 17:07:18,277:INFO:version 3.0.0.rc9
2023-03-04 17:07:18,277:INFO:Initializing setup()
2023-03-04 17:07:18,277:INFO:self.USI: e4a5
2023-03-04 17:07:18,277:INFO:self._variable_keys: {'log_plots_param', 'pipeline', 'fold_generator', 'target_param', 'X_test', 'y_train', 'idx', 'data', 'seed', 'USI', 'n_jobs_param', 'gpu_n_jobs_param', 'exp_id', '_ml_usecase', 'gpu_param', '_available_plots', 'y_test', 'fold_groups_param', 'logging_param', 'y', 'html_param', 'fold_shuffle_param', 'transform_target_param', 'exp_name_log', 'X_train', 'memory', 'X'}
2023-03-04 17:07:18,278:INFO:Checking environment
2023-03-04 17:07:18,278:INFO:python_version: 3.9.13
2023-03-04 17:07:18,278:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-03-04 17:07:18,278:INFO:machine: AMD64
2023-03-04 17:07:18,278:INFO:platform: Windows-10-10.0.19044-SP0
2023-03-04 17:07:18,278:INFO:Memory: svmem(total=17009516544, available=3376713728, percent=80.1, used=13632802816, free=3376713728)
2023-03-04 17:07:18,278:INFO:Physical Core: 4
2023-03-04 17:07:18,278:INFO:Logical Core: 8
2023-03-04 17:07:18,278:INFO:Checking libraries
2023-03-04 17:07:18,278:INFO:System:
2023-03-04 17:07:18,278:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-03-04 17:07:18,278:INFO:executable: C:\Users\Alvaro\anaconda3\python.exe
2023-03-04 17:07:18,278:INFO:   machine: Windows-10-10.0.19044-SP0
2023-03-04 17:07:18,279:INFO:PyCaret required dependencies:
2023-03-04 17:07:18,279:INFO:                 pip: 22.2.2
2023-03-04 17:07:18,279:INFO:          setuptools: 63.4.1
2023-03-04 17:07:18,279:INFO:             pycaret: 3.0.0rc9
2023-03-04 17:07:18,279:INFO:             IPython: 7.31.1
2023-03-04 17:07:18,279:INFO:          ipywidgets: 7.6.5
2023-03-04 17:07:18,279:INFO:                tqdm: 4.64.1
2023-03-04 17:07:18,279:INFO:               numpy: 1.21.5
2023-03-04 17:07:18,279:INFO:              pandas: 1.4.4
2023-03-04 17:07:18,279:INFO:              jinja2: 2.11.3
2023-03-04 17:07:18,279:INFO:               scipy: 1.9.1
2023-03-04 17:07:18,279:INFO:              joblib: 1.2.0
2023-03-04 17:07:18,279:INFO:             sklearn: 1.0.2
2023-03-04 17:07:18,280:INFO:                pyod: 1.0.7
2023-03-04 17:07:18,280:INFO:            imblearn: 0.10.1
2023-03-04 17:07:18,280:INFO:   category_encoders: 2.6.0
2023-03-04 17:07:18,280:INFO:            lightgbm: 3.3.5
2023-03-04 17:07:18,280:INFO:               numba: 0.55.1
2023-03-04 17:07:18,280:INFO:            requests: 2.28.1
2023-03-04 17:07:18,280:INFO:          matplotlib: 3.5.2
2023-03-04 17:07:18,280:INFO:          scikitplot: 0.3.7
2023-03-04 17:07:18,280:INFO:         yellowbrick: 1.5
2023-03-04 17:07:18,280:INFO:              plotly: 5.9.0
2023-03-04 17:07:18,280:INFO:             kaleido: 0.2.1
2023-03-04 17:07:18,280:INFO:         statsmodels: 0.13.2
2023-03-04 17:07:18,280:INFO:              sktime: 0.16.1
2023-03-04 17:07:18,280:INFO:               tbats: 1.1.2
2023-03-04 17:07:18,280:INFO:            pmdarima: 2.0.2
2023-03-04 17:07:18,280:INFO:              psutil: 5.9.0
2023-03-04 17:07:18,280:INFO:PyCaret optional dependencies:
2023-03-04 17:07:18,281:INFO:                shap: Not installed
2023-03-04 17:07:18,281:INFO:           interpret: Not installed
2023-03-04 17:07:18,281:INFO:                umap: Not installed
2023-03-04 17:07:18,281:INFO:    pandas_profiling: Not installed
2023-03-04 17:07:18,281:INFO:  explainerdashboard: Not installed
2023-03-04 17:07:18,281:INFO:             autoviz: Not installed
2023-03-04 17:07:18,281:INFO:           fairlearn: Not installed
2023-03-04 17:07:18,281:INFO:             xgboost: 1.7.4
2023-03-04 17:07:18,281:INFO:            catboost: Not installed
2023-03-04 17:07:18,281:INFO:              kmodes: Not installed
2023-03-04 17:07:18,281:INFO:             mlxtend: Not installed
2023-03-04 17:07:18,281:INFO:       statsforecast: Not installed
2023-03-04 17:07:18,281:INFO:        tune_sklearn: Not installed
2023-03-04 17:07:18,281:INFO:                 ray: Not installed
2023-03-04 17:07:18,281:INFO:            hyperopt: Not installed
2023-03-04 17:07:18,281:INFO:              optuna: Not installed
2023-03-04 17:07:18,281:INFO:               skopt: Not installed
2023-03-04 17:07:18,281:INFO:              mlflow: Not installed
2023-03-04 17:07:18,281:INFO:              gradio: Not installed
2023-03-04 17:07:18,281:INFO:             fastapi: Not installed
2023-03-04 17:07:18,281:INFO:             uvicorn: Not installed
2023-03-04 17:07:18,281:INFO:              m2cgen: Not installed
2023-03-04 17:07:18,281:INFO:           evidently: Not installed
2023-03-04 17:07:18,281:INFO:               fugue: Not installed
2023-03-04 17:07:18,282:INFO:           streamlit: Not installed
2023-03-04 17:07:18,282:INFO:             prophet: Not installed
2023-03-04 17:07:18,282:INFO:None
2023-03-04 17:07:18,282:INFO:Set up data.
2023-03-04 17:09:03,017:INFO:PyCaret RegressionExperiment
2023-03-04 17:09:03,017:INFO:Logging name: reg-default-name
2023-03-04 17:09:03,017:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-04 17:09:03,018:INFO:version 3.0.0.rc9
2023-03-04 17:09:03,018:INFO:Initializing setup()
2023-03-04 17:09:03,018:INFO:self.USI: 3d6c
2023-03-04 17:09:03,018:INFO:self._variable_keys: {'log_plots_param', 'pipeline', 'fold_generator', 'target_param', 'X_test', 'y_train', 'idx', 'data', 'seed', 'USI', 'n_jobs_param', 'gpu_n_jobs_param', 'exp_id', '_ml_usecase', 'gpu_param', '_available_plots', 'y_test', 'fold_groups_param', 'logging_param', 'y', 'html_param', 'fold_shuffle_param', 'transform_target_param', 'exp_name_log', 'X_train', 'memory', 'X'}
2023-03-04 17:09:03,018:INFO:Checking environment
2023-03-04 17:09:03,018:INFO:python_version: 3.9.13
2023-03-04 17:09:03,018:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-03-04 17:09:03,018:INFO:machine: AMD64
2023-03-04 17:09:03,018:INFO:platform: Windows-10-10.0.19044-SP0
2023-03-04 17:09:03,018:INFO:Memory: svmem(total=17009516544, available=3412193280, percent=79.9, used=13597323264, free=3412193280)
2023-03-04 17:09:03,018:INFO:Physical Core: 4
2023-03-04 17:09:03,018:INFO:Logical Core: 8
2023-03-04 17:09:03,018:INFO:Checking libraries
2023-03-04 17:09:03,018:INFO:System:
2023-03-04 17:09:03,018:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-03-04 17:09:03,018:INFO:executable: C:\Users\Alvaro\anaconda3\python.exe
2023-03-04 17:09:03,018:INFO:   machine: Windows-10-10.0.19044-SP0
2023-03-04 17:09:03,018:INFO:PyCaret required dependencies:
2023-03-04 17:09:03,019:INFO:                 pip: 22.2.2
2023-03-04 17:09:03,019:INFO:          setuptools: 63.4.1
2023-03-04 17:09:03,019:INFO:             pycaret: 3.0.0rc9
2023-03-04 17:09:03,019:INFO:             IPython: 7.31.1
2023-03-04 17:09:03,019:INFO:          ipywidgets: 7.6.5
2023-03-04 17:09:03,019:INFO:                tqdm: 4.64.1
2023-03-04 17:09:03,019:INFO:               numpy: 1.21.5
2023-03-04 17:09:03,019:INFO:              pandas: 1.4.4
2023-03-04 17:09:03,019:INFO:              jinja2: 2.11.3
2023-03-04 17:09:03,019:INFO:               scipy: 1.9.1
2023-03-04 17:09:03,019:INFO:              joblib: 1.2.0
2023-03-04 17:09:03,019:INFO:             sklearn: 1.0.2
2023-03-04 17:09:03,019:INFO:                pyod: 1.0.7
2023-03-04 17:09:03,019:INFO:            imblearn: 0.10.1
2023-03-04 17:09:03,019:INFO:   category_encoders: 2.6.0
2023-03-04 17:09:03,019:INFO:            lightgbm: 3.3.5
2023-03-04 17:09:03,019:INFO:               numba: 0.55.1
2023-03-04 17:09:03,019:INFO:            requests: 2.28.1
2023-03-04 17:09:03,019:INFO:          matplotlib: 3.5.2
2023-03-04 17:09:03,019:INFO:          scikitplot: 0.3.7
2023-03-04 17:09:03,019:INFO:         yellowbrick: 1.5
2023-03-04 17:09:03,019:INFO:              plotly: 5.9.0
2023-03-04 17:09:03,019:INFO:             kaleido: 0.2.1
2023-03-04 17:09:03,020:INFO:         statsmodels: 0.13.2
2023-03-04 17:09:03,020:INFO:              sktime: 0.16.1
2023-03-04 17:09:03,020:INFO:               tbats: 1.1.2
2023-03-04 17:09:03,020:INFO:            pmdarima: 2.0.2
2023-03-04 17:09:03,020:INFO:              psutil: 5.9.0
2023-03-04 17:09:03,020:INFO:PyCaret optional dependencies:
2023-03-04 17:09:03,020:INFO:                shap: Not installed
2023-03-04 17:09:03,020:INFO:           interpret: Not installed
2023-03-04 17:09:03,020:INFO:                umap: Not installed
2023-03-04 17:09:03,020:INFO:    pandas_profiling: Not installed
2023-03-04 17:09:03,020:INFO:  explainerdashboard: Not installed
2023-03-04 17:09:03,020:INFO:             autoviz: Not installed
2023-03-04 17:09:03,020:INFO:           fairlearn: Not installed
2023-03-04 17:09:03,020:INFO:             xgboost: 1.7.4
2023-03-04 17:09:03,020:INFO:            catboost: Not installed
2023-03-04 17:09:03,020:INFO:              kmodes: Not installed
2023-03-04 17:09:03,020:INFO:             mlxtend: Not installed
2023-03-04 17:09:03,020:INFO:       statsforecast: Not installed
2023-03-04 17:09:03,020:INFO:        tune_sklearn: Not installed
2023-03-04 17:09:03,020:INFO:                 ray: Not installed
2023-03-04 17:09:03,020:INFO:            hyperopt: Not installed
2023-03-04 17:09:03,020:INFO:              optuna: Not installed
2023-03-04 17:09:03,020:INFO:               skopt: Not installed
2023-03-04 17:09:03,020:INFO:              mlflow: Not installed
2023-03-04 17:09:03,021:INFO:              gradio: Not installed
2023-03-04 17:09:03,021:INFO:             fastapi: Not installed
2023-03-04 17:09:03,021:INFO:             uvicorn: Not installed
2023-03-04 17:09:03,021:INFO:              m2cgen: Not installed
2023-03-04 17:09:03,021:INFO:           evidently: Not installed
2023-03-04 17:09:03,021:INFO:               fugue: Not installed
2023-03-04 17:09:03,021:INFO:           streamlit: Not installed
2023-03-04 17:09:03,021:INFO:             prophet: Not installed
2023-03-04 17:09:03,021:INFO:None
2023-03-04 17:09:03,021:INFO:Set up data.
2023-03-04 17:09:03,058:INFO:Set up train/test split.
2023-03-04 17:09:03,065:INFO:Set up index.
2023-03-04 17:09:03,065:INFO:Set up folding strategy.
2023-03-04 17:09:03,065:INFO:Assigning column types.
2023-03-04 17:09:03,068:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-04 17:09:03,068:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-04 17:09:03,073:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-04 17:09:03,077:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-04 17:09:03,132:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:09:03,172:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-04 17:09:03,173:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:09:03,175:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:09:03,176:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-04 17:09:03,180:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-04 17:09:03,184:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-04 17:09:03,238:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:09:03,279:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-04 17:09:03,279:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:09:03,282:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:09:03,282:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-03-04 17:09:03,287:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-04 17:09:03,292:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-04 17:09:03,345:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:09:03,386:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-04 17:09:03,387:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:09:03,389:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:09:03,394:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-04 17:09:03,398:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-04 17:09:03,452:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:09:03,492:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-04 17:09:03,492:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:09:03,495:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:09:03,495:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-03-04 17:09:03,506:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-04 17:09:03,559:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:09:03,600:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-04 17:09:03,601:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:09:03,604:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:09:03,612:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-04 17:09:03,666:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:09:03,707:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-04 17:09:03,708:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:09:03,710:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:09:03,711:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-03-04 17:09:03,773:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:09:03,813:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-04 17:09:03,813:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:09:03,816:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:09:03,880:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:09:03,919:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-04 17:09:03,920:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:09:03,922:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:09:03,923:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-04 17:09:03,982:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:09:04,021:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:09:04,024:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:09:04,083:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:09:04,123:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:09:04,126:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:09:04,126:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-03-04 17:09:04,232:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:09:04,235:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:09:04,336:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:09:04,339:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:09:04,340:INFO:Preparing preprocessing pipeline...
2023-03-04 17:09:04,341:INFO:Set up column name cleaning.
2023-03-04 17:09:04,341:INFO:Set up simple imputation.
2023-03-04 17:09:04,370:INFO:Finished creating preprocessing pipeline.
2023-03-04 17:09:04,375:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Alvaro\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('numerical_imputer',
                 TransformerWrapper(include=['work_year', 'remote_ratio',
                                             'experience_level_EX',
                                             'experience_level_MI',
                                             'experience_level_SE',
                                             'employment_type_FL',
                                             'employment_type_FT',
                                             'emplo...
                                             'job_title_Data Architect',
                                             'job_title_Data Engineer',
                                             'job_title_Data Engineering '
                                             'Manager',
                                             'job_title_Data Science '
                                             'Consultant',
                                             'job_title_Data Science Engineer',
                                             'job_title_Data Science Manager',
                                             'job_title_Data Scientist', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent')))])
2023-03-04 17:09:04,375:INFO:Creating final display dataframe.
2023-03-04 17:09:04,552:INFO:Setup _display_container:                     Description             Value
0                    Session id              1516
1                        Target     salary_in_usd
2                   Target type        Regression
3           Original data shape        (499, 165)
4        Transformed data shape        (499, 165)
5   Transformed train set shape        (349, 165)
6    Transformed test set shape        (150, 165)
7              Numeric features               164
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator             KFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  reg-default-name
18                          USI              3d6c
2023-03-04 17:09:04,670:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:09:04,672:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:09:04,772:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:09:04,774:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:09:04,774:INFO:setup() successfully completed in 1.76s...............
2023-03-04 17:09:06,457:INFO:Initializing compare_models()
2023-03-04 17:09:06,457:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC14B3A60>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000011BC14B3A60>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-03-04 17:09:06,458:INFO:Checking exceptions
2023-03-04 17:09:06,461:INFO:Preparing display monitor
2023-03-04 17:09:06,512:INFO:Initializing Linear Regression
2023-03-04 17:09:06,512:INFO:Total runtime is 0.0 minutes
2023-03-04 17:09:06,522:INFO:SubProcess create_model() called ==================================
2023-03-04 17:09:06,523:INFO:Initializing create_model()
2023-03-04 17:09:06,523:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC14B3A60>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BD2B364F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:09:06,523:INFO:Checking exceptions
2023-03-04 17:09:06,523:INFO:Importing libraries
2023-03-04 17:09:06,523:INFO:Copying training dataset
2023-03-04 17:09:06,535:INFO:Defining folds
2023-03-04 17:09:06,535:INFO:Declaring metric variables
2023-03-04 17:09:06,538:INFO:Importing untrained model
2023-03-04 17:09:06,541:INFO:Linear Regression Imported successfully
2023-03-04 17:09:06,551:INFO:Starting cross validation
2023-03-04 17:09:06,554:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:09:14,215:INFO:Calculating mean and std
2023-03-04 17:09:14,216:INFO:Creating metrics dataframe
2023-03-04 17:09:14,219:INFO:Uploading results into container
2023-03-04 17:09:14,219:INFO:Uploading model into container now
2023-03-04 17:09:14,220:INFO:_master_model_container: 1
2023-03-04 17:09:14,220:INFO:_display_container: 2
2023-03-04 17:09:14,220:INFO:LinearRegression(n_jobs=-1)
2023-03-04 17:09:14,220:INFO:create_model() successfully completed......................................
2023-03-04 17:09:14,323:INFO:SubProcess create_model() end ==================================
2023-03-04 17:09:14,324:INFO:Creating metrics dataframe
2023-03-04 17:09:14,330:INFO:Initializing Lasso Regression
2023-03-04 17:09:14,330:INFO:Total runtime is 0.13031154076258342 minutes
2023-03-04 17:09:14,335:INFO:SubProcess create_model() called ==================================
2023-03-04 17:09:14,335:INFO:Initializing create_model()
2023-03-04 17:09:14,335:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC14B3A60>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BD2B364F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:09:14,335:INFO:Checking exceptions
2023-03-04 17:09:14,336:INFO:Importing libraries
2023-03-04 17:09:14,336:INFO:Copying training dataset
2023-03-04 17:09:14,344:INFO:Defining folds
2023-03-04 17:09:14,345:INFO:Declaring metric variables
2023-03-04 17:09:14,348:INFO:Importing untrained model
2023-03-04 17:09:14,352:INFO:Lasso Regression Imported successfully
2023-03-04 17:09:14,363:INFO:Starting cross validation
2023-03-04 17:09:14,365:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:09:14,523:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.238e+10, tolerance: 1.565e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-04 17:09:14,535:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.741e+09, tolerance: 1.535e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-04 17:09:14,567:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.253e+10, tolerance: 1.583e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-04 17:09:14,573:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.040e+10, tolerance: 1.571e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-04 17:09:14,586:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.194e+10, tolerance: 1.491e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-04 17:09:14,602:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.014e+10, tolerance: 1.491e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-04 17:09:14,615:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.953e+10, tolerance: 1.587e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-04 17:09:14,689:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.778e+10, tolerance: 1.413e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-04 17:09:14,693:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.733e+10, tolerance: 1.527e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-04 17:09:14,720:INFO:Calculating mean and std
2023-03-04 17:09:14,722:INFO:Creating metrics dataframe
2023-03-04 17:09:14,725:INFO:Uploading results into container
2023-03-04 17:09:14,726:INFO:Uploading model into container now
2023-03-04 17:09:14,726:INFO:_master_model_container: 2
2023-03-04 17:09:14,726:INFO:_display_container: 2
2023-03-04 17:09:14,727:INFO:Lasso(random_state=1516)
2023-03-04 17:09:14,727:INFO:create_model() successfully completed......................................
2023-03-04 17:09:14,824:INFO:SubProcess create_model() end ==================================
2023-03-04 17:09:14,824:INFO:Creating metrics dataframe
2023-03-04 17:09:14,835:INFO:Initializing Ridge Regression
2023-03-04 17:09:14,835:INFO:Total runtime is 0.13872245947519937 minutes
2023-03-04 17:09:14,842:INFO:SubProcess create_model() called ==================================
2023-03-04 17:09:14,842:INFO:Initializing create_model()
2023-03-04 17:09:14,843:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC14B3A60>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BD2B364F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:09:14,843:INFO:Checking exceptions
2023-03-04 17:09:14,843:INFO:Importing libraries
2023-03-04 17:09:14,843:INFO:Copying training dataset
2023-03-04 17:09:14,849:INFO:Defining folds
2023-03-04 17:09:14,849:INFO:Declaring metric variables
2023-03-04 17:09:14,852:INFO:Importing untrained model
2023-03-04 17:09:14,860:INFO:Ridge Regression Imported successfully
2023-03-04 17:09:14,868:INFO:Starting cross validation
2023-03-04 17:09:14,870:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:09:15,139:INFO:Calculating mean and std
2023-03-04 17:09:15,140:INFO:Creating metrics dataframe
2023-03-04 17:09:15,143:INFO:Uploading results into container
2023-03-04 17:09:15,143:INFO:Uploading model into container now
2023-03-04 17:09:15,144:INFO:_master_model_container: 3
2023-03-04 17:09:15,144:INFO:_display_container: 2
2023-03-04 17:09:15,144:INFO:Ridge(random_state=1516)
2023-03-04 17:09:15,144:INFO:create_model() successfully completed......................................
2023-03-04 17:09:15,248:INFO:SubProcess create_model() end ==================================
2023-03-04 17:09:15,249:INFO:Creating metrics dataframe
2023-03-04 17:09:15,259:INFO:Initializing Elastic Net
2023-03-04 17:09:15,259:INFO:Total runtime is 0.1457809527715047 minutes
2023-03-04 17:09:15,263:INFO:SubProcess create_model() called ==================================
2023-03-04 17:09:15,263:INFO:Initializing create_model()
2023-03-04 17:09:15,263:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC14B3A60>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BD2B364F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:09:15,264:INFO:Checking exceptions
2023-03-04 17:09:15,264:INFO:Importing libraries
2023-03-04 17:09:15,264:INFO:Copying training dataset
2023-03-04 17:09:15,270:INFO:Defining folds
2023-03-04 17:09:15,270:INFO:Declaring metric variables
2023-03-04 17:09:15,275:INFO:Importing untrained model
2023-03-04 17:09:15,280:INFO:Elastic Net Imported successfully
2023-03-04 17:09:15,288:INFO:Starting cross validation
2023-03-04 17:09:15,290:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:09:15,570:INFO:Calculating mean and std
2023-03-04 17:09:15,571:INFO:Creating metrics dataframe
2023-03-04 17:09:15,574:INFO:Uploading results into container
2023-03-04 17:09:15,574:INFO:Uploading model into container now
2023-03-04 17:09:15,574:INFO:_master_model_container: 4
2023-03-04 17:09:15,575:INFO:_display_container: 2
2023-03-04 17:09:15,575:INFO:ElasticNet(random_state=1516)
2023-03-04 17:09:15,575:INFO:create_model() successfully completed......................................
2023-03-04 17:09:15,672:INFO:SubProcess create_model() end ==================================
2023-03-04 17:09:15,672:INFO:Creating metrics dataframe
2023-03-04 17:09:15,684:INFO:Initializing Least Angle Regression
2023-03-04 17:09:15,684:INFO:Total runtime is 0.1528620481491089 minutes
2023-03-04 17:09:15,687:INFO:SubProcess create_model() called ==================================
2023-03-04 17:09:15,688:INFO:Initializing create_model()
2023-03-04 17:09:15,688:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC14B3A60>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BD2B364F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:09:15,688:INFO:Checking exceptions
2023-03-04 17:09:15,688:INFO:Importing libraries
2023-03-04 17:09:15,689:INFO:Copying training dataset
2023-03-04 17:09:15,697:INFO:Defining folds
2023-03-04 17:09:15,698:INFO:Declaring metric variables
2023-03-04 17:09:15,702:INFO:Importing untrained model
2023-03-04 17:09:15,709:INFO:Least Angle Regression Imported successfully
2023-03-04 17:09:15,718:INFO:Starting cross validation
2023-03-04 17:09:15,720:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:09:15,816:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:09:15,832:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.429e+02, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,833:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:09:15,842:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.529e+02, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,846:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=1.368e+02, with an active set of 56 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,846:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=1.368e+02, with an active set of 56 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,847:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=1.368e+02, with an active set of 56 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,847:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.641e+02, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,848:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=1.368e+02, with an active set of 56 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,848:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.641e+02, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,851:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=1.237e+02, with an active set of 65 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,852:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=1.220e+02, with an active set of 66 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,854:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=1.164e+02, with an active set of 47 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,855:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.153e+02, with an active set of 48 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,855:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:09:15,856:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=1.098e+02, with an active set of 50 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,856:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=1.098e+02, with an active set of 50 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,858:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=1.006e+02, with an active set of 53 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,859:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=1.189e+02, with an active set of 79 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,864:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=8.322e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,865:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=8.322e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,865:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.339e+02, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,866:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=8.189e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,866:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:09:15,866:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=8.189e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,867:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=8.089e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,867:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=8.089e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,868:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.065e+02, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,869:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=2.032e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,870:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:09:15,871:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=8.224e+01, with an active set of 79 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,874:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=5.103e+02, with an active set of 105 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,875:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=4.733e+02, with an active set of 105 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,875:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.427e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,875:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=4.577e+02, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,875:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=4.477e+02, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,876:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=4.401e+02, with an active set of 105 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,876:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=4.398e+02, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,876:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 98 iterations, i.e. alpha=1.138e+02, with an active set of 89 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,877:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=4.101e+02, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,877:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=3.946e+02, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,877:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=3.926e+02, with an active set of 105 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,878:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=3.641e+02, with an active set of 105 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,878:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=3.526e+02, with an active set of 105 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,878:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=3.511e+02, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,879:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=2.993e+02, with an active set of 105 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,879:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=2.029e+02, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,879:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.993e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,879:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=1.913e+02, with an active set of 105 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,880:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=1.797e+02, with an active set of 105 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,880:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=1.298e+02, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,880:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=1.249e+02, with an active set of 105 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,881:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=1.170e+02, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,881:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=6.944e+01, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,881:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=1.239e+02, with an active set of 96 regressors, and the smallest cholesky pivot element being 9.940e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,882:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.836e+02, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,882:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=1.170e+02, with an active set of 55 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,884:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 115 iterations, i.e. alpha=1.223e+02, with an active set of 101 regressors, and the smallest cholesky pivot element being 9.940e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,884:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=1.027e+02, with an active set of 59 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,884:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.533e+02, with an active set of 40 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,885:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.458e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,886:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.022e+05, with an active set of 103 regressors, and the smallest cholesky pivot element being 9.424e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,886:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=9.051e+03, with an active set of 103 regressors, and the smallest cholesky pivot element being 9.940e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,886:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.329e+03, with an active set of 103 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,887:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.242e+03, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,887:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:09:15,887:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.232e+03, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,888:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.184e+03, with an active set of 103 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,888:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.166e+03, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,888:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.147e+03, with an active set of 103 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,888:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.691e+02, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,889:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.122e+03, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,889:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=1.276e+02, with an active set of 51 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,889:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.089e+03, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,889:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.691e+02, with an active set of 33 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,889:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=1.454e+02, with an active set of 70 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,889:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.073e+03, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,889:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.691e+02, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,889:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=1.276e+02, with an active set of 51 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,890:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.050e+03, with an active set of 103 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,890:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.034e+03, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,890:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=1.437e+02, with an active set of 71 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,890:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.033e+03, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,891:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=1.261e+02, with an active set of 53 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-04 17:09:15,891:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=9.941e+02, with an active set of 103 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,891:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=9.901e+02, with an active set of 103 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,892:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=9.837e+02, with an active set of 103 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,892:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=9.577e+02, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,892:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=1.176e+02, with an active set of 56 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,892:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=9.574e+02, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,892:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=1.382e+02, with an active set of 76 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,892:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=8.859e+02, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,892:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=1.382e+02, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,893:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=8.547e+02, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,893:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=1.342e+02, with an active set of 76 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,893:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=7.848e+02, with an active set of 103 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,894:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=1.295e+02, with an active set of 78 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,894:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=1.215e+02, with an active set of 78 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,895:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=1.188e+02, with an active set of 79 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,896:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=1.063e+02, with an active set of 50 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,896:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=1.024e+02, with an active set of 66 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,897:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=9.854e+01, with an active set of 51 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,897:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=1.026e+02, with an active set of 83 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,898:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=1.017e+02, with an active set of 83 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,898:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=9.657e+01, with an active set of 53 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,900:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:09:15,900:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=1.057e+02, with an active set of 75 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,902:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=8.942e+01, with an active set of 63 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,902:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=2.182e+04, with an active set of 87 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,902:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=8.942e+01, with an active set of 63 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,902:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.569e+02, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,902:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=8.941e+01, with an active set of 63 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,903:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=8.938e+01, with an active set of 63 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,903:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.511e+02, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,903:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=8.935e+01, with an active set of 63 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,904:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=1.220e+04, with an active set of 89 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,905:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.394e+02, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,906:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=9.282e+01, with an active set of 67 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,906:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=9.128e+03, with an active set of 93 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,907:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=6.933e+03, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,907:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.164e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,907:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=6.930e+03, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,908:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=6.876e+03, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,908:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=6.863e+03, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,908:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.061e+02, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-04 17:09:15,909:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.061e+02, with an active set of 44 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-04 17:09:15,909:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=6.806e+03, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,909:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=6.806e+03, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,910:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.008e+02, with an active set of 45 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,910:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=6.777e+03, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,910:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=6.763e+03, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,910:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.245e+02, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,910:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=6.757e+03, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,911:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=6.740e+03, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,911:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=6.700e+03, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,911:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=6.696e+03, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,912:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 89 iterations, i.e. alpha=9.927e+01, with an active set of 79 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,912:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=6.684e+03, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,912:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 89 iterations, i.e. alpha=9.927e+01, with an active set of 79 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,912:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=8.358e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,912:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.181e+04, with an active set of 95 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,912:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.846e+02, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,912:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=6.680e+03, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,913:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.846e+02, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,913:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=6.676e+03, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,913:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=8.799e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,913:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.846e+02, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,913:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=6.410e+03, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,913:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=6.344e+03, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,914:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=6.335e+03, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,914:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:09:15,914:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=6.316e+03, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,914:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=2.396e+04, with an active set of 97 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,915:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=8.806e+01, with an active set of 83 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,915:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=3.952e+03, with an active set of 97 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,915:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=7.517e+01, with an active set of 59 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,915:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=7.517e+01, with an active set of 59 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,915:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.428e+02, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,916:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 96 iterations, i.e. alpha=8.629e+01, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,916:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 96 iterations, i.e. alpha=8.629e+01, with an active set of 85 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,917:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=7.072e+03, with an active set of 99 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,917:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 127 iterations, i.e. alpha=1.249e+04, with an active set of 100 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,917:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 97 iterations, i.e. alpha=7.243e+01, with an active set of 86 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,917:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=7.318e+01, with an active set of 63 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,918:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 98 iterations, i.e. alpha=6.961e+01, with an active set of 87 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,918:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.130e+02, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,919:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.069e+02, with an active set of 39 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,919:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 100 iterations, i.e. alpha=6.859e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,919:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=6.781e+01, with an active set of 67 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,919:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 100 iterations, i.e. alpha=6.859e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,919:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 133 iterations, i.e. alpha=6.717e+04, with an active set of 103 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,920:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=6.561e+01, with an active set of 67 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,920:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 100 iterations, i.e. alpha=6.859e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,920:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=1.495e+07, with an active set of 102 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,920:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=5.875e+01, with an active set of 68 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,921:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=2.473e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,921:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=2.256e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,921:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 102 iterations, i.e. alpha=7.946e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,921:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=1.352e+04, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,922:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=2.220e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,922:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=9.228e+01, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,922:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=1.348e+04, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,922:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=2.205e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,922:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=9.228e+01, with an active set of 45 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,922:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=5.594e+01, with an active set of 91 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-04 17:09:15,922:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=2.197e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,923:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=1.332e+04, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,923:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=2.180e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,923:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=1.267e+04, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,923:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=2.172e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,923:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=8.830e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,923:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=1.266e+04, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,923:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=2.163e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,924:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=2.149e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,924:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=1.253e+04, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,924:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=2.148e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,924:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=4.492e+01, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,924:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=1.234e+04, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,924:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=2.141e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,924:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=1.213e+04, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,924:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=4.262e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,925:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=2.140e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,925:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=4.490e+01, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,925:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=1.209e+04, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,925:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=4.097e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,925:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=4.489e+01, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-04 17:09:15,925:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=1.180e+04, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,925:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=1.149e+04, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,926:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=2.105e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,926:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=1.055e+04, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,926:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=7.941e+01, with an active set of 53 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,926:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=2.094e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,926:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=1.025e+04, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,926:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=2.092e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,926:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=9.451e+03, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,926:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.859e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,926:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=4.118e+01, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,927:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=2.084e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,927:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.859e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,927:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=2.048e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,927:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=2.021e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,928:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=4.042e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,928:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=4.042e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,928:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=3.880e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,929:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=5.781e+01, with an active set of 61 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,929:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.595e+02, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,930:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=4.649e+01, with an active set of 99 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,930:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 97 iterations, i.e. alpha=6.147e+06, with an active set of 82 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,932:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=5.411e+01, with an active set of 67 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,932:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=5.411e+01, with an active set of 67 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,933:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=5.411e+01, with an active set of 67 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,933:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.373e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,933:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=1.765e+03, with an active set of 102 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,933:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=5.299e+01, with an active set of 68 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,934:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.361e+02, with an active set of 44 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,934:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=3.223e+02, with an active set of 103 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,934:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=9.258e+01, with an active set of 103 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,934:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.357e+02, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,935:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=9.237e+01, with an active set of 103 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,935:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=2.956e+07, with an active set of 88 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,935:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=8.889e+01, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,935:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=8.785e+01, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,935:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=8.687e+01, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,936:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=5.099e+01, with an active set of 72 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,936:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=8.369e+01, with an active set of 103 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,936:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=5.099e+01, with an active set of 72 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,936:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=8.297e+01, with an active set of 103 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,936:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=4.513e+01, with an active set of 72 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,936:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=8.243e+01, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,936:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=4.365e+01, with an active set of 72 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,937:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=8.238e+01, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,937:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=4.354e+01, with an active set of 72 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,937:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=8.227e+01, with an active set of 103 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,937:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=4.351e+01, with an active set of 72 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,937:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=8.191e+01, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,937:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=2.494e+07, with an active set of 93 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,937:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=1.231e+02, with an active set of 53 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,938:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=2.381e+07, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,938:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=2.367e+07, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-04 17:09:15,938:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=4.202e+01, with an active set of 74 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-04 17:09:15,938:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=8.115e+01, with an active set of 103 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,938:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=1.206e+02, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,939:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=4.202e+01, with an active set of 74 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-04 17:09:15,939:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=7.956e+01, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,939:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=1.204e+02, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,939:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=7.934e+01, with an active set of 103 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,939:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=7.065e+01, with an active set of 103 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,939:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=9.690e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,940:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=6.896e+01, with an active set of 103 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,940:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.736e+07, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,940:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=6.410e+01, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,940:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.714e+07, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,940:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=5.716e+01, with an active set of 103 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,940:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=3.547e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-04 17:09:15,940:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=4.418e+01, with an active set of 103 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,941:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.691e+07, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,941:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=3.547e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,941:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=9.353e+00, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,941:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.671e+07, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,941:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.666e+07, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,942:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.661e+07, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,942:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=3.402e+01, with an active set of 78 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,942:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.655e+07, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,942:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=9.034e+01, with an active set of 62 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,942:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=3.218e+01, with an active set of 78 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,942:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.640e+07, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,943:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.630e+07, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,943:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.628e+07, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,943:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.584e+07, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,944:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.112e+07, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,944:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=8.934e+01, with an active set of 64 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,944:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.108e+07, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,944:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=8.934e+01, with an active set of 64 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,945:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=6.895e+06, with an active set of 97 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,945:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=6.699e+06, with an active set of 97 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,946:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=2.631e+01, with an active set of 86 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,946:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=3.860e+06, with an active set of 98 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,946:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=2.631e+01, with an active set of 86 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,946:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=2.896e+06, with an active set of 98 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,947:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.768e+06, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,948:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.712e+06, with an active set of 99 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,948:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 96 iterations, i.e. alpha=2.437e+01, with an active set of 87 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,948:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=8.141e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,948:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.685e+06, with an active set of 99 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,948:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.637e+06, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,949:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=2.249e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,949:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 100 iterations, i.e. alpha=2.379e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,949:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=2.110e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,950:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=2.098e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,950:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=2.084e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,950:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=2.082e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,951:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=2.074e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,951:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=2.060e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,951:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=2.043e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,951:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=8.651e+01, with an active set of 78 regressors, and the smallest cholesky pivot element being 7.885e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,952:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=2.042e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,952:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=5.567e+01, with an active set of 93 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,952:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=2.032e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,952:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=2.516e+01, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,952:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=2.030e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,953:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=2.192e+01, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,953:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=2.004e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,953:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=2.740e+02, with an active set of 80 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,953:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=2.188e+01, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,953:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=1.985e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,953:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=2.139e+01, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,953:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=1.917e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,954:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 91 iterations, i.e. alpha=6.817e+01, with an active set of 81 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,954:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=1.259e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,954:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 91 iterations, i.e. alpha=6.817e+01, with an active set of 81 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,954:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=1.227e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,954:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=3.523e+05, with an active set of 100 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,955:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=2.315e+01, with an active set of 94 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,955:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 91 iterations, i.e. alpha=6.770e+01, with an active set of 81 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,955:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=2.268e+05, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,955:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=1.059e+05, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,955:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 91 iterations, i.e. alpha=6.770e+01, with an active set of 81 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,955:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=1.059e+05, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,955:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=2.100e+01, with an active set of 95 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,956:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=1.033e+05, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,956:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=1.888e+01, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,956:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=9.816e+04, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,956:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=1.888e+01, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,956:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 93 iterations, i.e. alpha=6.623e+01, with an active set of 83 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,956:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=1.869e+01, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,957:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=1.820e+01, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,958:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 96 iterations, i.e. alpha=6.064e+01, with an active set of 85 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,958:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=1.719e+01, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,958:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 96 iterations, i.e. alpha=6.064e+01, with an active set of 85 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,958:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=1.706e+01, with an active set of 96 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,959:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=1.673e+01, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,959:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=1.631e+01, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,959:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=1.465e+01, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,959:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 98 iterations, i.e. alpha=5.937e+01, with an active set of 87 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,960:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=1.378e+01, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,960:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 98 iterations, i.e. alpha=5.937e+01, with an active set of 87 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,960:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.189e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,961:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.118e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,961:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.025e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,961:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=5.768e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,961:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.017e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,961:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=5.768e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,961:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=9.523e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,962:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=9.406e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,962:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=8.401e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,962:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 102 iterations, i.e. alpha=5.724e+01, with an active set of 91 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,962:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 102 iterations, i.e. alpha=5.692e+01, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,963:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 115 iterations, i.e. alpha=7.971e+00, with an active set of 98 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,963:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=4.667e+01, with an active set of 92 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,964:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=6.260e+00, with an active set of 99 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,964:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=4.417e+01, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,964:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=4.586e+00, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,964:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=4.242e+00, with an active set of 99 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,965:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=4.070e+00, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,965:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=3.932e+00, with an active set of 99 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,965:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=5.470e+01, with an active set of 94 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,965:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=2.019e+00, with an active set of 99 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,966:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.990e+00, with an active set of 99 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,966:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=4.287e+01, with an active set of 95 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,966:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.963e+00, with an active set of 99 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,966:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=4.137e+01, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,967:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=3.085e+01, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,967:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=1.936e+00, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,967:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=1.850e+00, with an active set of 100 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,967:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=1.650e+00, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,968:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=2.908e+01, with an active set of 96 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,968:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=2.849e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,969:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=2.630e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,969:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.041e+00, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,969:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=2.530e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,969:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.005e+00, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,969:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=2.530e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,969:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=8.952e-01, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,970:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=2.363e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,970:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=8.848e-01, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,970:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=8.542e-01, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,970:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=8.498e-01, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,971:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=7.995e-01, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,971:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=3.487e+01, with an active set of 98 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,971:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=7.512e-01, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,971:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=6.900e-01, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,971:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=1.978e+01, with an active set of 98 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,972:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=6.775e-01, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,972:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=6.484e-01, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,972:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=6.303e-01, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,972:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=5.960e-01, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,973:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=5.938e-01, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,973:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=5.251e-01, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,973:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=8.748e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,974:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=6.933e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,974:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=6.755e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,974:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=6.737e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,975:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=6.497e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,975:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=6.298e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,975:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=6.154e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,976:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=5.915e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,976:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=3.943e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,976:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 126 iterations, i.e. alpha=3.009e+06, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,976:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=3.920e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,977:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 126 iterations, i.e. alpha=3.008e+06, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,977:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 126 iterations, i.e. alpha=2.954e+06, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,977:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 126 iterations, i.e. alpha=2.322e+06, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,978:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 126 iterations, i.e. alpha=2.211e+06, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,978:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 126 iterations, i.e. alpha=2.181e+06, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,978:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 126 iterations, i.e. alpha=1.490e+06, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,978:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 126 iterations, i.e. alpha=1.487e+06, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,979:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 126 iterations, i.e. alpha=1.468e+06, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,979:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 126 iterations, i.e. alpha=1.464e+06, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,979:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 126 iterations, i.e. alpha=1.464e+06, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,980:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 126 iterations, i.e. alpha=1.436e+06, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,980:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 126 iterations, i.e. alpha=1.373e+06, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,980:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 126 iterations, i.e. alpha=7.477e+05, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,981:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 126 iterations, i.e. alpha=7.185e+05, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,981:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 126 iterations, i.e. alpha=6.941e+05, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,981:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 126 iterations, i.e. alpha=6.618e+05, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,982:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 126 iterations, i.e. alpha=5.298e+04, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:15,982:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 126 iterations, i.e. alpha=2.704e+04, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,034:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:09:16,039:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.150e+02, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,041:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.739e+02, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,041:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.739e+02, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,043:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.030e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,043:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=9.644e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,044:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=9.644e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,044:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=8.491e+01, with an active set of 48 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,045:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=8.491e+01, with an active set of 48 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,045:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=8.491e+01, with an active set of 48 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,046:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=7.932e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,046:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=7.932e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,047:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=7.012e+01, with an active set of 61 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,048:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:09:16,048:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=7.012e+01, with an active set of 61 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,049:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=6.459e+01, with an active set of 65 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,049:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=6.056e+01, with an active set of 66 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,049:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=5.934e+01, with an active set of 67 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,051:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=5.651e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,052:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=4.819e+01, with an active set of 75 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,052:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=4.819e+01, with an active set of 75 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,052:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=4.819e+01, with an active set of 75 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,052:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=4.534e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,052:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.227e+02, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,053:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=3.909e+01, with an active set of 78 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,053:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=3.909e+01, with an active set of 78 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,053:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=2.064e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,054:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.898e+02, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,054:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=3.592e+01, with an active set of 82 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,054:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=3.592e+01, with an active set of 82 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,054:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=3.394e+01, with an active set of 82 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,054:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=3.391e+01, with an active set of 82 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,055:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 92 iterations, i.e. alpha=3.274e+01, with an active set of 86 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,055:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 93 iterations, i.e. alpha=3.078e+01, with an active set of 87 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,056:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.507e+02, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,058:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=1.291e+02, with an active set of 57 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,058:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=1.291e+02, with an active set of 57 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,059:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=1.265e+02, with an active set of 60 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,059:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=1.256e+02, with an active set of 61 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,059:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=1.609e+05, with an active set of 99 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,059:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=1.256e+02, with an active set of 61 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,060:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=1.551e+05, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,061:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=1.169e+02, with an active set of 68 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,061:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 130 iterations, i.e. alpha=1.222e+05, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,061:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=1.169e+02, with an active set of 68 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,061:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 130 iterations, i.e. alpha=1.204e+05, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,061:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 130 iterations, i.e. alpha=1.195e+05, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,061:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=1.159e+02, with an active set of 69 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,061:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 130 iterations, i.e. alpha=1.193e+05, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,061:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 130 iterations, i.e. alpha=1.167e+05, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,061:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 130 iterations, i.e. alpha=1.166e+05, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,061:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 130 iterations, i.e. alpha=1.162e+05, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,062:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 130 iterations, i.e. alpha=1.157e+05, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,062:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=1.139e+02, with an active set of 72 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,062:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 130 iterations, i.e. alpha=1.155e+05, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,062:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 130 iterations, i.e. alpha=1.148e+05, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,062:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 130 iterations, i.e. alpha=1.147e+05, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,062:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 130 iterations, i.e. alpha=1.139e+05, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,062:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 130 iterations, i.e. alpha=1.137e+05, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,062:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 130 iterations, i.e. alpha=1.132e+05, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,063:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 130 iterations, i.e. alpha=1.130e+05, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,063:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 130 iterations, i.e. alpha=1.029e+05, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,063:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 130 iterations, i.e. alpha=9.780e+04, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,063:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=6.092e+04, with an active set of 105 regressors, and the smallest cholesky pivot element being 8.752e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,063:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=5.232e+04, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,063:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=4.549e+04, with an active set of 105 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,064:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=1.090e+04, with an active set of 105 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,064:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=1.062e+04, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,064:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=1.041e+04, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,064:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=1.037e+04, with an active set of 105 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,064:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=1.024e+04, with an active set of 105 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,064:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=1.060e+02, with an active set of 87 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,064:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=1.023e+04, with an active set of 105 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,064:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=1.019e+04, with an active set of 105 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,065:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=1.019e+04, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,065:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=1.012e+04, with an active set of 105 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,065:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=1.009e+04, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,065:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=1.005e+04, with an active set of 105 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,065:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=1.001e+04, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,065:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=9.970e+03, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,065:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=9.935e+03, with an active set of 105 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,066:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=9.935e+03, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,066:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=9.723e+03, with an active set of 105 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,066:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=4.687e+02, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,067:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.691e+03, with an active set of 98 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,067:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.668e+03, with an active set of 98 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,067:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.618e+03, with an active set of 98 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,068:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.611e+03, with an active set of 98 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,068:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.583e+03, with an active set of 98 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,068:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.536e+03, with an active set of 98 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,068:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.535e+03, with an active set of 98 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,068:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.533e+03, with an active set of 98 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,068:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.533e+03, with an active set of 98 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,068:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.531e+03, with an active set of 98 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,069:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.515e+03, with an active set of 98 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,069:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.512e+03, with an active set of 98 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,069:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.493e+03, with an active set of 98 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,069:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.485e+03, with an active set of 98 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,069:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.484e+03, with an active set of 98 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,069:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.463e+03, with an active set of 98 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,069:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.403e+03, with an active set of 98 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,070:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=4.773e+02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,070:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=4.594e+02, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,070:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=4.572e+02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,070:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=4.570e+02, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,070:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=4.508e+02, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,071:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=4.501e+02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,071:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=4.490e+02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,071:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=4.441e+02, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,071:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=4.419e+02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,071:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=4.388e+02, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,071:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=4.366e+02, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,071:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=4.332e+02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,072:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=4.308e+02, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,072:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=4.303e+02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,072:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=4.288e+02, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,072:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=4.272e+02, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,072:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=4.156e+02, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,105:INFO:Calculating mean and std
2023-03-04 17:09:16,106:INFO:Creating metrics dataframe
2023-03-04 17:09:16,110:INFO:Uploading results into container
2023-03-04 17:09:16,111:INFO:Uploading model into container now
2023-03-04 17:09:16,111:INFO:_master_model_container: 5
2023-03-04 17:09:16,111:INFO:_display_container: 2
2023-03-04 17:09:16,111:INFO:Lars(random_state=1516)
2023-03-04 17:09:16,111:INFO:create_model() successfully completed......................................
2023-03-04 17:09:16,217:INFO:SubProcess create_model() end ==================================
2023-03-04 17:09:16,217:INFO:Creating metrics dataframe
2023-03-04 17:09:16,226:INFO:Initializing Lasso Least Angle Regression
2023-03-04 17:09:16,226:INFO:Total runtime is 0.1619062344233195 minutes
2023-03-04 17:09:16,229:INFO:SubProcess create_model() called ==================================
2023-03-04 17:09:16,230:INFO:Initializing create_model()
2023-03-04 17:09:16,230:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC14B3A60>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BD2B364F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:09:16,230:INFO:Checking exceptions
2023-03-04 17:09:16,230:INFO:Importing libraries
2023-03-04 17:09:16,230:INFO:Copying training dataset
2023-03-04 17:09:16,236:INFO:Defining folds
2023-03-04 17:09:16,236:INFO:Declaring metric variables
2023-03-04 17:09:16,240:INFO:Importing untrained model
2023-03-04 17:09:16,245:INFO:Lasso Least Angle Regression Imported successfully
2023-03-04 17:09:16,253:INFO:Starting cross validation
2023-03-04 17:09:16,255:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:09:16,343:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-04 17:09:16,351:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.429e+02, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,355:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.894e+02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,361:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.116e+02, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,366:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=9.164e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,366:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=9.164e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,367:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=8.902e+01, with an active set of 53 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,368:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=8.435e+01, with an active set of 56 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,369:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=7.967e+01, with an active set of 57 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,370:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=7.765e+01, with an active set of 60 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,372:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=7.647e+01, with an active set of 62 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,374:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=6.720e+01, with an active set of 68 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,375:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=6.720e+01, with an active set of 68 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,376:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=5.621e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,378:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=5.426e+01, with an active set of 75 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,378:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=5.123e+01, with an active set of 75 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,379:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=4.643e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,383:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 89 iterations, i.e. alpha=4.032e+01, with an active set of 83 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,383:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 89 iterations, i.e. alpha=4.032e+01, with an active set of 83 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,384:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 89 iterations, i.e. alpha=3.993e+01, with an active set of 83 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,384:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-04 17:09:16,384:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 89 iterations, i.e. alpha=3.813e+01, with an active set of 83 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,385:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=3.608e+01, with an active set of 84 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,385:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=3.524e+01, with an active set of 84 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,386:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 91 iterations, i.e. alpha=3.223e+01, with an active set of 85 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,387:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 92 iterations, i.e. alpha=3.029e+01, with an active set of 86 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,389:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 95 iterations, alpha=4.144e+01, previous alpha=2.707e+01, with an active set of 88 regressors.
  warnings.warn(

2023-03-04 17:09:16,394:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 21 iterations, alpha=2.469e+02, previous alpha=2.443e+02, with an active set of 22 regressors.
  warnings.warn(

2023-03-04 17:09:16,396:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.779e+02, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,396:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.779e+02, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,398:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-04 17:09:16,398:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.245e+02, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,399:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.141e+02, with an active set of 34 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,401:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.037e+02, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,402:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.002e+02, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,405:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=8.883e+01, with an active set of 48 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,405:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=8.883e+01, with an active set of 48 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,406:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=8.883e+01, with an active set of 48 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,408:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=7.813e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,409:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=7.393e+01, with an active set of 57 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

odel = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-04 17:09:16,410:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.691e+02, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,410:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=7.189e+01, with an active set of 59 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,410:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.691e+02, with an active set of 33 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,411:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.691e+02, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,411:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=6.324e+01, with an active set of 61 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,412:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=6.324e+01, with an active set of 61 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,412:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=6.259e+01, with an active set of 62 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,413:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 38 iterations, alpha=1.454e+02, previous alpha=1.442e+02, with an active set of 37 regressors.
  warnings.warn(

2023-03-04 17:09:16,415:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=5.454e+01, with an active set of 68 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,417:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 74 iterations, alpha=5.255e+01, previous alpha=5.062e+01, with an active set of 71 regressors.
  warnings.warn(

2023-03-04 17:09:16,417:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-04 17:09:16,426:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.566e+02, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,426:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.196e+02, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,429:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.859e+02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,430:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.638e+02, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,430:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.638e+02, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,431:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=9.082e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,431:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=9.082e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,432:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.413e+02, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,432:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=8.689e+01, with an active set of 56 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,433:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.268e+02, with an active set of 34 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,433:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=7.797e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,434:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-04 17:09:16,435:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.005e+02, with an active set of 40 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,436:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=6.941e+01, with an active set of 64 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,436:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 43 iterations, alpha=1.030e+02, previous alpha=1.005e+02, with an active set of 40 regressors.
  warnings.warn(

2023-03-04 17:09:16,438:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=5.982e+01, with an active set of 65 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,439:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=5.875e+01, with an active set of 66 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,439:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=5.795e+01, with an active set of 66 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,441:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 75 iterations, alpha=5.461e+01, previous alpha=5.340e+01, with an active set of 68 regressors.
  warnings.warn(

2023-03-04 17:09:16,442:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.245e+02, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,444:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.846e+02, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,445:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.846e+02, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,445:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.846e+02, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,447:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.428e+02, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,450:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-04 17:09:16,450:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.130e+02, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,451:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.069e+02, with an active set of 39 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,453:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=9.228e+01, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,453:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=9.228e+01, with an active set of 45 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,454:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=8.830e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,456:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=8.073e+01, with an active set of 51 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,457:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=7.941e+01, with an active set of 53 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,461:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=5.781e+01, with an active set of 60 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,461:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.922e+02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,462:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=5.729e+01, with an active set of 61 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,463:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.660e+02, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,464:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=5.408e+01, with an active set of 66 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,465:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=5.394e+01, with an active set of 67 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,465:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=5.394e+01, with an active set of 67 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,466:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=5.284e+01, with an active set of 68 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,466:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=5.227e+01, with an active set of 68 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,468:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.090e+02, with an active set of 45 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,468:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 73 iterations, alpha=5.162e+01, previous alpha=4.975e+01, with an active set of 70 regressors.
  warnings.warn(

2023-03-04 17:09:16,470:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=9.611e+01, with an active set of 50 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,471:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=9.611e+01, with an active set of 50 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,471:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=9.611e+01, with an active set of 50 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,472:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=8.538e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,473:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=8.538e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,474:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=8.202e+01, with an active set of 56 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,475:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=8.202e+01, with an active set of 56 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,476:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=7.336e+01, with an active set of 60 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,477:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=7.336e+01, with an active set of 60 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,479:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=6.696e+01, with an active set of 64 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,480:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=6.584e+01, with an active set of 65 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,481:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 68 iterations, alpha=6.968e+01, previous alpha=6.584e+01, with an active set of 65 regressors.
  warnings.warn(

2023-03-04 17:09:16,535:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-04 17:09:16,540:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-04 17:09:16,542:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.150e+02, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,544:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.739e+02, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,544:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.739e+02, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,544:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=2.816e+02, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,546:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.030e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,546:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=9.644e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,546:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.744e+02, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,546:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=9.644e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,546:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.744e+02, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,547:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.717e+02, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,547:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=8.491e+01, with an active set of 48 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,547:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=8.491e+01, with an active set of 48 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,547:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=8.491e+01, with an active set of 48 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,548:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.293e+02, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,548:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=7.932e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,548:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.043e+02, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,549:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=9.606e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,550:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=7.012e+01, with an active set of 61 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,550:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=8.562e+01, with an active set of 50 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,550:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=8.562e+01, with an active set of 50 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,551:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=6.459e+01, with an active set of 65 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,551:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=7.858e+01, with an active set of 53 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,551:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=7.858e+01, with an active set of 53 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,551:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=6.056e+01, with an active set of 66 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,551:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=7.431e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,551:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=5.934e+01, with an active set of 67 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,552:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 68 iterations, alpha=5.934e+01, previous alpha=5.888e+01, with an active set of 67 regressors.
  warnings.warn(

2023-03-04 17:09:16,552:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=6.659e+01, with an active set of 62 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,552:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=6.659e+01, with an active set of 62 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,553:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=5.906e+01, with an active set of 66 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,554:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=5.478e+01, with an active set of 68 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,554:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=5.049e+01, with an active set of 69 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,554:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=4.751e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,555:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=4.159e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,555:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=4.159e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,556:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=3.577e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,556:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=3.577e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,556:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=3.577e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,557:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=3.391e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,557:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=3.346e+01, with an active set of 78 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,558:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=2.958e+01, with an active set of 82 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,558:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=2.958e+01, with an active set of 82 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,558:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=2.729e+01, with an active set of 82 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,558:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=2.464e+01, with an active set of 83 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,558:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=2.288e+01, with an active set of 83 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,559:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 89 iterations, i.e. alpha=2.174e+01, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,559:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=1.944e+01, with an active set of 86 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,559:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=1.944e+01, with an active set of 86 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,560:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 92 iterations, i.e. alpha=1.522e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,560:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 92 iterations, i.e. alpha=1.501e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-04 17:09:16,560:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 93 iterations, alpha=2.079e+01, previous alpha=1.291e+01, with an active set of 88 regressors.
  warnings.warn(

2023-03-04 17:09:16,593:INFO:Calculating mean and std
2023-03-04 17:09:16,594:INFO:Creating metrics dataframe
2023-03-04 17:09:16,598:INFO:Uploading results into container
2023-03-04 17:09:16,599:INFO:Uploading model into container now
2023-03-04 17:09:16,599:INFO:_master_model_container: 6
2023-03-04 17:09:16,599:INFO:_display_container: 2
2023-03-04 17:09:16,600:INFO:LassoLars(random_state=1516)
2023-03-04 17:09:16,600:INFO:create_model() successfully completed......................................
2023-03-04 17:09:16,705:INFO:SubProcess create_model() end ==================================
2023-03-04 17:09:16,705:INFO:Creating metrics dataframe
2023-03-04 17:09:16,713:INFO:Initializing Orthogonal Matching Pursuit
2023-03-04 17:09:16,713:INFO:Total runtime is 0.17002710501352947 minutes
2023-03-04 17:09:16,716:INFO:SubProcess create_model() called ==================================
2023-03-04 17:09:16,717:INFO:Initializing create_model()
2023-03-04 17:09:16,717:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC14B3A60>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BD2B364F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:09:16,717:INFO:Checking exceptions
2023-03-04 17:09:16,717:INFO:Importing libraries
2023-03-04 17:09:16,717:INFO:Copying training dataset
2023-03-04 17:09:16,725:INFO:Defining folds
2023-03-04 17:09:16,725:INFO:Declaring metric variables
2023-03-04 17:09:16,730:INFO:Importing untrained model
2023-03-04 17:09:16,736:INFO:Orthogonal Matching Pursuit Imported successfully
2023-03-04 17:09:16,747:INFO:Starting cross validation
2023-03-04 17:09:16,749:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:09:16,840:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:09:16,849:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:09:16,871:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:09:16,886:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:09:16,897:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:09:16,906:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:09:16,914:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:09:16,930:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:09:16,981:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:09:16,988:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:09:17,021:INFO:Calculating mean and std
2023-03-04 17:09:17,023:INFO:Creating metrics dataframe
2023-03-04 17:09:17,026:INFO:Uploading results into container
2023-03-04 17:09:17,026:INFO:Uploading model into container now
2023-03-04 17:09:17,027:INFO:_master_model_container: 7
2023-03-04 17:09:17,027:INFO:_display_container: 2
2023-03-04 17:09:17,027:INFO:OrthogonalMatchingPursuit()
2023-03-04 17:09:17,027:INFO:create_model() successfully completed......................................
2023-03-04 17:09:17,134:INFO:SubProcess create_model() end ==================================
2023-03-04 17:09:17,135:INFO:Creating metrics dataframe
2023-03-04 17:09:17,143:INFO:Initializing Bayesian Ridge
2023-03-04 17:09:17,143:INFO:Total runtime is 0.1771912415822347 minutes
2023-03-04 17:09:17,146:INFO:SubProcess create_model() called ==================================
2023-03-04 17:09:17,146:INFO:Initializing create_model()
2023-03-04 17:09:17,147:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC14B3A60>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BD2B364F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:09:17,147:INFO:Checking exceptions
2023-03-04 17:09:17,147:INFO:Importing libraries
2023-03-04 17:09:17,147:INFO:Copying training dataset
2023-03-04 17:09:17,153:INFO:Defining folds
2023-03-04 17:09:17,154:INFO:Declaring metric variables
2023-03-04 17:09:17,158:INFO:Importing untrained model
2023-03-04 17:09:17,163:INFO:Bayesian Ridge Imported successfully
2023-03-04 17:09:17,172:INFO:Starting cross validation
2023-03-04 17:09:17,174:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:09:17,497:INFO:Calculating mean and std
2023-03-04 17:09:17,499:INFO:Creating metrics dataframe
2023-03-04 17:09:17,501:INFO:Uploading results into container
2023-03-04 17:09:17,502:INFO:Uploading model into container now
2023-03-04 17:09:17,502:INFO:_master_model_container: 8
2023-03-04 17:09:17,502:INFO:_display_container: 2
2023-03-04 17:09:17,503:INFO:BayesianRidge()
2023-03-04 17:09:17,503:INFO:create_model() successfully completed......................................
2023-03-04 17:09:17,612:INFO:SubProcess create_model() end ==================================
2023-03-04 17:09:17,613:INFO:Creating metrics dataframe
2023-03-04 17:09:17,621:INFO:Initializing Passive Aggressive Regressor
2023-03-04 17:09:17,621:INFO:Total runtime is 0.1851485331853231 minutes
2023-03-04 17:09:17,624:INFO:SubProcess create_model() called ==================================
2023-03-04 17:09:17,625:INFO:Initializing create_model()
2023-03-04 17:09:17,625:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC14B3A60>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BD2B364F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:09:17,625:INFO:Checking exceptions
2023-03-04 17:09:17,625:INFO:Importing libraries
2023-03-04 17:09:17,625:INFO:Copying training dataset
2023-03-04 17:09:17,631:INFO:Defining folds
2023-03-04 17:09:17,632:INFO:Declaring metric variables
2023-03-04 17:09:17,636:INFO:Importing untrained model
2023-03-04 17:09:17,639:INFO:Passive Aggressive Regressor Imported successfully
2023-03-04 17:09:17,649:INFO:Starting cross validation
2023-03-04 17:09:17,650:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:09:17,922:INFO:Calculating mean and std
2023-03-04 17:09:17,924:INFO:Creating metrics dataframe
2023-03-04 17:09:17,927:INFO:Uploading results into container
2023-03-04 17:09:17,928:INFO:Uploading model into container now
2023-03-04 17:09:17,928:INFO:_master_model_container: 9
2023-03-04 17:09:17,928:INFO:_display_container: 2
2023-03-04 17:09:17,929:INFO:PassiveAggressiveRegressor(random_state=1516)
2023-03-04 17:09:17,929:INFO:create_model() successfully completed......................................
2023-03-04 17:09:18,031:INFO:SubProcess create_model() end ==================================
2023-03-04 17:09:18,031:INFO:Creating metrics dataframe
2023-03-04 17:09:18,043:INFO:Initializing Huber Regressor
2023-03-04 17:09:18,044:INFO:Total runtime is 0.1921974539756775 minutes
2023-03-04 17:09:18,047:INFO:SubProcess create_model() called ==================================
2023-03-04 17:09:18,047:INFO:Initializing create_model()
2023-03-04 17:09:18,047:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC14B3A60>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BD2B364F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:09:18,048:INFO:Checking exceptions
2023-03-04 17:09:18,048:INFO:Importing libraries
2023-03-04 17:09:18,048:INFO:Copying training dataset
2023-03-04 17:09:18,054:INFO:Defining folds
2023-03-04 17:09:18,054:INFO:Declaring metric variables
2023-03-04 17:09:18,057:INFO:Importing untrained model
2023-03-04 17:09:18,062:INFO:Huber Regressor Imported successfully
2023-03-04 17:09:18,070:INFO:Starting cross validation
2023-03-04 17:09:18,071:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:09:18,304:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-04 17:09:18,319:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-04 17:09:18,371:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-04 17:09:18,378:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-04 17:09:18,421:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-04 17:09:18,422:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-04 17:09:18,460:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-04 17:09:18,460:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-04 17:09:18,865:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-04 17:09:18,870:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-04 17:09:18,897:INFO:Calculating mean and std
2023-03-04 17:09:18,898:INFO:Creating metrics dataframe
2023-03-04 17:09:18,901:INFO:Uploading results into container
2023-03-04 17:09:18,902:INFO:Uploading model into container now
2023-03-04 17:09:18,902:INFO:_master_model_container: 10
2023-03-04 17:09:18,902:INFO:_display_container: 2
2023-03-04 17:09:18,903:INFO:HuberRegressor()
2023-03-04 17:09:18,903:INFO:create_model() successfully completed......................................
2023-03-04 17:09:19,007:INFO:SubProcess create_model() end ==================================
2023-03-04 17:09:19,007:INFO:Creating metrics dataframe
2023-03-04 17:09:19,017:INFO:Initializing K Neighbors Regressor
2023-03-04 17:09:19,017:INFO:Total runtime is 0.2084206541379293 minutes
2023-03-04 17:09:19,020:INFO:SubProcess create_model() called ==================================
2023-03-04 17:09:19,020:INFO:Initializing create_model()
2023-03-04 17:09:19,020:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC14B3A60>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BD2B364F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:09:19,020:INFO:Checking exceptions
2023-03-04 17:09:19,021:INFO:Importing libraries
2023-03-04 17:09:19,021:INFO:Copying training dataset
2023-03-04 17:09:19,027:INFO:Defining folds
2023-03-04 17:09:19,027:INFO:Declaring metric variables
2023-03-04 17:09:19,031:INFO:Importing untrained model
2023-03-04 17:09:19,035:INFO:K Neighbors Regressor Imported successfully
2023-03-04 17:09:19,046:INFO:Starting cross validation
2023-03-04 17:09:19,049:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:09:19,349:INFO:Calculating mean and std
2023-03-04 17:09:19,351:INFO:Creating metrics dataframe
2023-03-04 17:09:19,354:INFO:Uploading results into container
2023-03-04 17:09:19,354:INFO:Uploading model into container now
2023-03-04 17:09:19,354:INFO:_master_model_container: 11
2023-03-04 17:09:19,355:INFO:_display_container: 2
2023-03-04 17:09:19,355:INFO:KNeighborsRegressor(n_jobs=-1)
2023-03-04 17:09:19,355:INFO:create_model() successfully completed......................................
2023-03-04 17:09:19,459:INFO:SubProcess create_model() end ==================================
2023-03-04 17:09:19,459:INFO:Creating metrics dataframe
2023-03-04 17:09:19,469:INFO:Initializing Decision Tree Regressor
2023-03-04 17:09:19,469:INFO:Total runtime is 0.21595194737116497 minutes
2023-03-04 17:09:19,472:INFO:SubProcess create_model() called ==================================
2023-03-04 17:09:19,473:INFO:Initializing create_model()
2023-03-04 17:09:19,473:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC14B3A60>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BD2B364F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:09:19,473:INFO:Checking exceptions
2023-03-04 17:09:19,473:INFO:Importing libraries
2023-03-04 17:09:19,473:INFO:Copying training dataset
2023-03-04 17:09:19,480:INFO:Defining folds
2023-03-04 17:09:19,480:INFO:Declaring metric variables
2023-03-04 17:09:19,484:INFO:Importing untrained model
2023-03-04 17:09:19,488:INFO:Decision Tree Regressor Imported successfully
2023-03-04 17:09:19,497:INFO:Starting cross validation
2023-03-04 17:09:19,501:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:09:19,784:INFO:Calculating mean and std
2023-03-04 17:09:19,786:INFO:Creating metrics dataframe
2023-03-04 17:09:19,789:INFO:Uploading results into container
2023-03-04 17:09:19,789:INFO:Uploading model into container now
2023-03-04 17:09:19,789:INFO:_master_model_container: 12
2023-03-04 17:09:19,790:INFO:_display_container: 2
2023-03-04 17:09:19,790:INFO:DecisionTreeRegressor(random_state=1516)
2023-03-04 17:09:19,790:INFO:create_model() successfully completed......................................
2023-03-04 17:09:19,907:INFO:SubProcess create_model() end ==================================
2023-03-04 17:09:19,907:INFO:Creating metrics dataframe
2023-03-04 17:09:19,918:INFO:Initializing Random Forest Regressor
2023-03-04 17:09:19,918:INFO:Total runtime is 0.22343307733535767 minutes
2023-03-04 17:09:19,921:INFO:SubProcess create_model() called ==================================
2023-03-04 17:09:19,921:INFO:Initializing create_model()
2023-03-04 17:09:19,921:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC14B3A60>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BD2B364F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:09:19,921:INFO:Checking exceptions
2023-03-04 17:09:19,921:INFO:Importing libraries
2023-03-04 17:09:19,922:INFO:Copying training dataset
2023-03-04 17:09:19,930:INFO:Defining folds
2023-03-04 17:09:19,930:INFO:Declaring metric variables
2023-03-04 17:09:19,934:INFO:Importing untrained model
2023-03-04 17:09:19,940:INFO:Random Forest Regressor Imported successfully
2023-03-04 17:09:19,951:INFO:Starting cross validation
2023-03-04 17:09:19,953:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:09:21,713:INFO:Calculating mean and std
2023-03-04 17:09:21,715:INFO:Creating metrics dataframe
2023-03-04 17:09:21,718:INFO:Uploading results into container
2023-03-04 17:09:21,718:INFO:Uploading model into container now
2023-03-04 17:09:21,718:INFO:_master_model_container: 13
2023-03-04 17:09:21,719:INFO:_display_container: 2
2023-03-04 17:09:21,719:INFO:RandomForestRegressor(n_jobs=-1, random_state=1516)
2023-03-04 17:09:21,719:INFO:create_model() successfully completed......................................
2023-03-04 17:09:21,823:INFO:SubProcess create_model() end ==================================
2023-03-04 17:09:21,823:INFO:Creating metrics dataframe
2023-03-04 17:09:21,833:INFO:Initializing Extra Trees Regressor
2023-03-04 17:09:21,834:INFO:Total runtime is 0.25536471605300903 minutes
2023-03-04 17:09:21,837:INFO:SubProcess create_model() called ==================================
2023-03-04 17:09:21,837:INFO:Initializing create_model()
2023-03-04 17:09:21,837:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC14B3A60>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BD2B364F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:09:21,837:INFO:Checking exceptions
2023-03-04 17:09:21,837:INFO:Importing libraries
2023-03-04 17:09:21,837:INFO:Copying training dataset
2023-03-04 17:09:21,843:INFO:Defining folds
2023-03-04 17:09:21,844:INFO:Declaring metric variables
2023-03-04 17:09:21,849:INFO:Importing untrained model
2023-03-04 17:09:21,854:INFO:Extra Trees Regressor Imported successfully
2023-03-04 17:09:21,863:INFO:Starting cross validation
2023-03-04 17:09:21,867:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:09:23,708:INFO:Calculating mean and std
2023-03-04 17:09:23,709:INFO:Creating metrics dataframe
2023-03-04 17:09:23,712:INFO:Uploading results into container
2023-03-04 17:09:23,712:INFO:Uploading model into container now
2023-03-04 17:09:23,713:INFO:_master_model_container: 14
2023-03-04 17:09:23,713:INFO:_display_container: 2
2023-03-04 17:09:23,713:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=1516)
2023-03-04 17:09:23,714:INFO:create_model() successfully completed......................................
2023-03-04 17:09:23,827:INFO:SubProcess create_model() end ==================================
2023-03-04 17:09:23,827:INFO:Creating metrics dataframe
2023-03-04 17:09:23,838:INFO:Initializing AdaBoost Regressor
2023-03-04 17:09:23,838:INFO:Total runtime is 0.288770866394043 minutes
2023-03-04 17:09:23,842:INFO:SubProcess create_model() called ==================================
2023-03-04 17:09:23,842:INFO:Initializing create_model()
2023-03-04 17:09:23,842:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC14B3A60>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BD2B364F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:09:23,843:INFO:Checking exceptions
2023-03-04 17:09:23,843:INFO:Importing libraries
2023-03-04 17:09:23,843:INFO:Copying training dataset
2023-03-04 17:09:23,849:INFO:Defining folds
2023-03-04 17:09:23,849:INFO:Declaring metric variables
2023-03-04 17:09:23,853:INFO:Importing untrained model
2023-03-04 17:09:23,857:INFO:AdaBoost Regressor Imported successfully
2023-03-04 17:09:23,866:INFO:Starting cross validation
2023-03-04 17:09:23,869:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:09:25,027:INFO:Calculating mean and std
2023-03-04 17:09:25,029:INFO:Creating metrics dataframe
2023-03-04 17:09:25,034:INFO:Uploading results into container
2023-03-04 17:09:25,035:INFO:Uploading model into container now
2023-03-04 17:09:25,035:INFO:_master_model_container: 15
2023-03-04 17:09:25,035:INFO:_display_container: 2
2023-03-04 17:09:25,036:INFO:AdaBoostRegressor(random_state=1516)
2023-03-04 17:09:25,036:INFO:create_model() successfully completed......................................
2023-03-04 17:09:25,140:INFO:SubProcess create_model() end ==================================
2023-03-04 17:09:25,140:INFO:Creating metrics dataframe
2023-03-04 17:09:25,152:INFO:Initializing Gradient Boosting Regressor
2023-03-04 17:09:25,152:INFO:Total runtime is 0.310664439201355 minutes
2023-03-04 17:09:25,156:INFO:SubProcess create_model() called ==================================
2023-03-04 17:09:25,157:INFO:Initializing create_model()
2023-03-04 17:09:25,157:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC14B3A60>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BD2B364F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:09:25,157:INFO:Checking exceptions
2023-03-04 17:09:25,157:INFO:Importing libraries
2023-03-04 17:09:25,157:INFO:Copying training dataset
2023-03-04 17:09:25,163:INFO:Defining folds
2023-03-04 17:09:25,163:INFO:Declaring metric variables
2023-03-04 17:09:25,169:INFO:Importing untrained model
2023-03-04 17:09:25,175:INFO:Gradient Boosting Regressor Imported successfully
2023-03-04 17:09:25,184:INFO:Starting cross validation
2023-03-04 17:09:25,185:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:09:26,290:INFO:Calculating mean and std
2023-03-04 17:09:26,292:INFO:Creating metrics dataframe
2023-03-04 17:09:26,296:INFO:Uploading results into container
2023-03-04 17:09:26,296:INFO:Uploading model into container now
2023-03-04 17:09:26,297:INFO:_master_model_container: 16
2023-03-04 17:09:26,297:INFO:_display_container: 2
2023-03-04 17:09:26,297:INFO:GradientBoostingRegressor(random_state=1516)
2023-03-04 17:09:26,297:INFO:create_model() successfully completed......................................
2023-03-04 17:09:26,410:INFO:SubProcess create_model() end ==================================
2023-03-04 17:09:26,410:INFO:Creating metrics dataframe
2023-03-04 17:09:26,427:INFO:Initializing Extreme Gradient Boosting
2023-03-04 17:09:26,427:INFO:Total runtime is 0.3319277763366699 minutes
2023-03-04 17:09:26,431:INFO:SubProcess create_model() called ==================================
2023-03-04 17:09:26,431:INFO:Initializing create_model()
2023-03-04 17:09:26,431:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC14B3A60>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BD2B364F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:09:26,431:INFO:Checking exceptions
2023-03-04 17:09:26,432:INFO:Importing libraries
2023-03-04 17:09:26,433:INFO:Copying training dataset
2023-03-04 17:09:26,443:INFO:Defining folds
2023-03-04 17:09:26,444:INFO:Declaring metric variables
2023-03-04 17:09:26,447:INFO:Importing untrained model
2023-03-04 17:09:26,456:INFO:Extreme Gradient Boosting Imported successfully
2023-03-04 17:09:26,464:INFO:Starting cross validation
2023-03-04 17:09:26,467:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:09:28,740:INFO:Calculating mean and std
2023-03-04 17:09:28,742:INFO:Creating metrics dataframe
2023-03-04 17:09:28,745:INFO:Uploading results into container
2023-03-04 17:09:28,746:INFO:Uploading model into container now
2023-03-04 17:09:28,746:INFO:_master_model_container: 17
2023-03-04 17:09:28,746:INFO:_display_container: 2
2023-03-04 17:09:28,747:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=1516, ...)
2023-03-04 17:09:28,747:INFO:create_model() successfully completed......................................
2023-03-04 17:09:28,853:INFO:SubProcess create_model() end ==================================
2023-03-04 17:09:28,853:INFO:Creating metrics dataframe
2023-03-04 17:09:28,871:INFO:Initializing Light Gradient Boosting Machine
2023-03-04 17:09:28,871:INFO:Total runtime is 0.3726468841234843 minutes
2023-03-04 17:09:28,875:INFO:SubProcess create_model() called ==================================
2023-03-04 17:09:28,876:INFO:Initializing create_model()
2023-03-04 17:09:28,876:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC14B3A60>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BD2B364F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:09:28,876:INFO:Checking exceptions
2023-03-04 17:09:28,876:INFO:Importing libraries
2023-03-04 17:09:28,876:INFO:Copying training dataset
2023-03-04 17:09:28,883:INFO:Defining folds
2023-03-04 17:09:28,883:INFO:Declaring metric variables
2023-03-04 17:09:28,889:INFO:Importing untrained model
2023-03-04 17:09:28,895:INFO:Light Gradient Boosting Machine Imported successfully
2023-03-04 17:09:28,905:INFO:Starting cross validation
2023-03-04 17:09:28,907:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:09:31,306:INFO:Calculating mean and std
2023-03-04 17:09:31,308:INFO:Creating metrics dataframe
2023-03-04 17:09:31,313:INFO:Uploading results into container
2023-03-04 17:09:31,313:INFO:Uploading model into container now
2023-03-04 17:09:31,314:INFO:_master_model_container: 18
2023-03-04 17:09:31,314:INFO:_display_container: 2
2023-03-04 17:09:31,314:INFO:LGBMRegressor(random_state=1516)
2023-03-04 17:09:31,314:INFO:create_model() successfully completed......................................
2023-03-04 17:09:31,430:INFO:SubProcess create_model() end ==================================
2023-03-04 17:09:31,431:INFO:Creating metrics dataframe
2023-03-04 17:09:31,445:INFO:Initializing Dummy Regressor
2023-03-04 17:09:31,446:INFO:Total runtime is 0.41556534767150877 minutes
2023-03-04 17:09:31,451:INFO:SubProcess create_model() called ==================================
2023-03-04 17:09:31,451:INFO:Initializing create_model()
2023-03-04 17:09:31,451:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC14B3A60>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011BD2B364F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:09:31,451:INFO:Checking exceptions
2023-03-04 17:09:31,452:INFO:Importing libraries
2023-03-04 17:09:31,452:INFO:Copying training dataset
2023-03-04 17:09:31,462:INFO:Defining folds
2023-03-04 17:09:31,462:INFO:Declaring metric variables
2023-03-04 17:09:31,467:INFO:Importing untrained model
2023-03-04 17:09:31,473:INFO:Dummy Regressor Imported successfully
2023-03-04 17:09:31,483:INFO:Starting cross validation
2023-03-04 17:09:31,485:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:09:31,819:INFO:Calculating mean and std
2023-03-04 17:09:31,821:INFO:Creating metrics dataframe
2023-03-04 17:09:31,825:INFO:Uploading results into container
2023-03-04 17:09:31,826:INFO:Uploading model into container now
2023-03-04 17:09:31,827:INFO:_master_model_container: 19
2023-03-04 17:09:31,827:INFO:_display_container: 2
2023-03-04 17:09:31,827:INFO:DummyRegressor()
2023-03-04 17:09:31,827:INFO:create_model() successfully completed......................................
2023-03-04 17:09:31,947:INFO:SubProcess create_model() end ==================================
2023-03-04 17:09:31,947:INFO:Creating metrics dataframe
2023-03-04 17:09:31,974:INFO:Initializing create_model()
2023-03-04 17:09:31,974:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC14B3A60>, estimator=Ridge(random_state=1516), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:09:31,974:INFO:Checking exceptions
2023-03-04 17:09:31,977:INFO:Importing libraries
2023-03-04 17:09:31,977:INFO:Copying training dataset
2023-03-04 17:09:31,983:INFO:Defining folds
2023-03-04 17:09:31,983:INFO:Declaring metric variables
2023-03-04 17:09:31,983:INFO:Importing untrained model
2023-03-04 17:09:31,983:INFO:Declaring custom model
2023-03-04 17:09:31,984:INFO:Ridge Regression Imported successfully
2023-03-04 17:09:31,985:INFO:Cross validation set to False
2023-03-04 17:09:31,985:INFO:Fitting Model
2023-03-04 17:09:32,035:INFO:Ridge(random_state=1516)
2023-03-04 17:09:32,035:INFO:create_model() successfully completed......................................
2023-03-04 17:09:32,202:INFO:_master_model_container: 19
2023-03-04 17:09:32,203:INFO:_display_container: 2
2023-03-04 17:09:32,203:INFO:Ridge(random_state=1516)
2023-03-04 17:09:32,203:INFO:compare_models() successfully completed......................................
2023-03-04 17:09:32,214:INFO:Initializing evaluate_model()
2023-03-04 17:09:32,214:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC14B3A60>, estimator=Ridge(random_state=1516), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-03-04 17:09:32,243:INFO:Initializing plot_model()
2023-03-04 17:09:32,244:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=Ridge(random_state=1516), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC14B3A60>, system=True)
2023-03-04 17:09:32,244:INFO:Checking exceptions
2023-03-04 17:09:32,247:INFO:Preloading libraries
2023-03-04 17:09:32,247:INFO:Copying training dataset
2023-03-04 17:09:32,247:INFO:Plot type: pipeline
2023-03-04 17:09:32,334:INFO:Visual Rendered Successfully
2023-03-04 17:09:32,449:INFO:plot_model() successfully completed......................................
2023-03-04 17:09:32,460:INFO:Initializing predict_model()
2023-03-04 17:09:32,460:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC14B3A60>, estimator=Ridge(random_state=1516), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x0000011BD25959D0>)
2023-03-04 17:09:32,460:INFO:Checking exceptions
2023-03-04 17:09:32,461:INFO:Preloading libraries
2023-03-04 17:09:32,463:INFO:Set up data.
2023-03-04 17:11:02,952:INFO:Initializing predict_model()
2023-03-04 17:11:02,952:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC14B3A60>, estimator=Ridge(random_state=1516), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x0000011BD36131F0>)
2023-03-04 17:11:02,952:INFO:Checking exceptions
2023-03-04 17:11:02,952:INFO:Preloading libraries
2023-03-04 17:11:02,955:INFO:Set up data.
2023-03-04 17:13:19,328:INFO:Initializing predict_model()
2023-03-04 17:13:19,328:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC14B3A60>, estimator=Ridge(random_state=1516), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x0000011BD3613D30>)
2023-03-04 17:13:19,329:INFO:Checking exceptions
2023-03-04 17:13:19,329:INFO:Preloading libraries
2023-03-04 17:13:19,331:INFO:Set up data.
2023-03-04 17:13:19,363:INFO:Set up index.
2023-03-04 17:25:59,530:INFO:Initializing predict_model()
2023-03-04 17:25:59,530:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000011BC14B3A60>, estimator=Ridge(random_state=1516), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x0000011BD2C70310>)
2023-03-04 17:25:59,530:INFO:Checking exceptions
2023-03-04 17:25:59,530:INFO:Preloading libraries
2023-03-04 17:25:59,533:INFO:Set up data.
2023-03-04 17:25:59,574:INFO:Set up index.
2023-03-04 17:42:39,437:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-04 17:42:39,437:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-04 17:42:39,437:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-04 17:42:39,437:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-04 17:42:40,118:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-03-04 17:42:48,044:INFO:PyCaret RegressionExperiment
2023-03-04 17:42:48,044:INFO:Logging name: reg-default-name
2023-03-04 17:42:48,044:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-04 17:42:48,044:INFO:version 3.0.0.rc9
2023-03-04 17:42:48,044:INFO:Initializing setup()
2023-03-04 17:42:48,044:INFO:self.USI: 9cf2
2023-03-04 17:42:48,044:INFO:self._variable_keys: {'seed', 'transform_target_param', 'fold_groups_param', 'fold_generator', 'memory', 'fold_shuffle_param', 'USI', 'y_train', 'y', 'X_train', 'data', 'gpu_n_jobs_param', 'y_test', 'html_param', 'exp_name_log', 'X_test', 'target_param', 'log_plots_param', 'exp_id', '_available_plots', '_ml_usecase', 'idx', 'pipeline', 'gpu_param', 'n_jobs_param', 'logging_param', 'X'}
2023-03-04 17:42:48,044:INFO:Checking environment
2023-03-04 17:42:48,044:INFO:python_version: 3.9.13
2023-03-04 17:42:48,044:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-03-04 17:42:48,044:INFO:machine: AMD64
2023-03-04 17:42:48,044:INFO:platform: Windows-10-10.0.19044-SP0
2023-03-04 17:42:48,045:INFO:Memory: svmem(total=17009516544, available=4563148800, percent=73.2, used=12446367744, free=4563148800)
2023-03-04 17:42:48,045:INFO:Physical Core: 4
2023-03-04 17:42:48,045:INFO:Logical Core: 8
2023-03-04 17:42:48,045:INFO:Checking libraries
2023-03-04 17:42:48,045:INFO:System:
2023-03-04 17:42:48,045:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-03-04 17:42:48,045:INFO:executable: C:\Users\Alvaro\anaconda3\python.exe
2023-03-04 17:42:48,045:INFO:   machine: Windows-10-10.0.19044-SP0
2023-03-04 17:42:48,045:INFO:PyCaret required dependencies:
2023-03-04 17:42:48,045:INFO:                 pip: 22.2.2
2023-03-04 17:42:48,045:INFO:          setuptools: 63.4.1
2023-03-04 17:42:48,045:INFO:             pycaret: 3.0.0rc9
2023-03-04 17:42:48,045:INFO:             IPython: 7.31.1
2023-03-04 17:42:48,045:INFO:          ipywidgets: 7.6.5
2023-03-04 17:42:48,045:INFO:                tqdm: 4.64.1
2023-03-04 17:42:48,045:INFO:               numpy: 1.21.5
2023-03-04 17:42:48,046:INFO:              pandas: 1.4.4
2023-03-04 17:42:48,046:INFO:              jinja2: 2.11.3
2023-03-04 17:42:48,046:INFO:               scipy: 1.9.1
2023-03-04 17:42:48,046:INFO:              joblib: 1.2.0
2023-03-04 17:42:48,046:INFO:             sklearn: 1.0.2
2023-03-04 17:42:48,046:INFO:                pyod: 1.0.7
2023-03-04 17:42:48,046:INFO:            imblearn: 0.10.1
2023-03-04 17:42:48,046:INFO:   category_encoders: 2.6.0
2023-03-04 17:42:48,046:INFO:            lightgbm: 3.3.5
2023-03-04 17:42:48,046:INFO:               numba: 0.55.1
2023-03-04 17:42:48,046:INFO:            requests: 2.28.1
2023-03-04 17:42:48,046:INFO:          matplotlib: 3.5.2
2023-03-04 17:42:48,046:INFO:          scikitplot: 0.3.7
2023-03-04 17:42:48,046:INFO:         yellowbrick: 1.5
2023-03-04 17:42:48,046:INFO:              plotly: 5.9.0
2023-03-04 17:42:48,046:INFO:             kaleido: 0.2.1
2023-03-04 17:42:48,046:INFO:         statsmodels: 0.13.2
2023-03-04 17:42:48,046:INFO:              sktime: 0.16.1
2023-03-04 17:42:48,047:INFO:               tbats: 1.1.2
2023-03-04 17:42:48,047:INFO:            pmdarima: 2.0.2
2023-03-04 17:42:48,047:INFO:              psutil: 5.9.0
2023-03-04 17:42:48,047:INFO:PyCaret optional dependencies:
2023-03-04 17:42:48,068:INFO:                shap: Not installed
2023-03-04 17:42:48,069:INFO:           interpret: Not installed
2023-03-04 17:42:48,069:INFO:                umap: Not installed
2023-03-04 17:42:48,069:INFO:    pandas_profiling: Not installed
2023-03-04 17:42:48,069:INFO:  explainerdashboard: Not installed
2023-03-04 17:42:48,069:INFO:             autoviz: Not installed
2023-03-04 17:42:48,069:INFO:           fairlearn: Not installed
2023-03-04 17:42:48,069:INFO:             xgboost: 1.7.4
2023-03-04 17:42:48,069:INFO:            catboost: Not installed
2023-03-04 17:42:48,069:INFO:              kmodes: Not installed
2023-03-04 17:42:48,069:INFO:             mlxtend: Not installed
2023-03-04 17:42:48,069:INFO:       statsforecast: Not installed
2023-03-04 17:42:48,069:INFO:        tune_sklearn: Not installed
2023-03-04 17:42:48,069:INFO:                 ray: Not installed
2023-03-04 17:42:48,069:INFO:            hyperopt: Not installed
2023-03-04 17:42:48,069:INFO:              optuna: Not installed
2023-03-04 17:42:48,069:INFO:               skopt: Not installed
2023-03-04 17:42:48,069:INFO:              mlflow: Not installed
2023-03-04 17:42:48,069:INFO:              gradio: Not installed
2023-03-04 17:42:48,069:INFO:             fastapi: Not installed
2023-03-04 17:42:48,069:INFO:             uvicorn: Not installed
2023-03-04 17:42:48,069:INFO:              m2cgen: Not installed
2023-03-04 17:42:48,069:INFO:           evidently: Not installed
2023-03-04 17:42:48,069:INFO:               fugue: Not installed
2023-03-04 17:42:48,069:INFO:           streamlit: Not installed
2023-03-04 17:42:48,070:INFO:             prophet: Not installed
2023-03-04 17:42:48,070:INFO:None
2023-03-04 17:42:48,070:INFO:Set up data.
2023-03-04 17:42:48,076:INFO:Set up train/test split.
2023-03-04 17:42:48,081:INFO:Set up index.
2023-03-04 17:42:48,081:INFO:Set up folding strategy.
2023-03-04 17:42:48,081:INFO:Assigning column types.
2023-03-04 17:42:48,084:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-04 17:42:48,084:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-04 17:42:48,088:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-04 17:42:48,092:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-04 17:42:48,153:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:42:48,194:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-04 17:42:48,195:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:42:48,229:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:42:48,230:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-04 17:42:48,237:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-04 17:42:48,244:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-04 17:42:48,302:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:42:48,342:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-04 17:42:48,343:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:42:48,345:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:42:48,346:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-03-04 17:42:48,350:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-04 17:42:48,354:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-04 17:42:48,407:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:42:48,448:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-04 17:42:48,448:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:42:48,451:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:42:48,455:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-04 17:42:48,460:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-04 17:42:48,513:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:42:48,554:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-04 17:42:48,555:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:42:48,557:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:42:48,557:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-03-04 17:42:48,566:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-04 17:42:48,619:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:42:48,660:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-04 17:42:48,661:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:42:48,663:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:42:48,672:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-04 17:42:48,723:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:42:48,763:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-04 17:42:48,764:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:42:48,766:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:42:48,767:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-03-04 17:42:48,827:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:42:48,868:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-04 17:42:48,869:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:42:48,871:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:42:48,932:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:42:48,973:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-04 17:42:48,973:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:42:48,976:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:42:48,976:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-04 17:42:49,037:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:42:49,078:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:42:49,080:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:42:49,143:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-04 17:42:49,184:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:42:49,187:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:42:49,187:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-03-04 17:42:49,296:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:42:49,298:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:42:49,403:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:42:49,405:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:42:49,407:INFO:Preparing preprocessing pipeline...
2023-03-04 17:42:49,408:INFO:Set up simple imputation.
2023-03-04 17:42:49,410:INFO:Set up encoding of categorical features.
2023-03-04 17:42:49,503:INFO:Finished creating preprocessing pipeline.
2023-03-04 17:42:49,513:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Alvaro\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['work_year', 'remote_ratio'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['experience_level',
                                             'employment_type', 'job_title',
                                             'employee_residence',
                                             'company_location',
                                             'company_size'],
                                    t...
                                    transformer=OneHotEncoder(cols=['experience_level',
                                                                    'employment_type',
                                                                    'company_size'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['job_title', 'employee_residence',
                                             'company_location'],
                                    transformer=LeaveOneOutEncoder(cols=['job_title',
                                                                         'employee_residence',
                                                                         'company_location'],
                                                                   handle_missing='return_nan',
                                                                   random_state=4226)))])
2023-03-04 17:42:49,513:INFO:Creating final display dataframe.
2023-03-04 17:42:49,919:INFO:Setup _display_container:                     Description             Value
0                    Session id              4226
1                        Target     salary_in_usd
2                   Target type        Regression
3           Original data shape          (500, 9)
4        Transformed data shape         (500, 17)
5   Transformed train set shape         (350, 17)
6    Transformed test set shape         (150, 17)
7              Numeric features                 2
8          Categorical features                 6
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator             KFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  reg-default-name
21                          USI              9cf2
2023-03-04 17:42:50,044:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:42:50,046:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:42:50,150:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-04 17:42:50,152:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-04 17:42:50,153:INFO:setup() successfully completed in 2.14s...............
2023-03-04 17:43:08,538:INFO:Initializing compare_models()
2023-03-04 17:43:08,538:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000248F5A0B940>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x00000248F5A0B940>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-03-04 17:43:08,539:INFO:Checking exceptions
2023-03-04 17:43:08,542:INFO:Preparing display monitor
2023-03-04 17:43:08,590:INFO:Initializing Linear Regression
2023-03-04 17:43:08,591:INFO:Total runtime is 1.6641616821289062e-05 minutes
2023-03-04 17:43:08,598:INFO:SubProcess create_model() called ==================================
2023-03-04 17:43:08,598:INFO:Initializing create_model()
2023-03-04 17:43:08,598:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000248F5A0B940>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000248FB98F7F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:43:08,598:INFO:Checking exceptions
2023-03-04 17:43:08,599:INFO:Importing libraries
2023-03-04 17:43:08,599:INFO:Copying training dataset
2023-03-04 17:43:08,606:INFO:Defining folds
2023-03-04 17:43:08,606:INFO:Declaring metric variables
2023-03-04 17:43:08,609:INFO:Importing untrained model
2023-03-04 17:43:08,613:INFO:Linear Regression Imported successfully
2023-03-04 17:43:08,619:INFO:Starting cross validation
2023-03-04 17:43:08,626:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:43:17,562:INFO:Calculating mean and std
2023-03-04 17:43:17,563:INFO:Creating metrics dataframe
2023-03-04 17:43:17,566:INFO:Uploading results into container
2023-03-04 17:43:17,567:INFO:Uploading model into container now
2023-03-04 17:43:17,567:INFO:_master_model_container: 1
2023-03-04 17:43:17,568:INFO:_display_container: 2
2023-03-04 17:43:17,568:INFO:LinearRegression(n_jobs=-1)
2023-03-04 17:43:17,568:INFO:create_model() successfully completed......................................
2023-03-04 17:43:17,721:INFO:SubProcess create_model() end ==================================
2023-03-04 17:43:17,721:INFO:Creating metrics dataframe
2023-03-04 17:43:17,728:INFO:Initializing Lasso Regression
2023-03-04 17:43:17,728:INFO:Total runtime is 0.15230112473169963 minutes
2023-03-04 17:43:17,731:INFO:SubProcess create_model() called ==================================
2023-03-04 17:43:17,732:INFO:Initializing create_model()
2023-03-04 17:43:17,732:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000248F5A0B940>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000248FB98F7F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:43:17,732:INFO:Checking exceptions
2023-03-04 17:43:17,732:INFO:Importing libraries
2023-03-04 17:43:17,732:INFO:Copying training dataset
2023-03-04 17:43:17,737:INFO:Defining folds
2023-03-04 17:43:17,738:INFO:Declaring metric variables
2023-03-04 17:43:17,741:INFO:Importing untrained model
2023-03-04 17:43:17,745:INFO:Lasso Regression Imported successfully
2023-03-04 17:43:17,753:INFO:Starting cross validation
2023-03-04 17:43:17,755:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:43:18,532:INFO:Calculating mean and std
2023-03-04 17:43:18,534:INFO:Creating metrics dataframe
2023-03-04 17:43:18,537:INFO:Uploading results into container
2023-03-04 17:43:18,538:INFO:Uploading model into container now
2023-03-04 17:43:18,538:INFO:_master_model_container: 2
2023-03-04 17:43:18,538:INFO:_display_container: 2
2023-03-04 17:43:18,538:INFO:Lasso(random_state=4226)
2023-03-04 17:43:18,538:INFO:create_model() successfully completed......................................
2023-03-04 17:43:18,692:INFO:SubProcess create_model() end ==================================
2023-03-04 17:43:18,693:INFO:Creating metrics dataframe
2023-03-04 17:43:18,700:INFO:Initializing Ridge Regression
2023-03-04 17:43:18,700:INFO:Total runtime is 0.16849033832550048 minutes
2023-03-04 17:43:18,703:INFO:SubProcess create_model() called ==================================
2023-03-04 17:43:18,704:INFO:Initializing create_model()
2023-03-04 17:43:18,704:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000248F5A0B940>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000248FB98F7F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:43:18,704:INFO:Checking exceptions
2023-03-04 17:43:18,704:INFO:Importing libraries
2023-03-04 17:43:18,704:INFO:Copying training dataset
2023-03-04 17:43:18,710:INFO:Defining folds
2023-03-04 17:43:18,710:INFO:Declaring metric variables
2023-03-04 17:43:18,713:INFO:Importing untrained model
2023-03-04 17:43:18,717:INFO:Ridge Regression Imported successfully
2023-03-04 17:43:18,724:INFO:Starting cross validation
2023-03-04 17:43:18,726:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:43:19,610:INFO:Calculating mean and std
2023-03-04 17:43:19,612:INFO:Creating metrics dataframe
2023-03-04 17:43:19,616:INFO:Uploading results into container
2023-03-04 17:43:19,616:INFO:Uploading model into container now
2023-03-04 17:43:19,617:INFO:_master_model_container: 3
2023-03-04 17:43:19,617:INFO:_display_container: 2
2023-03-04 17:43:19,617:INFO:Ridge(random_state=4226)
2023-03-04 17:43:19,617:INFO:create_model() successfully completed......................................
2023-03-04 17:43:19,794:INFO:SubProcess create_model() end ==================================
2023-03-04 17:43:19,794:INFO:Creating metrics dataframe
2023-03-04 17:43:19,803:INFO:Initializing Elastic Net
2023-03-04 17:43:19,803:INFO:Total runtime is 0.18688262303670247 minutes
2023-03-04 17:43:19,806:INFO:SubProcess create_model() called ==================================
2023-03-04 17:43:19,806:INFO:Initializing create_model()
2023-03-04 17:43:19,806:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000248F5A0B940>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000248FB98F7F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:43:19,806:INFO:Checking exceptions
2023-03-04 17:43:19,807:INFO:Importing libraries
2023-03-04 17:43:19,807:INFO:Copying training dataset
2023-03-04 17:43:19,813:INFO:Defining folds
2023-03-04 17:43:19,814:INFO:Declaring metric variables
2023-03-04 17:43:19,819:INFO:Importing untrained model
2023-03-04 17:43:19,824:INFO:Elastic Net Imported successfully
2023-03-04 17:43:19,832:INFO:Starting cross validation
2023-03-04 17:43:19,833:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:43:20,825:INFO:Calculating mean and std
2023-03-04 17:43:20,827:INFO:Creating metrics dataframe
2023-03-04 17:43:20,830:INFO:Uploading results into container
2023-03-04 17:43:20,830:INFO:Uploading model into container now
2023-03-04 17:43:20,831:INFO:_master_model_container: 4
2023-03-04 17:43:20,831:INFO:_display_container: 2
2023-03-04 17:43:20,831:INFO:ElasticNet(random_state=4226)
2023-03-04 17:43:20,831:INFO:create_model() successfully completed......................................
2023-03-04 17:43:20,996:INFO:SubProcess create_model() end ==================================
2023-03-04 17:43:20,996:INFO:Creating metrics dataframe
2023-03-04 17:43:21,005:INFO:Initializing Least Angle Regression
2023-03-04 17:43:21,005:INFO:Total runtime is 0.20691858132680258 minutes
2023-03-04 17:43:21,008:INFO:SubProcess create_model() called ==================================
2023-03-04 17:43:21,009:INFO:Initializing create_model()
2023-03-04 17:43:21,009:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000248F5A0B940>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000248FB98F7F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:43:21,009:INFO:Checking exceptions
2023-03-04 17:43:21,009:INFO:Importing libraries
2023-03-04 17:43:21,009:INFO:Copying training dataset
2023-03-04 17:43:21,015:INFO:Defining folds
2023-03-04 17:43:21,016:INFO:Declaring metric variables
2023-03-04 17:43:21,020:INFO:Importing untrained model
2023-03-04 17:43:21,026:INFO:Least Angle Regression Imported successfully
2023-03-04 17:43:21,036:INFO:Starting cross validation
2023-03-04 17:43:21,038:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:43:21,377:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:43:21,377:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:43:21,378:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:43:21,378:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:43:21,378:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:43:21,414:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:43:21,429:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:43:21,462:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:43:21,828:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:43:21,845:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:43:21,909:INFO:Calculating mean and std
2023-03-04 17:43:21,911:INFO:Creating metrics dataframe
2023-03-04 17:43:21,914:INFO:Uploading results into container
2023-03-04 17:43:21,915:INFO:Uploading model into container now
2023-03-04 17:43:21,915:INFO:_master_model_container: 5
2023-03-04 17:43:21,916:INFO:_display_container: 2
2023-03-04 17:43:21,916:INFO:Lars(random_state=4226)
2023-03-04 17:43:21,916:INFO:create_model() successfully completed......................................
2023-03-04 17:43:22,080:INFO:SubProcess create_model() end ==================================
2023-03-04 17:43:22,081:INFO:Creating metrics dataframe
2023-03-04 17:43:22,089:INFO:Initializing Lasso Least Angle Regression
2023-03-04 17:43:22,089:INFO:Total runtime is 0.22498721679051717 minutes
2023-03-04 17:43:22,092:INFO:SubProcess create_model() called ==================================
2023-03-04 17:43:22,093:INFO:Initializing create_model()
2023-03-04 17:43:22,093:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000248F5A0B940>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000248FB98F7F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:43:22,093:INFO:Checking exceptions
2023-03-04 17:43:22,093:INFO:Importing libraries
2023-03-04 17:43:22,093:INFO:Copying training dataset
2023-03-04 17:43:22,100:INFO:Defining folds
2023-03-04 17:43:22,100:INFO:Declaring metric variables
2023-03-04 17:43:22,104:INFO:Importing untrained model
2023-03-04 17:43:22,109:INFO:Lasso Least Angle Regression Imported successfully
2023-03-04 17:43:22,117:INFO:Starting cross validation
2023-03-04 17:43:22,118:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:43:22,441:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-04 17:43:22,461:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-04 17:43:22,475:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-04 17:43:22,493:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-04 17:43:22,495:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-04 17:43:22,517:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-04 17:43:22,555:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-04 17:43:22,562:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-04 17:43:22,900:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-04 17:43:22,913:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-04 17:43:22,974:INFO:Calculating mean and std
2023-03-04 17:43:22,975:INFO:Creating metrics dataframe
2023-03-04 17:43:22,978:INFO:Uploading results into container
2023-03-04 17:43:22,979:INFO:Uploading model into container now
2023-03-04 17:43:22,979:INFO:_master_model_container: 6
2023-03-04 17:43:22,979:INFO:_display_container: 2
2023-03-04 17:43:22,980:INFO:LassoLars(random_state=4226)
2023-03-04 17:43:22,980:INFO:create_model() successfully completed......................................
2023-03-04 17:43:23,142:INFO:SubProcess create_model() end ==================================
2023-03-04 17:43:23,142:INFO:Creating metrics dataframe
2023-03-04 17:43:23,151:INFO:Initializing Orthogonal Matching Pursuit
2023-03-04 17:43:23,151:INFO:Total runtime is 0.24267954826354982 minutes
2023-03-04 17:43:23,154:INFO:SubProcess create_model() called ==================================
2023-03-04 17:43:23,154:INFO:Initializing create_model()
2023-03-04 17:43:23,155:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000248F5A0B940>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000248FB98F7F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:43:23,155:INFO:Checking exceptions
2023-03-04 17:43:23,155:INFO:Importing libraries
2023-03-04 17:43:23,155:INFO:Copying training dataset
2023-03-04 17:43:23,160:INFO:Defining folds
2023-03-04 17:43:23,160:INFO:Declaring metric variables
2023-03-04 17:43:23,165:INFO:Importing untrained model
2023-03-04 17:43:23,169:INFO:Orthogonal Matching Pursuit Imported successfully
2023-03-04 17:43:23,178:INFO:Starting cross validation
2023-03-04 17:43:23,180:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:43:23,451:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:43:23,478:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:43:23,491:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:43:23,494:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:43:23,504:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:43:23,545:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:43:23,547:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:43:23,567:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:43:23,906:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:43:23,912:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-04 17:43:23,975:INFO:Calculating mean and std
2023-03-04 17:43:23,977:INFO:Creating metrics dataframe
2023-03-04 17:43:23,980:INFO:Uploading results into container
2023-03-04 17:43:23,980:INFO:Uploading model into container now
2023-03-04 17:43:23,981:INFO:_master_model_container: 7
2023-03-04 17:43:23,981:INFO:_display_container: 2
2023-03-04 17:43:23,981:INFO:OrthogonalMatchingPursuit()
2023-03-04 17:43:23,981:INFO:create_model() successfully completed......................................
2023-03-04 17:43:24,144:INFO:SubProcess create_model() end ==================================
2023-03-04 17:43:24,145:INFO:Creating metrics dataframe
2023-03-04 17:43:24,153:INFO:Initializing Bayesian Ridge
2023-03-04 17:43:24,154:INFO:Total runtime is 0.2593921899795532 minutes
2023-03-04 17:43:24,157:INFO:SubProcess create_model() called ==================================
2023-03-04 17:43:24,157:INFO:Initializing create_model()
2023-03-04 17:43:24,157:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000248F5A0B940>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000248FB98F7F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:43:24,157:INFO:Checking exceptions
2023-03-04 17:43:24,157:INFO:Importing libraries
2023-03-04 17:43:24,157:INFO:Copying training dataset
2023-03-04 17:43:24,163:INFO:Defining folds
2023-03-04 17:43:24,163:INFO:Declaring metric variables
2023-03-04 17:43:24,169:INFO:Importing untrained model
2023-03-04 17:43:24,175:INFO:Bayesian Ridge Imported successfully
2023-03-04 17:43:24,183:INFO:Starting cross validation
2023-03-04 17:43:24,186:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:43:24,999:INFO:Calculating mean and std
2023-03-04 17:43:25,001:INFO:Creating metrics dataframe
2023-03-04 17:43:25,004:INFO:Uploading results into container
2023-03-04 17:43:25,005:INFO:Uploading model into container now
2023-03-04 17:43:25,005:INFO:_master_model_container: 8
2023-03-04 17:43:25,005:INFO:_display_container: 2
2023-03-04 17:43:25,005:INFO:BayesianRidge()
2023-03-04 17:43:25,005:INFO:create_model() successfully completed......................................
2023-03-04 17:43:25,164:INFO:SubProcess create_model() end ==================================
2023-03-04 17:43:25,164:INFO:Creating metrics dataframe
2023-03-04 17:43:25,175:INFO:Initializing Passive Aggressive Regressor
2023-03-04 17:43:25,175:INFO:Total runtime is 0.2764112869898478 minutes
2023-03-04 17:43:25,180:INFO:SubProcess create_model() called ==================================
2023-03-04 17:43:25,181:INFO:Initializing create_model()
2023-03-04 17:43:25,181:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000248F5A0B940>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000248FB98F7F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:43:25,181:INFO:Checking exceptions
2023-03-04 17:43:25,181:INFO:Importing libraries
2023-03-04 17:43:25,181:INFO:Copying training dataset
2023-03-04 17:43:25,188:INFO:Defining folds
2023-03-04 17:43:25,188:INFO:Declaring metric variables
2023-03-04 17:43:25,192:INFO:Importing untrained model
2023-03-04 17:43:25,198:INFO:Passive Aggressive Regressor Imported successfully
2023-03-04 17:43:25,207:INFO:Starting cross validation
2023-03-04 17:43:25,209:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:43:25,998:INFO:Calculating mean and std
2023-03-04 17:43:25,999:INFO:Creating metrics dataframe
2023-03-04 17:43:26,002:INFO:Uploading results into container
2023-03-04 17:43:26,003:INFO:Uploading model into container now
2023-03-04 17:43:26,003:INFO:_master_model_container: 9
2023-03-04 17:43:26,003:INFO:_display_container: 2
2023-03-04 17:43:26,004:INFO:PassiveAggressiveRegressor(random_state=4226)
2023-03-04 17:43:26,004:INFO:create_model() successfully completed......................................
2023-03-04 17:43:26,161:INFO:SubProcess create_model() end ==================================
2023-03-04 17:43:26,161:INFO:Creating metrics dataframe
2023-03-04 17:43:26,170:INFO:Initializing Huber Regressor
2023-03-04 17:43:26,171:INFO:Total runtime is 0.293017037709554 minutes
2023-03-04 17:43:26,173:INFO:SubProcess create_model() called ==================================
2023-03-04 17:43:26,174:INFO:Initializing create_model()
2023-03-04 17:43:26,174:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000248F5A0B940>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000248FB98F7F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:43:26,174:INFO:Checking exceptions
2023-03-04 17:43:26,174:INFO:Importing libraries
2023-03-04 17:43:26,174:INFO:Copying training dataset
2023-03-04 17:43:26,180:INFO:Defining folds
2023-03-04 17:43:26,180:INFO:Declaring metric variables
2023-03-04 17:43:26,184:INFO:Importing untrained model
2023-03-04 17:43:26,190:INFO:Huber Regressor Imported successfully
2023-03-04 17:43:26,197:INFO:Starting cross validation
2023-03-04 17:43:26,199:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:43:27,072:INFO:Calculating mean and std
2023-03-04 17:43:27,074:INFO:Creating metrics dataframe
2023-03-04 17:43:27,076:INFO:Uploading results into container
2023-03-04 17:43:27,077:INFO:Uploading model into container now
2023-03-04 17:43:27,077:INFO:_master_model_container: 10
2023-03-04 17:43:27,077:INFO:_display_container: 2
2023-03-04 17:43:27,078:INFO:HuberRegressor()
2023-03-04 17:43:27,078:INFO:create_model() successfully completed......................................
2023-03-04 17:43:27,234:INFO:SubProcess create_model() end ==================================
2023-03-04 17:43:27,234:INFO:Creating metrics dataframe
2023-03-04 17:43:27,246:INFO:Initializing K Neighbors Regressor
2023-03-04 17:43:27,246:INFO:Total runtime is 0.3109355886777242 minutes
2023-03-04 17:43:27,248:INFO:SubProcess create_model() called ==================================
2023-03-04 17:43:27,249:INFO:Initializing create_model()
2023-03-04 17:43:27,249:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000248F5A0B940>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000248FB98F7F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:43:27,249:INFO:Checking exceptions
2023-03-04 17:43:27,249:INFO:Importing libraries
2023-03-04 17:43:27,249:INFO:Copying training dataset
2023-03-04 17:43:27,256:INFO:Defining folds
2023-03-04 17:43:27,256:INFO:Declaring metric variables
2023-03-04 17:43:27,260:INFO:Importing untrained model
2023-03-04 17:43:27,265:INFO:K Neighbors Regressor Imported successfully
2023-03-04 17:43:27,275:INFO:Starting cross validation
2023-03-04 17:43:27,278:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:43:28,095:INFO:Calculating mean and std
2023-03-04 17:43:28,097:INFO:Creating metrics dataframe
2023-03-04 17:43:28,099:INFO:Uploading results into container
2023-03-04 17:43:28,100:INFO:Uploading model into container now
2023-03-04 17:43:28,100:INFO:_master_model_container: 11
2023-03-04 17:43:28,100:INFO:_display_container: 2
2023-03-04 17:43:28,101:INFO:KNeighborsRegressor(n_jobs=-1)
2023-03-04 17:43:28,101:INFO:create_model() successfully completed......................................
2023-03-04 17:43:28,262:INFO:SubProcess create_model() end ==================================
2023-03-04 17:43:28,262:INFO:Creating metrics dataframe
2023-03-04 17:43:28,272:INFO:Initializing Decision Tree Regressor
2023-03-04 17:43:28,273:INFO:Total runtime is 0.3280417124430339 minutes
2023-03-04 17:43:28,276:INFO:SubProcess create_model() called ==================================
2023-03-04 17:43:28,276:INFO:Initializing create_model()
2023-03-04 17:43:28,276:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000248F5A0B940>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000248FB98F7F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:43:28,276:INFO:Checking exceptions
2023-03-04 17:43:28,276:INFO:Importing libraries
2023-03-04 17:43:28,276:INFO:Copying training dataset
2023-03-04 17:43:28,281:INFO:Defining folds
2023-03-04 17:43:28,282:INFO:Declaring metric variables
2023-03-04 17:43:28,286:INFO:Importing untrained model
2023-03-04 17:43:28,290:INFO:Decision Tree Regressor Imported successfully
2023-03-04 17:43:28,299:INFO:Starting cross validation
2023-03-04 17:43:28,300:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:43:29,242:INFO:Calculating mean and std
2023-03-04 17:43:29,243:INFO:Creating metrics dataframe
2023-03-04 17:43:29,248:INFO:Uploading results into container
2023-03-04 17:43:29,249:INFO:Uploading model into container now
2023-03-04 17:43:29,249:INFO:_master_model_container: 12
2023-03-04 17:43:29,250:INFO:_display_container: 2
2023-03-04 17:43:29,250:INFO:DecisionTreeRegressor(random_state=4226)
2023-03-04 17:43:29,250:INFO:create_model() successfully completed......................................
2023-03-04 17:43:29,428:INFO:SubProcess create_model() end ==================================
2023-03-04 17:43:29,428:INFO:Creating metrics dataframe
2023-03-04 17:43:29,440:INFO:Initializing Random Forest Regressor
2023-03-04 17:43:29,440:INFO:Total runtime is 0.3474903305371603 minutes
2023-03-04 17:43:29,444:INFO:SubProcess create_model() called ==================================
2023-03-04 17:43:29,444:INFO:Initializing create_model()
2023-03-04 17:43:29,445:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000248F5A0B940>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000248FB98F7F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:43:29,445:INFO:Checking exceptions
2023-03-04 17:43:29,445:INFO:Importing libraries
2023-03-04 17:43:29,445:INFO:Copying training dataset
2023-03-04 17:43:29,451:INFO:Defining folds
2023-03-04 17:43:29,452:INFO:Declaring metric variables
2023-03-04 17:43:29,457:INFO:Importing untrained model
2023-03-04 17:43:29,462:INFO:Random Forest Regressor Imported successfully
2023-03-04 17:43:29,472:INFO:Starting cross validation
2023-03-04 17:43:29,474:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:43:31,922:INFO:Calculating mean and std
2023-03-04 17:43:31,924:INFO:Creating metrics dataframe
2023-03-04 17:43:31,928:INFO:Uploading results into container
2023-03-04 17:43:31,929:INFO:Uploading model into container now
2023-03-04 17:43:31,929:INFO:_master_model_container: 13
2023-03-04 17:43:31,929:INFO:_display_container: 2
2023-03-04 17:43:31,930:INFO:RandomForestRegressor(n_jobs=-1, random_state=4226)
2023-03-04 17:43:31,930:INFO:create_model() successfully completed......................................
2023-03-04 17:43:32,115:INFO:SubProcess create_model() end ==================================
2023-03-04 17:43:32,116:INFO:Creating metrics dataframe
2023-03-04 17:43:32,129:INFO:Initializing Extra Trees Regressor
2023-03-04 17:43:32,129:INFO:Total runtime is 0.39230887889862065 minutes
2023-03-04 17:43:32,133:INFO:SubProcess create_model() called ==================================
2023-03-04 17:43:32,133:INFO:Initializing create_model()
2023-03-04 17:43:32,133:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000248F5A0B940>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000248FB98F7F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:43:32,133:INFO:Checking exceptions
2023-03-04 17:43:32,133:INFO:Importing libraries
2023-03-04 17:43:32,133:INFO:Copying training dataset
2023-03-04 17:43:32,140:INFO:Defining folds
2023-03-04 17:43:32,140:INFO:Declaring metric variables
2023-03-04 17:43:32,145:INFO:Importing untrained model
2023-03-04 17:43:32,149:INFO:Extra Trees Regressor Imported successfully
2023-03-04 17:43:32,161:INFO:Starting cross validation
2023-03-04 17:43:32,165:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:43:34,625:INFO:Calculating mean and std
2023-03-04 17:43:34,627:INFO:Creating metrics dataframe
2023-03-04 17:43:34,631:INFO:Uploading results into container
2023-03-04 17:43:34,631:INFO:Uploading model into container now
2023-03-04 17:43:34,632:INFO:_master_model_container: 14
2023-03-04 17:43:34,632:INFO:_display_container: 2
2023-03-04 17:43:34,632:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=4226)
2023-03-04 17:43:34,632:INFO:create_model() successfully completed......................................
2023-03-04 17:43:34,807:INFO:SubProcess create_model() end ==================================
2023-03-04 17:43:34,808:INFO:Creating metrics dataframe
2023-03-04 17:43:34,820:INFO:Initializing AdaBoost Regressor
2023-03-04 17:43:34,820:INFO:Total runtime is 0.4371560136477153 minutes
2023-03-04 17:43:34,823:INFO:SubProcess create_model() called ==================================
2023-03-04 17:43:34,823:INFO:Initializing create_model()
2023-03-04 17:43:34,823:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000248F5A0B940>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000248FB98F7F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:43:34,823:INFO:Checking exceptions
2023-03-04 17:43:34,823:INFO:Importing libraries
2023-03-04 17:43:34,823:INFO:Copying training dataset
2023-03-04 17:43:34,828:INFO:Defining folds
2023-03-04 17:43:34,829:INFO:Declaring metric variables
2023-03-04 17:43:34,833:INFO:Importing untrained model
2023-03-04 17:43:34,839:INFO:AdaBoost Regressor Imported successfully
2023-03-04 17:43:34,848:INFO:Starting cross validation
2023-03-04 17:43:34,851:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:43:36,423:INFO:Calculating mean and std
2023-03-04 17:43:36,425:INFO:Creating metrics dataframe
2023-03-04 17:43:36,428:INFO:Uploading results into container
2023-03-04 17:43:36,429:INFO:Uploading model into container now
2023-03-04 17:43:36,429:INFO:_master_model_container: 15
2023-03-04 17:43:36,430:INFO:_display_container: 2
2023-03-04 17:43:36,430:INFO:AdaBoostRegressor(random_state=4226)
2023-03-04 17:43:36,430:INFO:create_model() successfully completed......................................
2023-03-04 17:43:36,604:INFO:SubProcess create_model() end ==================================
2023-03-04 17:43:36,604:INFO:Creating metrics dataframe
2023-03-04 17:43:36,619:INFO:Initializing Gradient Boosting Regressor
2023-03-04 17:43:36,619:INFO:Total runtime is 0.467148502667745 minutes
2023-03-04 17:43:36,622:INFO:SubProcess create_model() called ==================================
2023-03-04 17:43:36,623:INFO:Initializing create_model()
2023-03-04 17:43:36,623:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000248F5A0B940>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000248FB98F7F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:43:36,623:INFO:Checking exceptions
2023-03-04 17:43:36,623:INFO:Importing libraries
2023-03-04 17:43:36,623:INFO:Copying training dataset
2023-03-04 17:43:36,629:INFO:Defining folds
2023-03-04 17:43:36,629:INFO:Declaring metric variables
2023-03-04 17:43:36,635:INFO:Importing untrained model
2023-03-04 17:43:36,640:INFO:Gradient Boosting Regressor Imported successfully
2023-03-04 17:43:36,650:INFO:Starting cross validation
2023-03-04 17:43:36,653:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:43:38,315:INFO:Calculating mean and std
2023-03-04 17:43:38,318:INFO:Creating metrics dataframe
2023-03-04 17:43:38,321:INFO:Uploading results into container
2023-03-04 17:43:38,322:INFO:Uploading model into container now
2023-03-04 17:43:38,322:INFO:_master_model_container: 16
2023-03-04 17:43:38,322:INFO:_display_container: 2
2023-03-04 17:43:38,323:INFO:GradientBoostingRegressor(random_state=4226)
2023-03-04 17:43:38,323:INFO:create_model() successfully completed......................................
2023-03-04 17:43:38,490:INFO:SubProcess create_model() end ==================================
2023-03-04 17:43:38,490:INFO:Creating metrics dataframe
2023-03-04 17:43:38,505:INFO:Initializing Extreme Gradient Boosting
2023-03-04 17:43:38,505:INFO:Total runtime is 0.4985730568567912 minutes
2023-03-04 17:43:38,508:INFO:SubProcess create_model() called ==================================
2023-03-04 17:43:38,509:INFO:Initializing create_model()
2023-03-04 17:43:38,509:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000248F5A0B940>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000248FB98F7F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:43:38,509:INFO:Checking exceptions
2023-03-04 17:43:38,509:INFO:Importing libraries
2023-03-04 17:43:38,509:INFO:Copying training dataset
2023-03-04 17:43:38,516:INFO:Defining folds
2023-03-04 17:43:38,516:INFO:Declaring metric variables
2023-03-04 17:43:38,520:INFO:Importing untrained model
2023-03-04 17:43:38,526:INFO:Extreme Gradient Boosting Imported successfully
2023-03-04 17:43:38,536:INFO:Starting cross validation
2023-03-04 17:43:38,539:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:43:40,316:INFO:Calculating mean and std
2023-03-04 17:43:40,318:INFO:Creating metrics dataframe
2023-03-04 17:43:40,322:INFO:Uploading results into container
2023-03-04 17:43:40,322:INFO:Uploading model into container now
2023-03-04 17:43:40,323:INFO:_master_model_container: 17
2023-03-04 17:43:40,323:INFO:_display_container: 2
2023-03-04 17:43:40,324:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=4226, ...)
2023-03-04 17:43:40,324:INFO:create_model() successfully completed......................................
2023-03-04 17:43:40,491:INFO:SubProcess create_model() end ==================================
2023-03-04 17:43:40,491:INFO:Creating metrics dataframe
2023-03-04 17:43:40,506:INFO:Initializing Light Gradient Boosting Machine
2023-03-04 17:43:40,506:INFO:Total runtime is 0.531930152575175 minutes
2023-03-04 17:43:40,510:INFO:SubProcess create_model() called ==================================
2023-03-04 17:43:40,510:INFO:Initializing create_model()
2023-03-04 17:43:40,510:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000248F5A0B940>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000248FB98F7F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:43:40,510:INFO:Checking exceptions
2023-03-04 17:43:40,510:INFO:Importing libraries
2023-03-04 17:43:40,510:INFO:Copying training dataset
2023-03-04 17:43:40,516:INFO:Defining folds
2023-03-04 17:43:40,516:INFO:Declaring metric variables
2023-03-04 17:43:40,521:INFO:Importing untrained model
2023-03-04 17:43:40,528:INFO:Light Gradient Boosting Machine Imported successfully
2023-03-04 17:43:40,537:INFO:Starting cross validation
2023-03-04 17:43:40,540:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:43:43,842:INFO:Calculating mean and std
2023-03-04 17:43:43,844:INFO:Creating metrics dataframe
2023-03-04 17:43:43,847:INFO:Uploading results into container
2023-03-04 17:43:43,848:INFO:Uploading model into container now
2023-03-04 17:43:43,848:INFO:_master_model_container: 18
2023-03-04 17:43:43,848:INFO:_display_container: 2
2023-03-04 17:43:43,849:INFO:LGBMRegressor(random_state=4226)
2023-03-04 17:43:43,849:INFO:create_model() successfully completed......................................
2023-03-04 17:43:44,017:INFO:SubProcess create_model() end ==================================
2023-03-04 17:43:44,017:INFO:Creating metrics dataframe
2023-03-04 17:43:44,031:INFO:Initializing Dummy Regressor
2023-03-04 17:43:44,031:INFO:Total runtime is 0.5906835436820984 minutes
2023-03-04 17:43:44,036:INFO:SubProcess create_model() called ==================================
2023-03-04 17:43:44,036:INFO:Initializing create_model()
2023-03-04 17:43:44,036:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000248F5A0B940>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000248FB98F7F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:43:44,036:INFO:Checking exceptions
2023-03-04 17:43:44,037:INFO:Importing libraries
2023-03-04 17:43:44,037:INFO:Copying training dataset
2023-03-04 17:43:44,045:INFO:Defining folds
2023-03-04 17:43:44,046:INFO:Declaring metric variables
2023-03-04 17:43:44,049:INFO:Importing untrained model
2023-03-04 17:43:44,055:INFO:Dummy Regressor Imported successfully
2023-03-04 17:43:44,063:INFO:Starting cross validation
2023-03-04 17:43:44,065:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-04 17:43:45,012:INFO:Calculating mean and std
2023-03-04 17:43:45,015:INFO:Creating metrics dataframe
2023-03-04 17:43:45,019:INFO:Uploading results into container
2023-03-04 17:43:45,019:INFO:Uploading model into container now
2023-03-04 17:43:45,020:INFO:_master_model_container: 19
2023-03-04 17:43:45,020:INFO:_display_container: 2
2023-03-04 17:43:45,021:INFO:DummyRegressor()
2023-03-04 17:43:45,021:INFO:create_model() successfully completed......................................
2023-03-04 17:43:45,195:INFO:SubProcess create_model() end ==================================
2023-03-04 17:43:45,196:INFO:Creating metrics dataframe
2023-03-04 17:43:45,221:INFO:Initializing create_model()
2023-03-04 17:43:45,221:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000248F5A0B940>, estimator=Ridge(random_state=4226), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-04 17:43:45,222:INFO:Checking exceptions
2023-03-04 17:43:45,224:INFO:Importing libraries
2023-03-04 17:43:45,225:INFO:Copying training dataset
2023-03-04 17:43:45,229:INFO:Defining folds
2023-03-04 17:43:45,229:INFO:Declaring metric variables
2023-03-04 17:43:45,229:INFO:Importing untrained model
2023-03-04 17:43:45,230:INFO:Declaring custom model
2023-03-04 17:43:45,230:INFO:Ridge Regression Imported successfully
2023-03-04 17:43:45,232:INFO:Cross validation set to False
2023-03-04 17:43:45,232:INFO:Fitting Model
2023-03-04 17:43:45,373:INFO:Ridge(random_state=4226)
2023-03-04 17:43:45,373:INFO:create_model() successfully completed......................................
2023-03-04 17:43:45,584:INFO:_master_model_container: 19
2023-03-04 17:43:45,584:INFO:_display_container: 2
2023-03-04 17:43:45,585:INFO:Ridge(random_state=4226)
2023-03-04 17:43:45,585:INFO:compare_models() successfully completed......................................
2023-03-04 17:43:53,975:INFO:Initializing evaluate_model()
2023-03-04 17:43:53,975:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000248F5A0B940>, estimator=Ridge(random_state=4226), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-03-04 17:43:54,009:INFO:Initializing plot_model()
2023-03-04 17:43:54,009:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=Ridge(random_state=4226), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x00000248F5A0B940>, system=True)
2023-03-04 17:43:54,010:INFO:Checking exceptions
2023-03-04 17:43:54,013:INFO:Preloading libraries
2023-03-04 17:43:54,014:INFO:Copying training dataset
2023-03-04 17:43:54,015:INFO:Plot type: pipeline
2023-03-04 17:43:54,169:INFO:Visual Rendered Successfully
2023-03-04 17:43:54,323:INFO:plot_model() successfully completed......................................
2023-03-04 17:44:23,109:INFO:Initializing plot_model()
2023-03-04 17:44:23,109:INFO:plot_model(plot=parameter, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=Ridge(random_state=4226), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x00000248F5A0B940>, system=True)
2023-03-04 17:44:23,110:INFO:Checking exceptions
2023-03-04 17:44:23,112:INFO:Preloading libraries
2023-03-04 17:44:23,112:INFO:Copying training dataset
2023-03-04 17:44:23,112:INFO:Plot type: parameter
2023-03-04 17:44:23,115:INFO:Visual Rendered Successfully
2023-03-04 17:44:23,282:INFO:plot_model() successfully completed......................................
2023-03-04 17:44:24,351:INFO:Initializing plot_model()
2023-03-04 17:44:24,351:INFO:plot_model(plot=learning, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=Ridge(random_state=4226), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x00000248F5A0B940>, system=True)
2023-03-04 17:44:24,352:INFO:Checking exceptions
2023-03-04 17:44:24,354:INFO:Preloading libraries
2023-03-04 17:44:24,354:INFO:Copying training dataset
2023-03-04 17:44:24,354:INFO:Plot type: learning
2023-03-04 17:44:24,582:INFO:Fitting Model
2023-03-04 17:44:24,988:INFO:Visual Rendered Successfully
2023-03-04 17:44:25,163:INFO:plot_model() successfully completed......................................
2023-03-04 17:46:19,781:INFO:Initializing predict_model()
2023-03-04 17:46:19,781:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000248F5A0B940>, estimator=Ridge(random_state=4226), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x00000248FC96F940>)
2023-03-04 17:46:19,781:INFO:Checking exceptions
2023-03-04 17:46:19,781:INFO:Preloading libraries
2023-03-04 17:46:19,783:INFO:Set up data.
2023-03-04 17:46:19,787:INFO:Set up index.
2023-03-05 16:30:55,876:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-05 16:30:55,877:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-05 16:30:55,877:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-05 16:30:55,877:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-05 16:30:59,734:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-03-05 16:31:00,449:INFO:PyCaret RegressionExperiment
2023-03-05 16:31:00,450:INFO:Logging name: reg-default-name
2023-03-05 16:31:00,450:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-05 16:31:00,450:INFO:version 3.0.0.rc9
2023-03-05 16:31:00,450:INFO:Initializing setup()
2023-03-05 16:31:00,450:INFO:self.USI: ed0b
2023-03-05 16:31:00,450:INFO:self._variable_keys: {'logging_param', 'y_train', 'seed', 'fold_shuffle_param', 'X_train', 'fold_generator', '_ml_usecase', 'fold_groups_param', 'X', 'gpu_n_jobs_param', 'memory', 'USI', '_available_plots', 'X_test', 'exp_id', 'gpu_param', 'html_param', 'idx', 'y', 'target_param', 'pipeline', 'n_jobs_param', 'exp_name_log', 'transform_target_param', 'data', 'y_test', 'log_plots_param'}
2023-03-05 16:31:00,450:INFO:Checking environment
2023-03-05 16:31:00,450:INFO:python_version: 3.9.13
2023-03-05 16:31:00,450:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-03-05 16:31:00,450:INFO:machine: AMD64
2023-03-05 16:31:00,451:INFO:platform: Windows-10-10.0.19044-SP0
2023-03-05 16:31:00,451:INFO:Memory: svmem(total=17009516544, available=6246453248, percent=63.3, used=10763063296, free=6246453248)
2023-03-05 16:31:00,451:INFO:Physical Core: 4
2023-03-05 16:31:00,451:INFO:Logical Core: 8
2023-03-05 16:31:00,451:INFO:Checking libraries
2023-03-05 16:31:00,451:INFO:System:
2023-03-05 16:31:00,451:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-03-05 16:31:00,451:INFO:executable: C:\Users\Alvaro\anaconda3\python.exe
2023-03-05 16:31:00,451:INFO:   machine: Windows-10-10.0.19044-SP0
2023-03-05 16:31:00,451:INFO:PyCaret required dependencies:
2023-03-05 16:31:00,451:INFO:                 pip: 22.2.2
2023-03-05 16:31:00,451:INFO:          setuptools: 63.4.1
2023-03-05 16:31:00,451:INFO:             pycaret: 3.0.0rc9
2023-03-05 16:31:00,451:INFO:             IPython: 7.31.1
2023-03-05 16:31:00,452:INFO:          ipywidgets: 7.6.5
2023-03-05 16:31:00,452:INFO:                tqdm: 4.64.1
2023-03-05 16:31:00,452:INFO:               numpy: 1.21.5
2023-03-05 16:31:00,452:INFO:              pandas: 1.4.4
2023-03-05 16:31:00,452:INFO:              jinja2: 2.11.3
2023-03-05 16:31:00,452:INFO:               scipy: 1.9.1
2023-03-05 16:31:00,452:INFO:              joblib: 1.2.0
2023-03-05 16:31:00,452:INFO:             sklearn: 1.0.2
2023-03-05 16:31:00,452:INFO:                pyod: 1.0.7
2023-03-05 16:31:00,452:INFO:            imblearn: 0.10.1
2023-03-05 16:31:00,452:INFO:   category_encoders: 2.6.0
2023-03-05 16:31:00,452:INFO:            lightgbm: 3.3.5
2023-03-05 16:31:00,452:INFO:               numba: 0.55.1
2023-03-05 16:31:00,452:INFO:            requests: 2.28.1
2023-03-05 16:31:00,452:INFO:          matplotlib: 3.5.2
2023-03-05 16:31:00,452:INFO:          scikitplot: 0.3.7
2023-03-05 16:31:00,452:INFO:         yellowbrick: 1.5
2023-03-05 16:31:00,452:INFO:              plotly: 5.9.0
2023-03-05 16:31:00,452:INFO:             kaleido: 0.2.1
2023-03-05 16:31:00,452:INFO:         statsmodels: 0.13.2
2023-03-05 16:31:00,452:INFO:              sktime: 0.16.1
2023-03-05 16:31:00,453:INFO:               tbats: 1.1.2
2023-03-05 16:31:00,453:INFO:            pmdarima: 2.0.2
2023-03-05 16:31:00,453:INFO:              psutil: 5.9.0
2023-03-05 16:31:00,453:INFO:PyCaret optional dependencies:
2023-03-05 16:31:00,472:INFO:                shap: Not installed
2023-03-05 16:31:00,473:INFO:           interpret: Not installed
2023-03-05 16:31:00,473:INFO:                umap: Not installed
2023-03-05 16:31:00,473:INFO:    pandas_profiling: Not installed
2023-03-05 16:31:00,473:INFO:  explainerdashboard: Not installed
2023-03-05 16:31:00,473:INFO:             autoviz: Not installed
2023-03-05 16:31:00,473:INFO:           fairlearn: Not installed
2023-03-05 16:31:00,473:INFO:             xgboost: 1.7.4
2023-03-05 16:31:00,473:INFO:            catboost: Not installed
2023-03-05 16:31:00,473:INFO:              kmodes: Not installed
2023-03-05 16:31:00,473:INFO:             mlxtend: Not installed
2023-03-05 16:31:00,473:INFO:       statsforecast: Not installed
2023-03-05 16:31:00,473:INFO:        tune_sklearn: Not installed
2023-03-05 16:31:00,473:INFO:                 ray: Not installed
2023-03-05 16:31:00,473:INFO:            hyperopt: Not installed
2023-03-05 16:31:00,473:INFO:              optuna: Not installed
2023-03-05 16:31:00,473:INFO:               skopt: Not installed
2023-03-05 16:31:00,473:INFO:              mlflow: Not installed
2023-03-05 16:31:00,473:INFO:              gradio: Not installed
2023-03-05 16:31:00,473:INFO:             fastapi: Not installed
2023-03-05 16:31:00,473:INFO:             uvicorn: Not installed
2023-03-05 16:31:00,473:INFO:              m2cgen: Not installed
2023-03-05 16:31:00,473:INFO:           evidently: Not installed
2023-03-05 16:31:00,473:INFO:               fugue: Not installed
2023-03-05 16:31:00,474:INFO:           streamlit: Not installed
2023-03-05 16:31:00,474:INFO:             prophet: Not installed
2023-03-05 16:31:00,474:INFO:None
2023-03-05 16:31:00,474:INFO:Set up data.
2023-03-05 16:31:00,479:INFO:Set up train/test split.
2023-03-05 16:31:00,484:INFO:Set up index.
2023-03-05 16:31:00,485:INFO:Set up folding strategy.
2023-03-05 16:31:00,485:INFO:Assigning column types.
2023-03-05 16:31:00,487:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-05 16:31:00,488:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-05 16:31:00,492:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 16:31:00,497:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:31:00,553:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:31:00,595:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:31:00,596:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:31:00,752:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:31:00,753:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-05 16:31:00,757:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 16:31:00,761:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:31:00,814:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:31:00,855:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:31:00,856:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:31:00,858:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:31:00,859:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-03-05 16:31:00,863:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 16:31:00,867:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:31:00,920:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:31:00,963:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:31:00,964:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:31:00,966:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:31:00,970:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 16:31:00,975:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:31:01,031:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:31:01,072:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:31:01,072:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:31:01,074:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:31:01,075:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-03-05 16:31:01,083:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:31:01,135:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:31:01,175:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:31:01,176:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:31:01,178:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:31:01,186:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:31:01,239:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:31:01,279:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:31:01,279:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:31:01,282:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:31:01,282:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-03-05 16:31:01,342:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:31:01,383:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:31:01,384:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:31:01,386:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:31:01,447:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:31:01,487:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:31:01,488:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:31:01,490:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:31:01,490:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-05 16:31:01,551:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:31:01,593:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:31:01,596:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:31:01,657:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:31:01,699:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:31:01,701:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:31:01,702:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-03-05 16:31:01,802:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:31:01,806:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:31:01,915:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:31:01,918:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:31:01,922:INFO:Preparing preprocessing pipeline...
2023-03-05 16:31:01,923:INFO:Set up simple imputation.
2023-03-05 16:31:01,926:INFO:Set up encoding of categorical features.
2023-03-05 16:31:02,016:INFO:Finished creating preprocessing pipeline.
2023-03-05 16:31:02,028:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Alvaro\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=[], transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['experience_level',
                                             'employment_type', 'job_title',
                                             'employee_residence',
                                             'company_size'],
                                    transformer=SimpleImputer(strategy='most_frequ...
                                             'company_size'],
                                    transformer=OneHotEncoder(cols=['experience_level',
                                                                    'employment_type',
                                                                    'company_size'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['job_title', 'employee_residence'],
                                    transformer=LeaveOneOutEncoder(cols=['job_title',
                                                                         'employee_residence'],
                                                                   handle_missing='return_nan',
                                                                   random_state=1557)))])
2023-03-05 16:31:02,028:INFO:Creating final display dataframe.
2023-03-05 16:31:02,389:INFO:Setup _display_container:                     Description             Value
0                    Session id              1557
1                        Target     salary_in_usd
2                   Target type        Regression
3           Original data shape          (500, 6)
4        Transformed data shape         (500, 14)
5   Transformed train set shape         (350, 14)
6    Transformed test set shape         (150, 14)
7          Categorical features                 5
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12     Maximum one-hot encoding                25
13              Encoding method              None
14               Fold Generator             KFold
15                  Fold Number                10
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  reg-default-name
20                          USI              ed0b
2023-03-05 16:31:02,507:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:31:02,510:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:31:02,619:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:31:02,621:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:31:02,621:INFO:setup() successfully completed in 2.19s...............
2023-03-05 16:31:15,733:INFO:Initializing compare_models()
2023-03-05 16:31:15,734:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000018D8FF81D30>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000018D8FF81D30>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-03-05 16:31:15,734:INFO:Checking exceptions
2023-03-05 16:31:15,735:INFO:Preparing display monitor
2023-03-05 16:31:15,783:INFO:Initializing Linear Regression
2023-03-05 16:31:15,784:INFO:Total runtime is 5.030632019042969e-06 minutes
2023-03-05 16:31:15,790:INFO:SubProcess create_model() called ==================================
2023-03-05 16:31:15,791:INFO:Initializing create_model()
2023-03-05 16:31:15,792:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000018D8FF81D30>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018D8E5374F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:31:15,792:INFO:Checking exceptions
2023-03-05 16:31:15,793:INFO:Importing libraries
2023-03-05 16:31:15,793:INFO:Copying training dataset
2023-03-05 16:31:15,802:INFO:Defining folds
2023-03-05 16:31:15,802:INFO:Declaring metric variables
2023-03-05 16:31:15,805:INFO:Importing untrained model
2023-03-05 16:31:15,812:INFO:Linear Regression Imported successfully
2023-03-05 16:31:15,821:INFO:Starting cross validation
2023-03-05 16:31:15,838:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:31:24,127:INFO:Calculating mean and std
2023-03-05 16:31:24,128:INFO:Creating metrics dataframe
2023-03-05 16:31:24,133:INFO:Uploading results into container
2023-03-05 16:31:24,134:INFO:Uploading model into container now
2023-03-05 16:31:24,134:INFO:_master_model_container: 1
2023-03-05 16:31:24,134:INFO:_display_container: 2
2023-03-05 16:31:24,135:INFO:LinearRegression(n_jobs=-1)
2023-03-05 16:31:24,135:INFO:create_model() successfully completed......................................
2023-03-05 16:31:24,242:INFO:SubProcess create_model() end ==================================
2023-03-05 16:31:24,242:INFO:Creating metrics dataframe
2023-03-05 16:31:24,250:INFO:Initializing Lasso Regression
2023-03-05 16:31:24,250:INFO:Total runtime is 0.14110918045043944 minutes
2023-03-05 16:31:24,253:INFO:SubProcess create_model() called ==================================
2023-03-05 16:31:24,253:INFO:Initializing create_model()
2023-03-05 16:31:24,254:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000018D8FF81D30>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018D8E5374F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:31:24,254:INFO:Checking exceptions
2023-03-05 16:31:24,254:INFO:Importing libraries
2023-03-05 16:31:24,254:INFO:Copying training dataset
2023-03-05 16:31:24,261:INFO:Defining folds
2023-03-05 16:31:24,261:INFO:Declaring metric variables
2023-03-05 16:31:24,265:INFO:Importing untrained model
2023-03-05 16:31:24,272:INFO:Lasso Regression Imported successfully
2023-03-05 16:31:24,279:INFO:Starting cross validation
2023-03-05 16:31:24,281:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:31:24,852:INFO:Calculating mean and std
2023-03-05 16:31:24,853:INFO:Creating metrics dataframe
2023-03-05 16:31:24,857:INFO:Uploading results into container
2023-03-05 16:31:24,858:INFO:Uploading model into container now
2023-03-05 16:31:24,859:INFO:_master_model_container: 2
2023-03-05 16:31:24,859:INFO:_display_container: 2
2023-03-05 16:31:24,859:INFO:Lasso(random_state=1557)
2023-03-05 16:31:24,859:INFO:create_model() successfully completed......................................
2023-03-05 16:31:24,961:INFO:SubProcess create_model() end ==================================
2023-03-05 16:31:24,961:INFO:Creating metrics dataframe
2023-03-05 16:31:24,969:INFO:Initializing Ridge Regression
2023-03-05 16:31:24,969:INFO:Total runtime is 0.15308487415313718 minutes
2023-03-05 16:31:24,972:INFO:SubProcess create_model() called ==================================
2023-03-05 16:31:24,972:INFO:Initializing create_model()
2023-03-05 16:31:24,972:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000018D8FF81D30>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018D8E5374F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:31:24,972:INFO:Checking exceptions
2023-03-05 16:31:24,972:INFO:Importing libraries
2023-03-05 16:31:24,973:INFO:Copying training dataset
2023-03-05 16:31:24,977:INFO:Defining folds
2023-03-05 16:31:24,978:INFO:Declaring metric variables
2023-03-05 16:31:24,983:INFO:Importing untrained model
2023-03-05 16:31:24,990:INFO:Ridge Regression Imported successfully
2023-03-05 16:31:24,999:INFO:Starting cross validation
2023-03-05 16:31:25,004:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:31:25,575:INFO:Calculating mean and std
2023-03-05 16:31:25,577:INFO:Creating metrics dataframe
2023-03-05 16:31:25,580:INFO:Uploading results into container
2023-03-05 16:31:25,581:INFO:Uploading model into container now
2023-03-05 16:31:25,581:INFO:_master_model_container: 3
2023-03-05 16:31:25,582:INFO:_display_container: 2
2023-03-05 16:31:25,582:INFO:Ridge(random_state=1557)
2023-03-05 16:31:25,583:INFO:create_model() successfully completed......................................
2023-03-05 16:31:25,682:INFO:SubProcess create_model() end ==================================
2023-03-05 16:31:25,682:INFO:Creating metrics dataframe
2023-03-05 16:31:25,690:INFO:Initializing Elastic Net
2023-03-05 16:31:25,691:INFO:Total runtime is 0.16512281497319536 minutes
2023-03-05 16:31:25,695:INFO:SubProcess create_model() called ==================================
2023-03-05 16:31:25,695:INFO:Initializing create_model()
2023-03-05 16:31:25,695:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000018D8FF81D30>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018D8E5374F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:31:25,695:INFO:Checking exceptions
2023-03-05 16:31:25,695:INFO:Importing libraries
2023-03-05 16:31:25,696:INFO:Copying training dataset
2023-03-05 16:31:25,701:INFO:Defining folds
2023-03-05 16:31:25,701:INFO:Declaring metric variables
2023-03-05 16:31:25,705:INFO:Importing untrained model
2023-03-05 16:31:25,710:INFO:Elastic Net Imported successfully
2023-03-05 16:31:25,718:INFO:Starting cross validation
2023-03-05 16:31:25,719:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:31:26,301:INFO:Calculating mean and std
2023-03-05 16:31:26,303:INFO:Creating metrics dataframe
2023-03-05 16:31:26,306:INFO:Uploading results into container
2023-03-05 16:31:26,307:INFO:Uploading model into container now
2023-03-05 16:31:26,307:INFO:_master_model_container: 4
2023-03-05 16:31:26,308:INFO:_display_container: 2
2023-03-05 16:31:26,308:INFO:ElasticNet(random_state=1557)
2023-03-05 16:31:26,308:INFO:create_model() successfully completed......................................
2023-03-05 16:31:26,408:INFO:SubProcess create_model() end ==================================
2023-03-05 16:31:26,408:INFO:Creating metrics dataframe
2023-03-05 16:31:26,418:INFO:Initializing Least Angle Regression
2023-03-05 16:31:26,418:INFO:Total runtime is 0.1772418975830078 minutes
2023-03-05 16:31:26,421:INFO:SubProcess create_model() called ==================================
2023-03-05 16:31:26,422:INFO:Initializing create_model()
2023-03-05 16:31:26,422:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000018D8FF81D30>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018D8E5374F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:31:26,422:INFO:Checking exceptions
2023-03-05 16:31:26,422:INFO:Importing libraries
2023-03-05 16:31:26,422:INFO:Copying training dataset
2023-03-05 16:31:26,427:INFO:Defining folds
2023-03-05 16:31:26,427:INFO:Declaring metric variables
2023-03-05 16:31:26,432:INFO:Importing untrained model
2023-03-05 16:31:26,436:INFO:Least Angle Regression Imported successfully
2023-03-05 16:31:26,447:INFO:Starting cross validation
2023-03-05 16:31:26,448:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:31:26,668:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:31:26,679:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:31:26,710:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:31:26,737:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:31:26,748:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:31:26,767:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:31:26,778:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:31:26,794:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:31:26,951:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:31:26,955:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:31:27,007:INFO:Calculating mean and std
2023-03-05 16:31:27,009:INFO:Creating metrics dataframe
2023-03-05 16:31:27,013:INFO:Uploading results into container
2023-03-05 16:31:27,013:INFO:Uploading model into container now
2023-03-05 16:31:27,014:INFO:_master_model_container: 5
2023-03-05 16:31:27,014:INFO:_display_container: 2
2023-03-05 16:31:27,014:INFO:Lars(random_state=1557)
2023-03-05 16:31:27,014:INFO:create_model() successfully completed......................................
2023-03-05 16:31:27,114:INFO:SubProcess create_model() end ==================================
2023-03-05 16:31:27,114:INFO:Creating metrics dataframe
2023-03-05 16:31:27,123:INFO:Initializing Lasso Least Angle Regression
2023-03-05 16:31:27,123:INFO:Total runtime is 0.1889969944953918 minutes
2023-03-05 16:31:27,128:INFO:SubProcess create_model() called ==================================
2023-03-05 16:31:27,128:INFO:Initializing create_model()
2023-03-05 16:31:27,128:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000018D8FF81D30>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018D8E5374F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:31:27,128:INFO:Checking exceptions
2023-03-05 16:31:27,128:INFO:Importing libraries
2023-03-05 16:31:27,128:INFO:Copying training dataset
2023-03-05 16:31:27,135:INFO:Defining folds
2023-03-05 16:31:27,135:INFO:Declaring metric variables
2023-03-05 16:31:27,140:INFO:Importing untrained model
2023-03-05 16:31:27,145:INFO:Lasso Least Angle Regression Imported successfully
2023-03-05 16:31:27,154:INFO:Starting cross validation
2023-03-05 16:31:27,156:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:31:27,388:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:31:27,405:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:31:27,418:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:31:27,453:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:31:27,454:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:31:27,474:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:31:27,488:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:31:27,501:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:31:27,659:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:31:27,668:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:31:27,723:INFO:Calculating mean and std
2023-03-05 16:31:27,724:INFO:Creating metrics dataframe
2023-03-05 16:31:27,728:INFO:Uploading results into container
2023-03-05 16:31:27,729:INFO:Uploading model into container now
2023-03-05 16:31:27,730:INFO:_master_model_container: 6
2023-03-05 16:31:27,730:INFO:_display_container: 2
2023-03-05 16:31:27,730:INFO:LassoLars(random_state=1557)
2023-03-05 16:31:27,730:INFO:create_model() successfully completed......................................
2023-03-05 16:31:27,828:INFO:SubProcess create_model() end ==================================
2023-03-05 16:31:27,828:INFO:Creating metrics dataframe
2023-03-05 16:31:27,837:INFO:Initializing Orthogonal Matching Pursuit
2023-03-05 16:31:27,837:INFO:Total runtime is 0.20088549852371212 minutes
2023-03-05 16:31:27,840:INFO:SubProcess create_model() called ==================================
2023-03-05 16:31:27,840:INFO:Initializing create_model()
2023-03-05 16:31:27,840:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000018D8FF81D30>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018D8E5374F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:31:27,840:INFO:Checking exceptions
2023-03-05 16:31:27,841:INFO:Importing libraries
2023-03-05 16:31:27,841:INFO:Copying training dataset
2023-03-05 16:31:27,845:INFO:Defining folds
2023-03-05 16:31:27,846:INFO:Declaring metric variables
2023-03-05 16:31:27,850:INFO:Importing untrained model
2023-03-05 16:31:27,855:INFO:Orthogonal Matching Pursuit Imported successfully
2023-03-05 16:31:27,863:INFO:Starting cross validation
2023-03-05 16:31:27,864:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:31:28,112:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:31:28,131:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:31:28,152:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:31:28,154:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:31:28,165:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:31:28,171:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:31:28,196:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:31:28,218:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:31:28,390:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:31:28,400:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:31:28,452:INFO:Calculating mean and std
2023-03-05 16:31:28,453:INFO:Creating metrics dataframe
2023-03-05 16:31:28,456:INFO:Uploading results into container
2023-03-05 16:31:28,457:INFO:Uploading model into container now
2023-03-05 16:31:28,457:INFO:_master_model_container: 7
2023-03-05 16:31:28,457:INFO:_display_container: 2
2023-03-05 16:31:28,458:INFO:OrthogonalMatchingPursuit()
2023-03-05 16:31:28,458:INFO:create_model() successfully completed......................................
2023-03-05 16:31:28,559:INFO:SubProcess create_model() end ==================================
2023-03-05 16:31:28,559:INFO:Creating metrics dataframe
2023-03-05 16:31:28,568:INFO:Initializing Bayesian Ridge
2023-03-05 16:31:28,568:INFO:Total runtime is 0.21307558616002398 minutes
2023-03-05 16:31:28,572:INFO:SubProcess create_model() called ==================================
2023-03-05 16:31:28,572:INFO:Initializing create_model()
2023-03-05 16:31:28,572:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000018D8FF81D30>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018D8E5374F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:31:28,572:INFO:Checking exceptions
2023-03-05 16:31:28,573:INFO:Importing libraries
2023-03-05 16:31:28,573:INFO:Copying training dataset
2023-03-05 16:31:28,578:INFO:Defining folds
2023-03-05 16:31:28,578:INFO:Declaring metric variables
2023-03-05 16:31:28,582:INFO:Importing untrained model
2023-03-05 16:31:28,586:INFO:Bayesian Ridge Imported successfully
2023-03-05 16:31:28,594:INFO:Starting cross validation
2023-03-05 16:31:28,595:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:31:29,170:INFO:Calculating mean and std
2023-03-05 16:31:29,171:INFO:Creating metrics dataframe
2023-03-05 16:31:29,174:INFO:Uploading results into container
2023-03-05 16:31:29,175:INFO:Uploading model into container now
2023-03-05 16:31:29,175:INFO:_master_model_container: 8
2023-03-05 16:31:29,175:INFO:_display_container: 2
2023-03-05 16:31:29,175:INFO:BayesianRidge()
2023-03-05 16:31:29,175:INFO:create_model() successfully completed......................................
2023-03-05 16:31:29,270:INFO:SubProcess create_model() end ==================================
2023-03-05 16:31:29,270:INFO:Creating metrics dataframe
2023-03-05 16:31:29,283:INFO:Initializing Passive Aggressive Regressor
2023-03-05 16:31:29,283:INFO:Total runtime is 0.2249916593233744 minutes
2023-03-05 16:31:29,287:INFO:SubProcess create_model() called ==================================
2023-03-05 16:31:29,288:INFO:Initializing create_model()
2023-03-05 16:31:29,288:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000018D8FF81D30>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018D8E5374F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:31:29,290:INFO:Checking exceptions
2023-03-05 16:31:29,290:INFO:Importing libraries
2023-03-05 16:31:29,290:INFO:Copying training dataset
2023-03-05 16:31:29,296:INFO:Defining folds
2023-03-05 16:31:29,296:INFO:Declaring metric variables
2023-03-05 16:31:29,300:INFO:Importing untrained model
2023-03-05 16:31:29,304:INFO:Passive Aggressive Regressor Imported successfully
2023-03-05 16:31:29,313:INFO:Starting cross validation
2023-03-05 16:31:29,315:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:31:29,877:INFO:Calculating mean and std
2023-03-05 16:31:29,878:INFO:Creating metrics dataframe
2023-03-05 16:31:29,882:INFO:Uploading results into container
2023-03-05 16:31:29,883:INFO:Uploading model into container now
2023-03-05 16:31:29,884:INFO:_master_model_container: 9
2023-03-05 16:31:29,884:INFO:_display_container: 2
2023-03-05 16:31:29,884:INFO:PassiveAggressiveRegressor(random_state=1557)
2023-03-05 16:31:29,884:INFO:create_model() successfully completed......................................
2023-03-05 16:31:29,986:INFO:SubProcess create_model() end ==================================
2023-03-05 16:31:29,986:INFO:Creating metrics dataframe
2023-03-05 16:31:29,997:INFO:Initializing Huber Regressor
2023-03-05 16:31:29,997:INFO:Total runtime is 0.23689597447713212 minutes
2023-03-05 16:31:30,001:INFO:SubProcess create_model() called ==================================
2023-03-05 16:31:30,001:INFO:Initializing create_model()
2023-03-05 16:31:30,001:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000018D8FF81D30>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018D8E5374F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:31:30,001:INFO:Checking exceptions
2023-03-05 16:31:30,001:INFO:Importing libraries
2023-03-05 16:31:30,002:INFO:Copying training dataset
2023-03-05 16:31:30,007:INFO:Defining folds
2023-03-05 16:31:30,007:INFO:Declaring metric variables
2023-03-05 16:31:30,012:INFO:Importing untrained model
2023-03-05 16:31:30,016:INFO:Huber Regressor Imported successfully
2023-03-05 16:31:30,025:INFO:Starting cross validation
2023-03-05 16:31:30,028:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:31:30,822:INFO:Calculating mean and std
2023-03-05 16:31:30,824:INFO:Creating metrics dataframe
2023-03-05 16:31:30,830:INFO:Uploading results into container
2023-03-05 16:31:30,831:INFO:Uploading model into container now
2023-03-05 16:31:30,831:INFO:_master_model_container: 10
2023-03-05 16:31:30,831:INFO:_display_container: 2
2023-03-05 16:31:30,832:INFO:HuberRegressor()
2023-03-05 16:31:30,832:INFO:create_model() successfully completed......................................
2023-03-05 16:31:30,948:INFO:SubProcess create_model() end ==================================
2023-03-05 16:31:30,948:INFO:Creating metrics dataframe
2023-03-05 16:31:30,962:INFO:Initializing K Neighbors Regressor
2023-03-05 16:31:30,962:INFO:Total runtime is 0.25298135677973427 minutes
2023-03-05 16:31:30,966:INFO:SubProcess create_model() called ==================================
2023-03-05 16:31:30,967:INFO:Initializing create_model()
2023-03-05 16:31:30,968:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000018D8FF81D30>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018D8E5374F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:31:30,968:INFO:Checking exceptions
2023-03-05 16:31:30,968:INFO:Importing libraries
2023-03-05 16:31:30,968:INFO:Copying training dataset
2023-03-05 16:31:30,976:INFO:Defining folds
2023-03-05 16:31:30,976:INFO:Declaring metric variables
2023-03-05 16:31:30,982:INFO:Importing untrained model
2023-03-05 16:31:30,987:INFO:K Neighbors Regressor Imported successfully
2023-03-05 16:31:30,998:INFO:Starting cross validation
2023-03-05 16:31:31,000:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:31:31,776:INFO:Calculating mean and std
2023-03-05 16:31:31,778:INFO:Creating metrics dataframe
2023-03-05 16:31:31,784:INFO:Uploading results into container
2023-03-05 16:31:31,784:INFO:Uploading model into container now
2023-03-05 16:31:31,785:INFO:_master_model_container: 11
2023-03-05 16:31:31,785:INFO:_display_container: 2
2023-03-05 16:31:31,786:INFO:KNeighborsRegressor(n_jobs=-1)
2023-03-05 16:31:31,786:INFO:create_model() successfully completed......................................
2023-03-05 16:31:31,901:INFO:SubProcess create_model() end ==================================
2023-03-05 16:31:31,902:INFO:Creating metrics dataframe
2023-03-05 16:31:31,916:INFO:Initializing Decision Tree Regressor
2023-03-05 16:31:31,916:INFO:Total runtime is 0.26888159910837806 minutes
2023-03-05 16:31:31,921:INFO:SubProcess create_model() called ==================================
2023-03-05 16:31:31,922:INFO:Initializing create_model()
2023-03-05 16:31:31,922:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000018D8FF81D30>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018D8E5374F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:31:31,922:INFO:Checking exceptions
2023-03-05 16:31:31,922:INFO:Importing libraries
2023-03-05 16:31:31,922:INFO:Copying training dataset
2023-03-05 16:31:31,929:INFO:Defining folds
2023-03-05 16:31:31,930:INFO:Declaring metric variables
2023-03-05 16:31:31,934:INFO:Importing untrained model
2023-03-05 16:31:31,939:INFO:Decision Tree Regressor Imported successfully
2023-03-05 16:31:31,949:INFO:Starting cross validation
2023-03-05 16:31:31,951:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:31:32,773:INFO:Calculating mean and std
2023-03-05 16:31:32,775:INFO:Creating metrics dataframe
2023-03-05 16:31:32,779:INFO:Uploading results into container
2023-03-05 16:31:32,780:INFO:Uploading model into container now
2023-03-05 16:31:32,781:INFO:_master_model_container: 12
2023-03-05 16:31:32,781:INFO:_display_container: 2
2023-03-05 16:31:32,782:INFO:DecisionTreeRegressor(random_state=1557)
2023-03-05 16:31:32,782:INFO:create_model() successfully completed......................................
2023-03-05 16:31:32,896:INFO:SubProcess create_model() end ==================================
2023-03-05 16:31:32,896:INFO:Creating metrics dataframe
2023-03-05 16:31:32,910:INFO:Initializing Random Forest Regressor
2023-03-05 16:31:32,910:INFO:Total runtime is 0.28543611764907834 minutes
2023-03-05 16:31:32,915:INFO:SubProcess create_model() called ==================================
2023-03-05 16:31:32,915:INFO:Initializing create_model()
2023-03-05 16:31:32,915:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000018D8FF81D30>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018D8E5374F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:31:32,915:INFO:Checking exceptions
2023-03-05 16:31:32,915:INFO:Importing libraries
2023-03-05 16:31:32,915:INFO:Copying training dataset
2023-03-05 16:31:32,922:INFO:Defining folds
2023-03-05 16:31:32,922:INFO:Declaring metric variables
2023-03-05 16:31:32,927:INFO:Importing untrained model
2023-03-05 16:31:32,933:INFO:Random Forest Regressor Imported successfully
2023-03-05 16:31:32,943:INFO:Starting cross validation
2023-03-05 16:31:32,945:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:31:35,254:INFO:Calculating mean and std
2023-03-05 16:31:35,256:INFO:Creating metrics dataframe
2023-03-05 16:31:35,259:INFO:Uploading results into container
2023-03-05 16:31:35,260:INFO:Uploading model into container now
2023-03-05 16:31:35,261:INFO:_master_model_container: 13
2023-03-05 16:31:35,261:INFO:_display_container: 2
2023-03-05 16:31:35,261:INFO:RandomForestRegressor(n_jobs=-1, random_state=1557)
2023-03-05 16:31:35,261:INFO:create_model() successfully completed......................................
2023-03-05 16:31:35,368:INFO:SubProcess create_model() end ==================================
2023-03-05 16:31:35,368:INFO:Creating metrics dataframe
2023-03-05 16:31:35,381:INFO:Initializing Extra Trees Regressor
2023-03-05 16:31:35,382:INFO:Total runtime is 0.3266436616579691 minutes
2023-03-05 16:31:35,386:INFO:SubProcess create_model() called ==================================
2023-03-05 16:31:35,387:INFO:Initializing create_model()
2023-03-05 16:31:35,387:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000018D8FF81D30>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018D8E5374F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:31:35,387:INFO:Checking exceptions
2023-03-05 16:31:35,387:INFO:Importing libraries
2023-03-05 16:31:35,388:INFO:Copying training dataset
2023-03-05 16:31:35,393:INFO:Defining folds
2023-03-05 16:31:35,394:INFO:Declaring metric variables
2023-03-05 16:31:35,398:INFO:Importing untrained model
2023-03-05 16:31:35,403:INFO:Extra Trees Regressor Imported successfully
2023-03-05 16:31:35,413:INFO:Starting cross validation
2023-03-05 16:31:35,417:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:31:37,583:INFO:Calculating mean and std
2023-03-05 16:31:37,585:INFO:Creating metrics dataframe
2023-03-05 16:31:37,588:INFO:Uploading results into container
2023-03-05 16:31:37,589:INFO:Uploading model into container now
2023-03-05 16:31:37,589:INFO:_master_model_container: 14
2023-03-05 16:31:37,590:INFO:_display_container: 2
2023-03-05 16:31:37,590:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=1557)
2023-03-05 16:31:37,591:INFO:create_model() successfully completed......................................
2023-03-05 16:31:37,702:INFO:SubProcess create_model() end ==================================
2023-03-05 16:31:37,703:INFO:Creating metrics dataframe
2023-03-05 16:31:37,716:INFO:Initializing AdaBoost Regressor
2023-03-05 16:31:37,716:INFO:Total runtime is 0.3655392885208129 minutes
2023-03-05 16:31:37,720:INFO:SubProcess create_model() called ==================================
2023-03-05 16:31:37,720:INFO:Initializing create_model()
2023-03-05 16:31:37,721:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000018D8FF81D30>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018D8E5374F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:31:37,721:INFO:Checking exceptions
2023-03-05 16:31:37,721:INFO:Importing libraries
2023-03-05 16:31:37,721:INFO:Copying training dataset
2023-03-05 16:31:37,727:INFO:Defining folds
2023-03-05 16:31:37,727:INFO:Declaring metric variables
2023-03-05 16:31:37,731:INFO:Importing untrained model
2023-03-05 16:31:37,737:INFO:AdaBoost Regressor Imported successfully
2023-03-05 16:31:37,747:INFO:Starting cross validation
2023-03-05 16:31:37,750:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:31:39,183:INFO:Calculating mean and std
2023-03-05 16:31:39,184:INFO:Creating metrics dataframe
2023-03-05 16:31:39,188:INFO:Uploading results into container
2023-03-05 16:31:39,190:INFO:Uploading model into container now
2023-03-05 16:31:39,191:INFO:_master_model_container: 15
2023-03-05 16:31:39,191:INFO:_display_container: 2
2023-03-05 16:31:39,191:INFO:AdaBoostRegressor(random_state=1557)
2023-03-05 16:31:39,191:INFO:create_model() successfully completed......................................
2023-03-05 16:31:39,306:INFO:SubProcess create_model() end ==================================
2023-03-05 16:31:39,306:INFO:Creating metrics dataframe
2023-03-05 16:31:39,321:INFO:Initializing Gradient Boosting Regressor
2023-03-05 16:31:39,321:INFO:Total runtime is 0.39229963223139436 minutes
2023-03-05 16:31:39,325:INFO:SubProcess create_model() called ==================================
2023-03-05 16:31:39,325:INFO:Initializing create_model()
2023-03-05 16:31:39,325:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000018D8FF81D30>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018D8E5374F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:31:39,325:INFO:Checking exceptions
2023-03-05 16:31:39,325:INFO:Importing libraries
2023-03-05 16:31:39,326:INFO:Copying training dataset
2023-03-05 16:31:39,332:INFO:Defining folds
2023-03-05 16:31:39,332:INFO:Declaring metric variables
2023-03-05 16:31:39,336:INFO:Importing untrained model
2023-03-05 16:31:39,342:INFO:Gradient Boosting Regressor Imported successfully
2023-03-05 16:31:39,352:INFO:Starting cross validation
2023-03-05 16:31:39,354:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:31:40,945:INFO:Calculating mean and std
2023-03-05 16:31:40,948:INFO:Creating metrics dataframe
2023-03-05 16:31:40,952:INFO:Uploading results into container
2023-03-05 16:31:40,952:INFO:Uploading model into container now
2023-03-05 16:31:40,953:INFO:_master_model_container: 16
2023-03-05 16:31:40,953:INFO:_display_container: 2
2023-03-05 16:31:40,953:INFO:GradientBoostingRegressor(random_state=1557)
2023-03-05 16:31:40,954:INFO:create_model() successfully completed......................................
2023-03-05 16:31:41,059:INFO:SubProcess create_model() end ==================================
2023-03-05 16:31:41,059:INFO:Creating metrics dataframe
2023-03-05 16:31:41,074:INFO:Initializing Extreme Gradient Boosting
2023-03-05 16:31:41,074:INFO:Total runtime is 0.42150827646255484 minutes
2023-03-05 16:31:41,079:INFO:SubProcess create_model() called ==================================
2023-03-05 16:31:41,079:INFO:Initializing create_model()
2023-03-05 16:31:41,079:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000018D8FF81D30>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018D8E5374F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:31:41,080:INFO:Checking exceptions
2023-03-05 16:31:41,080:INFO:Importing libraries
2023-03-05 16:31:41,080:INFO:Copying training dataset
2023-03-05 16:31:41,088:INFO:Defining folds
2023-03-05 16:31:41,088:INFO:Declaring metric variables
2023-03-05 16:31:41,094:INFO:Importing untrained model
2023-03-05 16:31:41,102:INFO:Extreme Gradient Boosting Imported successfully
2023-03-05 16:31:41,112:INFO:Starting cross validation
2023-03-05 16:31:41,114:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:31:42,695:INFO:Calculating mean and std
2023-03-05 16:31:42,697:INFO:Creating metrics dataframe
2023-03-05 16:31:42,702:INFO:Uploading results into container
2023-03-05 16:31:42,702:INFO:Uploading model into container now
2023-03-05 16:31:42,704:INFO:_master_model_container: 17
2023-03-05 16:31:42,704:INFO:_display_container: 2
2023-03-05 16:31:42,705:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=1557, ...)
2023-03-05 16:31:42,705:INFO:create_model() successfully completed......................................
2023-03-05 16:31:42,820:INFO:SubProcess create_model() end ==================================
2023-03-05 16:31:42,820:INFO:Creating metrics dataframe
2023-03-05 16:31:42,835:INFO:Initializing Light Gradient Boosting Machine
2023-03-05 16:31:42,835:INFO:Total runtime is 0.45086076656977325 minutes
2023-03-05 16:31:42,839:INFO:SubProcess create_model() called ==================================
2023-03-05 16:31:42,840:INFO:Initializing create_model()
2023-03-05 16:31:42,840:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000018D8FF81D30>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018D8E5374F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:31:42,840:INFO:Checking exceptions
2023-03-05 16:31:42,840:INFO:Importing libraries
2023-03-05 16:31:42,840:INFO:Copying training dataset
2023-03-05 16:31:42,847:INFO:Defining folds
2023-03-05 16:31:42,847:INFO:Declaring metric variables
2023-03-05 16:31:42,854:INFO:Importing untrained model
2023-03-05 16:31:42,858:INFO:Light Gradient Boosting Machine Imported successfully
2023-03-05 16:31:42,867:INFO:Starting cross validation
2023-03-05 16:31:42,869:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:31:45,955:INFO:Calculating mean and std
2023-03-05 16:31:45,957:INFO:Creating metrics dataframe
2023-03-05 16:31:45,961:INFO:Uploading results into container
2023-03-05 16:31:45,961:INFO:Uploading model into container now
2023-03-05 16:31:45,962:INFO:_master_model_container: 18
2023-03-05 16:31:45,962:INFO:_display_container: 2
2023-03-05 16:31:45,962:INFO:LGBMRegressor(random_state=1557)
2023-03-05 16:31:45,962:INFO:create_model() successfully completed......................................
2023-03-05 16:31:46,070:INFO:SubProcess create_model() end ==================================
2023-03-05 16:31:46,071:INFO:Creating metrics dataframe
2023-03-05 16:31:46,091:INFO:Initializing Dummy Regressor
2023-03-05 16:31:46,091:INFO:Total runtime is 0.5051306605339049 minutes
2023-03-05 16:31:46,096:INFO:SubProcess create_model() called ==================================
2023-03-05 16:31:46,097:INFO:Initializing create_model()
2023-03-05 16:31:46,097:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000018D8FF81D30>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000018D8E5374F0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:31:46,097:INFO:Checking exceptions
2023-03-05 16:31:46,097:INFO:Importing libraries
2023-03-05 16:31:46,097:INFO:Copying training dataset
2023-03-05 16:31:46,104:INFO:Defining folds
2023-03-05 16:31:46,105:INFO:Declaring metric variables
2023-03-05 16:31:46,112:INFO:Importing untrained model
2023-03-05 16:31:46,116:INFO:Dummy Regressor Imported successfully
2023-03-05 16:31:46,129:INFO:Starting cross validation
2023-03-05 16:31:46,132:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:31:47,003:INFO:Calculating mean and std
2023-03-05 16:31:47,005:INFO:Creating metrics dataframe
2023-03-05 16:31:47,009:INFO:Uploading results into container
2023-03-05 16:31:47,009:INFO:Uploading model into container now
2023-03-05 16:31:47,010:INFO:_master_model_container: 19
2023-03-05 16:31:47,010:INFO:_display_container: 2
2023-03-05 16:31:47,010:INFO:DummyRegressor()
2023-03-05 16:31:47,010:INFO:create_model() successfully completed......................................
2023-03-05 16:31:47,111:INFO:SubProcess create_model() end ==================================
2023-03-05 16:31:47,111:INFO:Creating metrics dataframe
2023-03-05 16:31:47,143:INFO:Initializing create_model()
2023-03-05 16:31:47,144:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000018D8FF81D30>, estimator=GradientBoostingRegressor(random_state=1557), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:31:47,144:INFO:Checking exceptions
2023-03-05 16:31:47,147:INFO:Importing libraries
2023-03-05 16:31:47,147:INFO:Copying training dataset
2023-03-05 16:31:47,151:INFO:Defining folds
2023-03-05 16:31:47,152:INFO:Declaring metric variables
2023-03-05 16:31:47,152:INFO:Importing untrained model
2023-03-05 16:31:47,152:INFO:Declaring custom model
2023-03-05 16:31:47,153:INFO:Gradient Boosting Regressor Imported successfully
2023-03-05 16:31:47,154:INFO:Cross validation set to False
2023-03-05 16:31:47,154:INFO:Fitting Model
2023-03-05 16:31:47,325:INFO:GradientBoostingRegressor(random_state=1557)
2023-03-05 16:31:47,325:INFO:create_model() successfully completed......................................
2023-03-05 16:31:47,463:INFO:_master_model_container: 19
2023-03-05 16:31:47,464:INFO:_display_container: 2
2023-03-05 16:31:47,465:INFO:GradientBoostingRegressor(random_state=1557)
2023-03-05 16:31:47,465:INFO:compare_models() successfully completed......................................
2023-03-05 16:35:51,102:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_6d7fb196494b48489935362f618c09a4_faadaf1d6ccf4aaa8ae2157a8a84d74e
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,102:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_698b9d9e22024f549b0826658f45e86d_c675f72bf18240319695187b6bca3acf
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,102:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_6d7fb196494b48489935362f618c09a4_5791e0985f9049569682993f9a4774fc
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,102:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_f73ddb3552ae40fd86615965897c539b_c17d23e442bf4205aaed70d300f2771d
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,103:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_6d7fb196494b48489935362f618c09a4_14717a29532944cdbd728a7850022c13
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,103:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_e4d00d78e1504cc6bbe754831c324ae1_9ced2c600c2d43a7881998c0cba11873
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,103:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_6d7fb196494b48489935362f618c09a4_01bfd3482d0d4d2484517c9c41976d00
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,103:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_be2107fc6f0f4a96b46188629f2bc4e9_3bdae4fd02c343cc8c47281a5b07376d
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,103:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_6d7fb196494b48489935362f618c09a4_508bf7895bf744cfb668ea178ed5c107
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,103:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_521a4df5aca044f6a45c1e5c2e380c15_d4eeb1669ff54f9daf536b86c55d0025
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,103:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_6d7fb196494b48489935362f618c09a4_7d6fec3141bd4187b6931af62547f568
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,103:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_65ac4304898c4eb4a2f4142b8662bf1e_a9047932f5ae4e07890e7aa9679ec077
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,104:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_6d7fb196494b48489935362f618c09a4_eefa2edb61f84110988f2b0dda1fcb84
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,104:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_d5c432e80e7a4dafabd356eb469f4930_aadfc5c877fd44f6b29432fad64e5ec8
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,104:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_6d7fb196494b48489935362f618c09a4_bdc0627eb85542968a1ab8d4fc095a3c
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,104:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_a18c63a88d0f449e9c22559aa9ec8774_5a6a13c1c12a4930ad16176c1de85196
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,104:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_6d7fb196494b48489935362f618c09a4_a702d5288212449ca19c8639d2efb0b7
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,104:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_4abba935921c428c8f9da384014f145f_9e52da918468492191a979cd00187aa3
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,104:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_6d7fb196494b48489935362f618c09a4_01086952f19044d0a6075d58253367ac
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,104:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_12d0b6f11a53460695628cc971a0cda2_5ad47cfe0e42448bbda6ce2d42fde49c
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,105:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_6d7fb196494b48489935362f618c09a4_6ef44e4980344c92b0664c9d919da8c9
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,105:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_d96388039b1e40798f156505c99e0a7c_0a79300d55e94b9285e29782d56da06a
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,105:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_6d7fb196494b48489935362f618c09a4_088ccdf9c1da4a96bcadc48f68401dbf
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,105:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_8e6470cf24074fd489730dad392c14f1_17467e6a8e1945a896fcd741619b2aa9
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,105:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_6d7fb196494b48489935362f618c09a4_4e6c8b6378ec41a3a6a1af4b6fa3da19
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,105:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_ec645a7997484002956ad9856ddfae06_a1a0abc7b6464960886e8d64259d2597
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,105:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_6d7fb196494b48489935362f618c09a4_8e449af3fd3a400393da752ee106d93c
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,105:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_285bcb7288874fd8b8644ad470450a2d_b22eedbb79c54489bf1663ad3e97bbbf
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,106:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_6d7fb196494b48489935362f618c09a4_990b36bc248c41349c686f416df571f3
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,106:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_1196e1a09f52459ea0e6f01db5b06da7_155aef9a32ef4a82838d1b17ea0d9370
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,106:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_6d7fb196494b48489935362f618c09a4_b28e0fa7539d4623bb5956a6404c8f89
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,106:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_e8d27b698960425395e064c5707dc62e_a07f21274c7940c2b4eacf19822d7ba6
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,106:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_6d7fb196494b48489935362f618c09a4_deb694eb93e64820ac036b717e3fc00f
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,106:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_b689dceff0f04c26aeed68c9df27e957_06dd53caf88c4948b7506ee3b07a2e7c
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,106:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_6d7fb196494b48489935362f618c09a4_48d5351531844411a3f1d0acf73f97e8
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,106:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_fe5de486eac34d549b7e1b23be6dab54_5f2c7a83c0df441a816494e4ce0fdd79
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,106:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_6d7fb196494b48489935362f618c09a4_860ab69aabda4fd59a808f0f51c7d362
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:35:51,107:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_8688_6d7fb196494b48489935362f618c09a4_b1808b4b15534f779486fc43c57f6242
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 16:36:12,098:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-05 16:36:12,098:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-05 16:36:12,098:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-05 16:36:12,098:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-05 16:36:12,714:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-03-05 16:38:58,581:INFO:PyCaret RegressionExperiment
2023-03-05 16:38:58,581:INFO:Logging name: reg-default-name
2023-03-05 16:38:58,582:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-05 16:38:58,582:INFO:version 3.0.0.rc9
2023-03-05 16:38:58,582:INFO:Initializing setup()
2023-03-05 16:38:58,582:INFO:self.USI: 81a3
2023-03-05 16:38:58,582:INFO:self._variable_keys: {'target_param', 'pipeline', 'idx', 'html_param', 'transform_target_param', 'fold_shuffle_param', 'log_plots_param', '_ml_usecase', 'y', 'logging_param', 'exp_name_log', 'fold_generator', 'X_test', 'n_jobs_param', 'USI', 'fold_groups_param', 'exp_id', 'seed', 'gpu_param', 'X', 'y_train', 'X_train', 'data', 'y_test', 'gpu_n_jobs_param', 'memory', '_available_plots'}
2023-03-05 16:38:58,582:INFO:Checking environment
2023-03-05 16:38:58,582:INFO:python_version: 3.9.13
2023-03-05 16:38:58,582:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-03-05 16:38:58,582:INFO:machine: AMD64
2023-03-05 16:38:58,582:INFO:platform: Windows-10-10.0.19044-SP0
2023-03-05 16:38:58,582:INFO:Memory: svmem(total=17009516544, available=6152646656, percent=63.8, used=10856869888, free=6152646656)
2023-03-05 16:38:58,582:INFO:Physical Core: 4
2023-03-05 16:38:58,582:INFO:Logical Core: 8
2023-03-05 16:38:58,582:INFO:Checking libraries
2023-03-05 16:38:58,582:INFO:System:
2023-03-05 16:38:58,582:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-03-05 16:38:58,582:INFO:executable: C:\Users\Alvaro\anaconda3\python.exe
2023-03-05 16:38:58,583:INFO:   machine: Windows-10-10.0.19044-SP0
2023-03-05 16:38:58,583:INFO:PyCaret required dependencies:
2023-03-05 16:38:58,583:INFO:                 pip: 22.2.2
2023-03-05 16:38:58,583:INFO:          setuptools: 63.4.1
2023-03-05 16:38:58,583:INFO:             pycaret: 3.0.0rc9
2023-03-05 16:38:58,583:INFO:             IPython: 7.31.1
2023-03-05 16:38:58,583:INFO:          ipywidgets: 7.6.5
2023-03-05 16:38:58,583:INFO:                tqdm: 4.64.1
2023-03-05 16:38:58,583:INFO:               numpy: 1.21.5
2023-03-05 16:38:58,583:INFO:              pandas: 1.4.4
2023-03-05 16:38:58,583:INFO:              jinja2: 2.11.3
2023-03-05 16:38:58,583:INFO:               scipy: 1.9.1
2023-03-05 16:38:58,583:INFO:              joblib: 1.2.0
2023-03-05 16:38:58,583:INFO:             sklearn: 1.0.2
2023-03-05 16:38:58,583:INFO:                pyod: 1.0.7
2023-03-05 16:38:58,583:INFO:            imblearn: 0.10.1
2023-03-05 16:38:58,583:INFO:   category_encoders: 2.6.0
2023-03-05 16:38:58,583:INFO:            lightgbm: 3.3.5
2023-03-05 16:38:58,583:INFO:               numba: 0.55.1
2023-03-05 16:38:58,583:INFO:            requests: 2.28.1
2023-03-05 16:38:58,583:INFO:          matplotlib: 3.5.2
2023-03-05 16:38:58,583:INFO:          scikitplot: 0.3.7
2023-03-05 16:38:58,584:INFO:         yellowbrick: 1.5
2023-03-05 16:38:58,584:INFO:              plotly: 5.9.0
2023-03-05 16:38:58,584:INFO:             kaleido: 0.2.1
2023-03-05 16:38:58,584:INFO:         statsmodels: 0.13.2
2023-03-05 16:38:58,584:INFO:              sktime: 0.16.1
2023-03-05 16:38:58,584:INFO:               tbats: 1.1.2
2023-03-05 16:38:58,584:INFO:            pmdarima: 2.0.2
2023-03-05 16:38:58,584:INFO:              psutil: 5.9.0
2023-03-05 16:38:58,584:INFO:PyCaret optional dependencies:
2023-03-05 16:38:58,608:INFO:                shap: Not installed
2023-03-05 16:38:58,608:INFO:           interpret: Not installed
2023-03-05 16:38:58,609:INFO:                umap: Not installed
2023-03-05 16:38:58,609:INFO:    pandas_profiling: Not installed
2023-03-05 16:38:58,609:INFO:  explainerdashboard: Not installed
2023-03-05 16:38:58,609:INFO:             autoviz: Not installed
2023-03-05 16:38:58,609:INFO:           fairlearn: Not installed
2023-03-05 16:38:58,609:INFO:             xgboost: 1.7.4
2023-03-05 16:38:58,609:INFO:            catboost: Not installed
2023-03-05 16:38:58,609:INFO:              kmodes: Not installed
2023-03-05 16:38:58,609:INFO:             mlxtend: Not installed
2023-03-05 16:38:58,609:INFO:       statsforecast: Not installed
2023-03-05 16:38:58,609:INFO:        tune_sklearn: Not installed
2023-03-05 16:38:58,609:INFO:                 ray: Not installed
2023-03-05 16:38:58,609:INFO:            hyperopt: Not installed
2023-03-05 16:38:58,609:INFO:              optuna: Not installed
2023-03-05 16:38:58,609:INFO:               skopt: Not installed
2023-03-05 16:38:58,609:INFO:              mlflow: Not installed
2023-03-05 16:38:58,609:INFO:              gradio: Not installed
2023-03-05 16:38:58,609:INFO:             fastapi: Not installed
2023-03-05 16:38:58,609:INFO:             uvicorn: Not installed
2023-03-05 16:38:58,609:INFO:              m2cgen: Not installed
2023-03-05 16:38:58,609:INFO:           evidently: Not installed
2023-03-05 16:38:58,609:INFO:               fugue: Not installed
2023-03-05 16:38:58,609:INFO:           streamlit: Not installed
2023-03-05 16:38:58,609:INFO:             prophet: Not installed
2023-03-05 16:38:58,610:INFO:None
2023-03-05 16:38:58,610:INFO:Set up data.
2023-03-05 16:38:58,614:INFO:Set up train/test split.
2023-03-05 16:38:58,619:INFO:Set up index.
2023-03-05 16:38:58,619:INFO:Set up folding strategy.
2023-03-05 16:38:58,619:INFO:Assigning column types.
2023-03-05 16:38:58,623:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-05 16:38:58,623:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-05 16:38:58,627:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 16:38:58,632:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:38:58,684:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:38:58,723:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:38:58,724:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:38:58,762:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:38:58,762:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-05 16:38:58,767:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 16:38:58,771:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:38:58,822:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:38:58,861:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:38:58,862:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:38:58,864:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:38:58,864:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-03-05 16:38:58,868:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 16:38:58,872:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:38:58,923:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:38:58,962:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:38:58,963:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:38:58,965:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:38:58,969:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 16:38:58,973:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:38:59,025:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:38:59,065:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:38:59,067:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:38:59,069:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:38:59,069:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-03-05 16:38:59,078:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:38:59,129:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:38:59,168:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:38:59,169:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:38:59,171:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:38:59,180:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:38:59,230:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:38:59,270:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:38:59,270:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:38:59,273:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:38:59,273:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-03-05 16:38:59,332:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:38:59,371:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:38:59,371:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:38:59,374:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:38:59,432:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:38:59,473:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:38:59,473:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:38:59,476:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:38:59,476:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-05 16:38:59,535:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:38:59,575:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:38:59,577:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:38:59,638:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:38:59,678:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:38:59,681:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:38:59,681:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-03-05 16:38:59,784:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:38:59,786:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:38:59,886:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:38:59,888:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:38:59,890:INFO:Preparing preprocessing pipeline...
2023-03-05 16:38:59,891:INFO:Set up simple imputation.
2023-03-05 16:38:59,893:INFO:Set up encoding of categorical features.
2023-03-05 16:38:59,893:INFO:Set up feature normalization.
2023-03-05 16:38:59,990:INFO:Finished creating preprocessing pipeline.
2023-03-05 16:39:00,000:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Alvaro\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=[], transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['experience_level',
                                             'employment_type', 'job_title',
                                             'employee_residence',
                                             'company_size'],
                                    transformer=SimpleImputer(strategy='most_frequ...
                                    transformer=OneHotEncoder(cols=['experience_level',
                                                                    'employment_type',
                                                                    'company_size'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['job_title', 'employee_residence'],
                                    transformer=LeaveOneOutEncoder(cols=['job_title',
                                                                         'employee_residence'],
                                                                   handle_missing='return_nan',
                                                                   random_state=1201))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))])
2023-03-05 16:39:00,000:INFO:Creating final display dataframe.
2023-03-05 16:39:00,404:INFO:Setup _display_container:                     Description             Value
0                    Session id              1201
1                        Target     salary_in_usd
2                   Target type        Regression
3           Original data shape          (500, 6)
4        Transformed data shape         (500, 14)
5   Transformed train set shape         (350, 14)
6    Transformed test set shape         (150, 14)
7          Categorical features                 5
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12     Maximum one-hot encoding                25
13              Encoding method              None
14                    Normalize              True
15             Normalize method            zscore
16               Fold Generator             KFold
17                  Fold Number                10
18                     CPU Jobs                -1
19                      Use GPU             False
20               Log Experiment             False
21              Experiment Name  reg-default-name
22                          USI              81a3
2023-03-05 16:39:00,521:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:39:00,524:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:39:00,624:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:39:00,627:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:39:00,627:INFO:setup() successfully completed in 2.07s...............
2023-03-05 16:39:00,640:INFO:Initializing compare_models()
2023-03-05 16:39:00,640:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7DADDDC0>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000013B7DADDDC0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-03-05 16:39:00,640:INFO:Checking exceptions
2023-03-05 16:39:00,642:INFO:Preparing display monitor
2023-03-05 16:39:00,688:INFO:Initializing Linear Regression
2023-03-05 16:39:00,689:INFO:Total runtime is 1.2874603271484375e-05 minutes
2023-03-05 16:39:00,698:INFO:SubProcess create_model() called ==================================
2023-03-05 16:39:00,698:INFO:Initializing create_model()
2023-03-05 16:39:00,698:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7DADDDC0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7DA113A0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:39:00,698:INFO:Checking exceptions
2023-03-05 16:39:00,698:INFO:Importing libraries
2023-03-05 16:39:00,698:INFO:Copying training dataset
2023-03-05 16:39:00,703:INFO:Defining folds
2023-03-05 16:39:00,703:INFO:Declaring metric variables
2023-03-05 16:39:00,709:INFO:Importing untrained model
2023-03-05 16:39:00,713:INFO:Linear Regression Imported successfully
2023-03-05 16:39:00,721:INFO:Starting cross validation
2023-03-05 16:39:00,730:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:39:09,266:INFO:Calculating mean and std
2023-03-05 16:39:09,268:INFO:Creating metrics dataframe
2023-03-05 16:39:09,271:INFO:Uploading results into container
2023-03-05 16:39:09,272:INFO:Uploading model into container now
2023-03-05 16:39:09,272:INFO:_master_model_container: 1
2023-03-05 16:39:09,272:INFO:_display_container: 2
2023-03-05 16:39:09,272:INFO:LinearRegression(n_jobs=-1)
2023-03-05 16:39:09,272:INFO:create_model() successfully completed......................................
2023-03-05 16:39:09,355:INFO:SubProcess create_model() end ==================================
2023-03-05 16:39:09,356:INFO:Creating metrics dataframe
2023-03-05 16:39:09,366:INFO:Initializing Lasso Regression
2023-03-05 16:39:09,366:INFO:Total runtime is 0.14463257789611816 minutes
2023-03-05 16:39:09,371:INFO:SubProcess create_model() called ==================================
2023-03-05 16:39:09,371:INFO:Initializing create_model()
2023-03-05 16:39:09,372:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7DADDDC0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7DA113A0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:39:09,372:INFO:Checking exceptions
2023-03-05 16:39:09,372:INFO:Importing libraries
2023-03-05 16:39:09,372:INFO:Copying training dataset
2023-03-05 16:39:09,378:INFO:Defining folds
2023-03-05 16:39:09,379:INFO:Declaring metric variables
2023-03-05 16:39:09,385:INFO:Importing untrained model
2023-03-05 16:39:09,389:INFO:Lasso Regression Imported successfully
2023-03-05 16:39:09,398:INFO:Starting cross validation
2023-03-05 16:39:09,401:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:39:09,672:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.443e+09, tolerance: 1.708e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-05 16:39:09,691:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.294e+09, tolerance: 1.721e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-05 16:39:09,710:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.875e+09, tolerance: 1.605e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-05 16:39:09,724:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.447e+09, tolerance: 1.623e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-05 16:39:09,751:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.328e+09, tolerance: 1.653e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-05 16:39:09,751:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.295e+09, tolerance: 1.735e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-05 16:39:09,769:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.518e+09, tolerance: 1.643e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-05 16:39:09,785:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.882e+09, tolerance: 1.570e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-05 16:39:10,028:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.472e+09, tolerance: 1.568e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-05 16:39:10,073:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.328e+09, tolerance: 1.643e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-05 16:39:10,126:INFO:Calculating mean and std
2023-03-05 16:39:10,127:INFO:Creating metrics dataframe
2023-03-05 16:39:10,130:INFO:Uploading results into container
2023-03-05 16:39:10,132:INFO:Uploading model into container now
2023-03-05 16:39:10,132:INFO:_master_model_container: 2
2023-03-05 16:39:10,132:INFO:_display_container: 2
2023-03-05 16:39:10,132:INFO:Lasso(random_state=1201)
2023-03-05 16:39:10,132:INFO:create_model() successfully completed......................................
2023-03-05 16:39:10,218:INFO:SubProcess create_model() end ==================================
2023-03-05 16:39:10,218:INFO:Creating metrics dataframe
2023-03-05 16:39:10,226:INFO:Initializing Ridge Regression
2023-03-05 16:39:10,226:INFO:Total runtime is 0.15896193981170653 minutes
2023-03-05 16:39:10,228:INFO:SubProcess create_model() called ==================================
2023-03-05 16:39:10,229:INFO:Initializing create_model()
2023-03-05 16:39:10,229:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7DADDDC0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7DA113A0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:39:10,229:INFO:Checking exceptions
2023-03-05 16:39:10,229:INFO:Importing libraries
2023-03-05 16:39:10,229:INFO:Copying training dataset
2023-03-05 16:39:10,235:INFO:Defining folds
2023-03-05 16:39:10,235:INFO:Declaring metric variables
2023-03-05 16:39:10,239:INFO:Importing untrained model
2023-03-05 16:39:10,243:INFO:Ridge Regression Imported successfully
2023-03-05 16:39:10,253:INFO:Starting cross validation
2023-03-05 16:39:10,256:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:39:10,924:INFO:Calculating mean and std
2023-03-05 16:39:10,926:INFO:Creating metrics dataframe
2023-03-05 16:39:10,929:INFO:Uploading results into container
2023-03-05 16:39:10,929:INFO:Uploading model into container now
2023-03-05 16:39:10,929:INFO:_master_model_container: 3
2023-03-05 16:39:10,930:INFO:_display_container: 2
2023-03-05 16:39:10,930:INFO:Ridge(random_state=1201)
2023-03-05 16:39:10,930:INFO:create_model() successfully completed......................................
2023-03-05 16:39:11,013:INFO:SubProcess create_model() end ==================================
2023-03-05 16:39:11,013:INFO:Creating metrics dataframe
2023-03-05 16:39:11,022:INFO:Initializing Elastic Net
2023-03-05 16:39:11,023:INFO:Total runtime is 0.1722403844197591 minutes
2023-03-05 16:39:11,025:INFO:SubProcess create_model() called ==================================
2023-03-05 16:39:11,025:INFO:Initializing create_model()
2023-03-05 16:39:11,026:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7DADDDC0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7DA113A0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:39:11,026:INFO:Checking exceptions
2023-03-05 16:39:11,026:INFO:Importing libraries
2023-03-05 16:39:11,026:INFO:Copying training dataset
2023-03-05 16:39:11,032:INFO:Defining folds
2023-03-05 16:39:11,032:INFO:Declaring metric variables
2023-03-05 16:39:11,037:INFO:Importing untrained model
2023-03-05 16:39:11,041:INFO:Elastic Net Imported successfully
2023-03-05 16:39:11,049:INFO:Starting cross validation
2023-03-05 16:39:11,052:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:39:11,677:INFO:Calculating mean and std
2023-03-05 16:39:11,679:INFO:Creating metrics dataframe
2023-03-05 16:39:11,682:INFO:Uploading results into container
2023-03-05 16:39:11,682:INFO:Uploading model into container now
2023-03-05 16:39:11,683:INFO:_master_model_container: 4
2023-03-05 16:39:11,683:INFO:_display_container: 2
2023-03-05 16:39:11,683:INFO:ElasticNet(random_state=1201)
2023-03-05 16:39:11,683:INFO:create_model() successfully completed......................................
2023-03-05 16:39:11,767:INFO:SubProcess create_model() end ==================================
2023-03-05 16:39:11,767:INFO:Creating metrics dataframe
2023-03-05 16:39:11,775:INFO:Initializing Least Angle Regression
2023-03-05 16:39:11,775:INFO:Total runtime is 0.18478480180104573 minutes
2023-03-05 16:39:11,778:INFO:SubProcess create_model() called ==================================
2023-03-05 16:39:11,779:INFO:Initializing create_model()
2023-03-05 16:39:11,779:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7DADDDC0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7DA113A0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:39:11,779:INFO:Checking exceptions
2023-03-05 16:39:11,779:INFO:Importing libraries
2023-03-05 16:39:11,779:INFO:Copying training dataset
2023-03-05 16:39:11,785:INFO:Defining folds
2023-03-05 16:39:11,785:INFO:Declaring metric variables
2023-03-05 16:39:11,788:INFO:Importing untrained model
2023-03-05 16:39:11,792:INFO:Least Angle Regression Imported successfully
2023-03-05 16:39:11,800:INFO:Starting cross validation
2023-03-05 16:39:11,802:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:39:12,085:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:39:12,106:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:39:12,128:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:39:12,165:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:39:12,166:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:39:12,176:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:39:12,188:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:39:12,195:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:39:12,416:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:39:12,434:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:39:12,491:INFO:Calculating mean and std
2023-03-05 16:39:12,492:INFO:Creating metrics dataframe
2023-03-05 16:39:12,495:INFO:Uploading results into container
2023-03-05 16:39:12,496:INFO:Uploading model into container now
2023-03-05 16:39:12,497:INFO:_master_model_container: 5
2023-03-05 16:39:12,497:INFO:_display_container: 2
2023-03-05 16:39:12,497:INFO:Lars(random_state=1201)
2023-03-05 16:39:12,497:INFO:create_model() successfully completed......................................
2023-03-05 16:39:12,579:INFO:SubProcess create_model() end ==================================
2023-03-05 16:39:12,579:INFO:Creating metrics dataframe
2023-03-05 16:39:12,587:INFO:Initializing Lasso Least Angle Regression
2023-03-05 16:39:12,587:INFO:Total runtime is 0.19832189480463663 minutes
2023-03-05 16:39:12,590:INFO:SubProcess create_model() called ==================================
2023-03-05 16:39:12,591:INFO:Initializing create_model()
2023-03-05 16:39:12,591:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7DADDDC0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7DA113A0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:39:12,591:INFO:Checking exceptions
2023-03-05 16:39:12,591:INFO:Importing libraries
2023-03-05 16:39:12,591:INFO:Copying training dataset
2023-03-05 16:39:12,597:INFO:Defining folds
2023-03-05 16:39:12,597:INFO:Declaring metric variables
2023-03-05 16:39:12,601:INFO:Importing untrained model
2023-03-05 16:39:12,605:INFO:Lasso Least Angle Regression Imported successfully
2023-03-05 16:39:12,614:INFO:Starting cross validation
2023-03-05 16:39:12,616:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:39:12,876:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:39:12,895:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:39:12,916:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:39:12,930:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:39:12,945:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:39:12,949:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:39:12,975:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:39:12,997:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:39:13,288:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:39:13,345:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:39:13,411:INFO:Calculating mean and std
2023-03-05 16:39:13,412:INFO:Creating metrics dataframe
2023-03-05 16:39:13,416:INFO:Uploading results into container
2023-03-05 16:39:13,417:INFO:Uploading model into container now
2023-03-05 16:39:13,417:INFO:_master_model_container: 6
2023-03-05 16:39:13,418:INFO:_display_container: 2
2023-03-05 16:39:13,418:INFO:LassoLars(random_state=1201)
2023-03-05 16:39:13,418:INFO:create_model() successfully completed......................................
2023-03-05 16:39:13,523:INFO:SubProcess create_model() end ==================================
2023-03-05 16:39:13,523:INFO:Creating metrics dataframe
2023-03-05 16:39:13,533:INFO:Initializing Orthogonal Matching Pursuit
2023-03-05 16:39:13,534:INFO:Total runtime is 0.21408949295679727 minutes
2023-03-05 16:39:13,537:INFO:SubProcess create_model() called ==================================
2023-03-05 16:39:13,537:INFO:Initializing create_model()
2023-03-05 16:39:13,537:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7DADDDC0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7DA113A0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:39:13,537:INFO:Checking exceptions
2023-03-05 16:39:13,537:INFO:Importing libraries
2023-03-05 16:39:13,538:INFO:Copying training dataset
2023-03-05 16:39:13,542:INFO:Defining folds
2023-03-05 16:39:13,543:INFO:Declaring metric variables
2023-03-05 16:39:13,546:INFO:Importing untrained model
2023-03-05 16:39:13,550:INFO:Orthogonal Matching Pursuit Imported successfully
2023-03-05 16:39:13,560:INFO:Starting cross validation
2023-03-05 16:39:13,564:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:39:13,820:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:39:13,823:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:39:13,856:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:39:13,882:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:39:13,885:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:39:13,898:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:39:13,919:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:39:13,932:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:39:14,108:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:39:14,120:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:39:14,177:INFO:Calculating mean and std
2023-03-05 16:39:14,178:INFO:Creating metrics dataframe
2023-03-05 16:39:14,181:INFO:Uploading results into container
2023-03-05 16:39:14,181:INFO:Uploading model into container now
2023-03-05 16:39:14,182:INFO:_master_model_container: 7
2023-03-05 16:39:14,182:INFO:_display_container: 2
2023-03-05 16:39:14,182:INFO:OrthogonalMatchingPursuit()
2023-03-05 16:39:14,182:INFO:create_model() successfully completed......................................
2023-03-05 16:39:14,257:INFO:SubProcess create_model() end ==================================
2023-03-05 16:39:14,257:INFO:Creating metrics dataframe
2023-03-05 16:39:14,267:INFO:Initializing Bayesian Ridge
2023-03-05 16:39:14,267:INFO:Total runtime is 0.22631027301152545 minutes
2023-03-05 16:39:14,271:INFO:SubProcess create_model() called ==================================
2023-03-05 16:39:14,271:INFO:Initializing create_model()
2023-03-05 16:39:14,271:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7DADDDC0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7DA113A0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:39:14,271:INFO:Checking exceptions
2023-03-05 16:39:14,271:INFO:Importing libraries
2023-03-05 16:39:14,272:INFO:Copying training dataset
2023-03-05 16:39:14,279:INFO:Defining folds
2023-03-05 16:39:14,280:INFO:Declaring metric variables
2023-03-05 16:39:14,284:INFO:Importing untrained model
2023-03-05 16:39:14,290:INFO:Bayesian Ridge Imported successfully
2023-03-05 16:39:14,297:INFO:Starting cross validation
2023-03-05 16:39:14,299:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:39:14,927:INFO:Calculating mean and std
2023-03-05 16:39:14,929:INFO:Creating metrics dataframe
2023-03-05 16:39:14,932:INFO:Uploading results into container
2023-03-05 16:39:14,933:INFO:Uploading model into container now
2023-03-05 16:39:14,933:INFO:_master_model_container: 8
2023-03-05 16:39:14,933:INFO:_display_container: 2
2023-03-05 16:39:14,933:INFO:BayesianRidge()
2023-03-05 16:39:14,933:INFO:create_model() successfully completed......................................
2023-03-05 16:39:15,019:INFO:SubProcess create_model() end ==================================
2023-03-05 16:39:15,019:INFO:Creating metrics dataframe
2023-03-05 16:39:15,028:INFO:Initializing Passive Aggressive Regressor
2023-03-05 16:39:15,028:INFO:Total runtime is 0.23899744749069213 minutes
2023-03-05 16:39:15,032:INFO:SubProcess create_model() called ==================================
2023-03-05 16:39:15,033:INFO:Initializing create_model()
2023-03-05 16:39:15,033:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7DADDDC0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7DA113A0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:39:15,033:INFO:Checking exceptions
2023-03-05 16:39:15,033:INFO:Importing libraries
2023-03-05 16:39:15,033:INFO:Copying training dataset
2023-03-05 16:39:15,039:INFO:Defining folds
2023-03-05 16:39:15,039:INFO:Declaring metric variables
2023-03-05 16:39:15,041:INFO:Importing untrained model
2023-03-05 16:39:15,047:INFO:Passive Aggressive Regressor Imported successfully
2023-03-05 16:39:15,055:INFO:Starting cross validation
2023-03-05 16:39:15,060:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:39:15,364:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2023-03-05 16:39:15,371:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2023-03-05 16:39:15,394:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2023-03-05 16:39:15,411:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2023-03-05 16:39:15,416:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2023-03-05 16:39:15,433:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2023-03-05 16:39:15,447:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2023-03-05 16:39:15,460:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2023-03-05 16:39:15,663:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2023-03-05 16:39:15,674:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  warnings.warn(

2023-03-05 16:39:15,727:INFO:Calculating mean and std
2023-03-05 16:39:15,728:INFO:Creating metrics dataframe
2023-03-05 16:39:15,731:INFO:Uploading results into container
2023-03-05 16:39:15,732:INFO:Uploading model into container now
2023-03-05 16:39:15,732:INFO:_master_model_container: 9
2023-03-05 16:39:15,732:INFO:_display_container: 2
2023-03-05 16:39:15,733:INFO:PassiveAggressiveRegressor(random_state=1201)
2023-03-05 16:39:15,733:INFO:create_model() successfully completed......................................
2023-03-05 16:39:15,818:INFO:SubProcess create_model() end ==================================
2023-03-05 16:39:15,818:INFO:Creating metrics dataframe
2023-03-05 16:39:15,828:INFO:Initializing Huber Regressor
2023-03-05 16:39:15,828:INFO:Total runtime is 0.2523343006769816 minutes
2023-03-05 16:39:15,832:INFO:SubProcess create_model() called ==================================
2023-03-05 16:39:15,832:INFO:Initializing create_model()
2023-03-05 16:39:15,832:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7DADDDC0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7DA113A0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:39:15,832:INFO:Checking exceptions
2023-03-05 16:39:15,832:INFO:Importing libraries
2023-03-05 16:39:15,832:INFO:Copying training dataset
2023-03-05 16:39:15,838:INFO:Defining folds
2023-03-05 16:39:15,838:INFO:Declaring metric variables
2023-03-05 16:39:15,843:INFO:Importing untrained model
2023-03-05 16:39:15,848:INFO:Huber Regressor Imported successfully
2023-03-05 16:39:15,856:INFO:Starting cross validation
2023-03-05 16:39:15,858:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:39:16,531:INFO:Calculating mean and std
2023-03-05 16:39:16,534:INFO:Creating metrics dataframe
2023-03-05 16:39:16,537:INFO:Uploading results into container
2023-03-05 16:39:16,538:INFO:Uploading model into container now
2023-03-05 16:39:16,538:INFO:_master_model_container: 10
2023-03-05 16:39:16,538:INFO:_display_container: 2
2023-03-05 16:39:16,539:INFO:HuberRegressor()
2023-03-05 16:39:16,539:INFO:create_model() successfully completed......................................
2023-03-05 16:39:16,625:INFO:SubProcess create_model() end ==================================
2023-03-05 16:39:16,626:INFO:Creating metrics dataframe
2023-03-05 16:39:16,640:INFO:Initializing K Neighbors Regressor
2023-03-05 16:39:16,641:INFO:Total runtime is 0.2658724308013916 minutes
2023-03-05 16:39:16,645:INFO:SubProcess create_model() called ==================================
2023-03-05 16:39:16,646:INFO:Initializing create_model()
2023-03-05 16:39:16,646:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7DADDDC0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7DA113A0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:39:16,646:INFO:Checking exceptions
2023-03-05 16:39:16,646:INFO:Importing libraries
2023-03-05 16:39:16,646:INFO:Copying training dataset
2023-03-05 16:39:16,652:INFO:Defining folds
2023-03-05 16:39:16,652:INFO:Declaring metric variables
2023-03-05 16:39:16,659:INFO:Importing untrained model
2023-03-05 16:39:16,664:INFO:K Neighbors Regressor Imported successfully
2023-03-05 16:39:16,674:INFO:Starting cross validation
2023-03-05 16:39:16,677:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:39:17,580:INFO:Calculating mean and std
2023-03-05 16:39:17,582:INFO:Creating metrics dataframe
2023-03-05 16:39:17,587:INFO:Uploading results into container
2023-03-05 16:39:17,587:INFO:Uploading model into container now
2023-03-05 16:39:17,588:INFO:_master_model_container: 11
2023-03-05 16:39:17,588:INFO:_display_container: 2
2023-03-05 16:39:17,589:INFO:KNeighborsRegressor(n_jobs=-1)
2023-03-05 16:39:17,589:INFO:create_model() successfully completed......................................
2023-03-05 16:39:17,681:INFO:SubProcess create_model() end ==================================
2023-03-05 16:39:17,681:INFO:Creating metrics dataframe
2023-03-05 16:39:17,694:INFO:Initializing Decision Tree Regressor
2023-03-05 16:39:17,694:INFO:Total runtime is 0.28343690633773805 minutes
2023-03-05 16:39:17,698:INFO:SubProcess create_model() called ==================================
2023-03-05 16:39:17,699:INFO:Initializing create_model()
2023-03-05 16:39:17,699:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7DADDDC0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7DA113A0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:39:17,699:INFO:Checking exceptions
2023-03-05 16:39:17,699:INFO:Importing libraries
2023-03-05 16:39:17,699:INFO:Copying training dataset
2023-03-05 16:39:17,706:INFO:Defining folds
2023-03-05 16:39:17,706:INFO:Declaring metric variables
2023-03-05 16:39:17,711:INFO:Importing untrained model
2023-03-05 16:39:17,716:INFO:Decision Tree Regressor Imported successfully
2023-03-05 16:39:17,726:INFO:Starting cross validation
2023-03-05 16:39:17,730:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:39:18,639:INFO:Calculating mean and std
2023-03-05 16:39:18,641:INFO:Creating metrics dataframe
2023-03-05 16:39:18,645:INFO:Uploading results into container
2023-03-05 16:39:18,646:INFO:Uploading model into container now
2023-03-05 16:39:18,646:INFO:_master_model_container: 12
2023-03-05 16:39:18,646:INFO:_display_container: 2
2023-03-05 16:39:18,646:INFO:DecisionTreeRegressor(random_state=1201)
2023-03-05 16:39:18,646:INFO:create_model() successfully completed......................................
2023-03-05 16:39:18,731:INFO:SubProcess create_model() end ==================================
2023-03-05 16:39:18,731:INFO:Creating metrics dataframe
2023-03-05 16:39:18,745:INFO:Initializing Random Forest Regressor
2023-03-05 16:39:18,745:INFO:Total runtime is 0.30095073382059734 minutes
2023-03-05 16:39:18,749:INFO:SubProcess create_model() called ==================================
2023-03-05 16:39:18,750:INFO:Initializing create_model()
2023-03-05 16:39:18,750:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7DADDDC0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7DA113A0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:39:18,750:INFO:Checking exceptions
2023-03-05 16:39:18,751:INFO:Importing libraries
2023-03-05 16:39:18,751:INFO:Copying training dataset
2023-03-05 16:39:18,758:INFO:Defining folds
2023-03-05 16:39:18,759:INFO:Declaring metric variables
2023-03-05 16:39:18,766:INFO:Importing untrained model
2023-03-05 16:39:18,771:INFO:Random Forest Regressor Imported successfully
2023-03-05 16:39:18,781:INFO:Starting cross validation
2023-03-05 16:39:18,784:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:39:21,085:INFO:Calculating mean and std
2023-03-05 16:39:21,087:INFO:Creating metrics dataframe
2023-03-05 16:39:21,091:INFO:Uploading results into container
2023-03-05 16:39:21,092:INFO:Uploading model into container now
2023-03-05 16:39:21,093:INFO:_master_model_container: 13
2023-03-05 16:39:21,093:INFO:_display_container: 2
2023-03-05 16:39:21,093:INFO:RandomForestRegressor(n_jobs=-1, random_state=1201)
2023-03-05 16:39:21,094:INFO:create_model() successfully completed......................................
2023-03-05 16:39:21,186:INFO:SubProcess create_model() end ==================================
2023-03-05 16:39:21,186:INFO:Creating metrics dataframe
2023-03-05 16:39:21,198:INFO:Initializing Extra Trees Regressor
2023-03-05 16:39:21,198:INFO:Total runtime is 0.3418328523635864 minutes
2023-03-05 16:39:21,202:INFO:SubProcess create_model() called ==================================
2023-03-05 16:39:21,202:INFO:Initializing create_model()
2023-03-05 16:39:21,202:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7DADDDC0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7DA113A0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:39:21,202:INFO:Checking exceptions
2023-03-05 16:39:21,203:INFO:Importing libraries
2023-03-05 16:39:21,203:INFO:Copying training dataset
2023-03-05 16:39:21,209:INFO:Defining folds
2023-03-05 16:39:21,210:INFO:Declaring metric variables
2023-03-05 16:39:21,214:INFO:Importing untrained model
2023-03-05 16:39:21,219:INFO:Extra Trees Regressor Imported successfully
2023-03-05 16:39:21,229:INFO:Starting cross validation
2023-03-05 16:39:21,232:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:39:23,413:INFO:Calculating mean and std
2023-03-05 16:39:23,415:INFO:Creating metrics dataframe
2023-03-05 16:39:23,419:INFO:Uploading results into container
2023-03-05 16:39:23,420:INFO:Uploading model into container now
2023-03-05 16:39:23,420:INFO:_master_model_container: 14
2023-03-05 16:39:23,421:INFO:_display_container: 2
2023-03-05 16:39:23,421:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=1201)
2023-03-05 16:39:23,421:INFO:create_model() successfully completed......................................
2023-03-05 16:39:23,511:INFO:SubProcess create_model() end ==================================
2023-03-05 16:39:23,511:INFO:Creating metrics dataframe
2023-03-05 16:39:23,525:INFO:Initializing AdaBoost Regressor
2023-03-05 16:39:23,525:INFO:Total runtime is 0.3806116859118144 minutes
2023-03-05 16:39:23,529:INFO:SubProcess create_model() called ==================================
2023-03-05 16:39:23,529:INFO:Initializing create_model()
2023-03-05 16:39:23,529:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7DADDDC0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7DA113A0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:39:23,529:INFO:Checking exceptions
2023-03-05 16:39:23,529:INFO:Importing libraries
2023-03-05 16:39:23,529:INFO:Copying training dataset
2023-03-05 16:39:23,535:INFO:Defining folds
2023-03-05 16:39:23,535:INFO:Declaring metric variables
2023-03-05 16:39:23,540:INFO:Importing untrained model
2023-03-05 16:39:23,545:INFO:AdaBoost Regressor Imported successfully
2023-03-05 16:39:23,554:INFO:Starting cross validation
2023-03-05 16:39:23,556:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:39:24,963:INFO:Calculating mean and std
2023-03-05 16:39:24,965:INFO:Creating metrics dataframe
2023-03-05 16:39:24,969:INFO:Uploading results into container
2023-03-05 16:39:24,970:INFO:Uploading model into container now
2023-03-05 16:39:24,970:INFO:_master_model_container: 15
2023-03-05 16:39:24,970:INFO:_display_container: 2
2023-03-05 16:39:24,971:INFO:AdaBoostRegressor(random_state=1201)
2023-03-05 16:39:24,971:INFO:create_model() successfully completed......................................
2023-03-05 16:39:25,063:INFO:SubProcess create_model() end ==================================
2023-03-05 16:39:25,063:INFO:Creating metrics dataframe
2023-03-05 16:39:25,077:INFO:Initializing Gradient Boosting Regressor
2023-03-05 16:39:25,077:INFO:Total runtime is 0.40648353497187295 minutes
2023-03-05 16:39:25,080:INFO:SubProcess create_model() called ==================================
2023-03-05 16:39:25,081:INFO:Initializing create_model()
2023-03-05 16:39:25,081:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7DADDDC0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7DA113A0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:39:25,081:INFO:Checking exceptions
2023-03-05 16:39:25,081:INFO:Importing libraries
2023-03-05 16:39:25,081:INFO:Copying training dataset
2023-03-05 16:39:25,090:INFO:Defining folds
2023-03-05 16:39:25,090:INFO:Declaring metric variables
2023-03-05 16:39:25,094:INFO:Importing untrained model
2023-03-05 16:39:25,100:INFO:Gradient Boosting Regressor Imported successfully
2023-03-05 16:39:25,110:INFO:Starting cross validation
2023-03-05 16:39:25,113:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:39:26,457:INFO:Calculating mean and std
2023-03-05 16:39:26,459:INFO:Creating metrics dataframe
2023-03-05 16:39:26,463:INFO:Uploading results into container
2023-03-05 16:39:26,464:INFO:Uploading model into container now
2023-03-05 16:39:26,464:INFO:_master_model_container: 16
2023-03-05 16:39:26,464:INFO:_display_container: 2
2023-03-05 16:39:26,464:INFO:GradientBoostingRegressor(random_state=1201)
2023-03-05 16:39:26,464:INFO:create_model() successfully completed......................................
2023-03-05 16:39:26,547:INFO:SubProcess create_model() end ==================================
2023-03-05 16:39:26,547:INFO:Creating metrics dataframe
2023-03-05 16:39:26,562:INFO:Initializing Extreme Gradient Boosting
2023-03-05 16:39:26,562:INFO:Total runtime is 0.43122573296229044 minutes
2023-03-05 16:39:26,567:INFO:SubProcess create_model() called ==================================
2023-03-05 16:39:26,567:INFO:Initializing create_model()
2023-03-05 16:39:26,568:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7DADDDC0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7DA113A0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:39:26,568:INFO:Checking exceptions
2023-03-05 16:39:26,568:INFO:Importing libraries
2023-03-05 16:39:26,568:INFO:Copying training dataset
2023-03-05 16:39:26,577:INFO:Defining folds
2023-03-05 16:39:26,577:INFO:Declaring metric variables
2023-03-05 16:39:26,583:INFO:Importing untrained model
2023-03-05 16:39:26,588:INFO:Extreme Gradient Boosting Imported successfully
2023-03-05 16:39:26,598:INFO:Starting cross validation
2023-03-05 16:39:26,600:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:39:28,248:INFO:Calculating mean and std
2023-03-05 16:39:28,250:INFO:Creating metrics dataframe
2023-03-05 16:39:28,254:INFO:Uploading results into container
2023-03-05 16:39:28,255:INFO:Uploading model into container now
2023-03-05 16:39:28,255:INFO:_master_model_container: 17
2023-03-05 16:39:28,255:INFO:_display_container: 2
2023-03-05 16:39:28,256:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=1201, ...)
2023-03-05 16:39:28,257:INFO:create_model() successfully completed......................................
2023-03-05 16:39:28,341:INFO:SubProcess create_model() end ==================================
2023-03-05 16:39:28,342:INFO:Creating metrics dataframe
2023-03-05 16:39:28,356:INFO:Initializing Light Gradient Boosting Machine
2023-03-05 16:39:28,356:INFO:Total runtime is 0.46113537152608236 minutes
2023-03-05 16:39:28,361:INFO:SubProcess create_model() called ==================================
2023-03-05 16:39:28,362:INFO:Initializing create_model()
2023-03-05 16:39:28,363:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7DADDDC0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7DA113A0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:39:28,363:INFO:Checking exceptions
2023-03-05 16:39:28,363:INFO:Importing libraries
2023-03-05 16:39:28,363:INFO:Copying training dataset
2023-03-05 16:39:28,371:INFO:Defining folds
2023-03-05 16:39:28,371:INFO:Declaring metric variables
2023-03-05 16:39:28,376:INFO:Importing untrained model
2023-03-05 16:39:28,381:INFO:Light Gradient Boosting Machine Imported successfully
2023-03-05 16:39:28,391:INFO:Starting cross validation
2023-03-05 16:39:28,393:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:39:31,654:INFO:Calculating mean and std
2023-03-05 16:39:31,656:INFO:Creating metrics dataframe
2023-03-05 16:39:31,660:INFO:Uploading results into container
2023-03-05 16:39:31,660:INFO:Uploading model into container now
2023-03-05 16:39:31,661:INFO:_master_model_container: 18
2023-03-05 16:39:31,661:INFO:_display_container: 2
2023-03-05 16:39:31,661:INFO:LGBMRegressor(random_state=1201)
2023-03-05 16:39:31,662:INFO:create_model() successfully completed......................................
2023-03-05 16:39:31,748:INFO:SubProcess create_model() end ==================================
2023-03-05 16:39:31,748:INFO:Creating metrics dataframe
2023-03-05 16:39:31,763:INFO:Initializing Dummy Regressor
2023-03-05 16:39:31,763:INFO:Total runtime is 0.5179207523663839 minutes
2023-03-05 16:39:31,768:INFO:SubProcess create_model() called ==================================
2023-03-05 16:39:31,768:INFO:Initializing create_model()
2023-03-05 16:39:31,768:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7DADDDC0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7DA113A0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:39:31,768:INFO:Checking exceptions
2023-03-05 16:39:31,768:INFO:Importing libraries
2023-03-05 16:39:31,768:INFO:Copying training dataset
2023-03-05 16:39:31,775:INFO:Defining folds
2023-03-05 16:39:31,776:INFO:Declaring metric variables
2023-03-05 16:39:31,781:INFO:Importing untrained model
2023-03-05 16:39:31,786:INFO:Dummy Regressor Imported successfully
2023-03-05 16:39:31,795:INFO:Starting cross validation
2023-03-05 16:39:31,797:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:39:32,656:INFO:Calculating mean and std
2023-03-05 16:39:32,657:INFO:Creating metrics dataframe
2023-03-05 16:39:32,661:INFO:Uploading results into container
2023-03-05 16:39:32,662:INFO:Uploading model into container now
2023-03-05 16:39:32,662:INFO:_master_model_container: 19
2023-03-05 16:39:32,663:INFO:_display_container: 2
2023-03-05 16:39:32,663:INFO:DummyRegressor()
2023-03-05 16:39:32,664:INFO:create_model() successfully completed......................................
2023-03-05 16:39:32,747:INFO:SubProcess create_model() end ==================================
2023-03-05 16:39:32,747:INFO:Creating metrics dataframe
2023-03-05 16:39:32,774:INFO:Initializing create_model()
2023-03-05 16:39:32,775:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7DADDDC0>, estimator=HuberRegressor(), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:39:32,775:INFO:Checking exceptions
2023-03-05 16:39:32,777:INFO:Importing libraries
2023-03-05 16:39:32,777:INFO:Copying training dataset
2023-03-05 16:39:32,781:INFO:Defining folds
2023-03-05 16:39:32,781:INFO:Declaring metric variables
2023-03-05 16:39:32,781:INFO:Importing untrained model
2023-03-05 16:39:32,781:INFO:Declaring custom model
2023-03-05 16:39:32,782:INFO:Huber Regressor Imported successfully
2023-03-05 16:39:32,783:INFO:Cross validation set to False
2023-03-05 16:39:32,783:INFO:Fitting Model
2023-03-05 16:39:32,922:INFO:HuberRegressor()
2023-03-05 16:39:32,922:INFO:create_model() successfully completed......................................
2023-03-05 16:39:33,037:INFO:_master_model_container: 19
2023-03-05 16:39:33,037:INFO:_display_container: 2
2023-03-05 16:39:33,038:INFO:HuberRegressor()
2023-03-05 16:39:33,039:INFO:compare_models() successfully completed......................................
2023-03-05 16:40:22,727:INFO:PyCaret RegressionExperiment
2023-03-05 16:40:22,727:INFO:Logging name: reg-default-name
2023-03-05 16:40:22,728:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-05 16:40:22,728:INFO:version 3.0.0.rc9
2023-03-05 16:40:22,728:INFO:Initializing setup()
2023-03-05 16:40:22,728:INFO:self.USI: 000e
2023-03-05 16:40:22,728:INFO:self._variable_keys: {'target_param', 'pipeline', 'idx', 'html_param', 'transform_target_param', 'fold_shuffle_param', 'log_plots_param', '_ml_usecase', 'y', 'logging_param', 'exp_name_log', 'fold_generator', 'X_test', 'n_jobs_param', 'USI', 'fold_groups_param', 'exp_id', 'seed', 'gpu_param', 'X', 'y_train', 'X_train', 'data', 'y_test', 'gpu_n_jobs_param', 'memory', '_available_plots'}
2023-03-05 16:40:22,728:INFO:Checking environment
2023-03-05 16:40:22,728:INFO:python_version: 3.9.13
2023-03-05 16:40:22,728:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-03-05 16:40:22,728:INFO:machine: AMD64
2023-03-05 16:40:22,728:INFO:platform: Windows-10-10.0.19044-SP0
2023-03-05 16:40:22,728:INFO:Memory: svmem(total=17009516544, available=4875067392, percent=71.3, used=12134449152, free=4875067392)
2023-03-05 16:40:22,728:INFO:Physical Core: 4
2023-03-05 16:40:22,728:INFO:Logical Core: 8
2023-03-05 16:40:22,728:INFO:Checking libraries
2023-03-05 16:40:22,728:INFO:System:
2023-03-05 16:40:22,728:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-03-05 16:40:22,728:INFO:executable: C:\Users\Alvaro\anaconda3\python.exe
2023-03-05 16:40:22,728:INFO:   machine: Windows-10-10.0.19044-SP0
2023-03-05 16:40:22,728:INFO:PyCaret required dependencies:
2023-03-05 16:40:22,729:INFO:                 pip: 22.2.2
2023-03-05 16:40:22,729:INFO:          setuptools: 63.4.1
2023-03-05 16:40:22,729:INFO:             pycaret: 3.0.0rc9
2023-03-05 16:40:22,729:INFO:             IPython: 7.31.1
2023-03-05 16:40:22,729:INFO:          ipywidgets: 7.6.5
2023-03-05 16:40:22,729:INFO:                tqdm: 4.64.1
2023-03-05 16:40:22,729:INFO:               numpy: 1.21.5
2023-03-05 16:40:22,729:INFO:              pandas: 1.4.4
2023-03-05 16:40:22,729:INFO:              jinja2: 2.11.3
2023-03-05 16:40:22,729:INFO:               scipy: 1.9.1
2023-03-05 16:40:22,729:INFO:              joblib: 1.2.0
2023-03-05 16:40:22,729:INFO:             sklearn: 1.0.2
2023-03-05 16:40:22,729:INFO:                pyod: 1.0.7
2023-03-05 16:40:22,729:INFO:            imblearn: 0.10.1
2023-03-05 16:40:22,729:INFO:   category_encoders: 2.6.0
2023-03-05 16:40:22,729:INFO:            lightgbm: 3.3.5
2023-03-05 16:40:22,729:INFO:               numba: 0.55.1
2023-03-05 16:40:22,729:INFO:            requests: 2.28.1
2023-03-05 16:40:22,729:INFO:          matplotlib: 3.5.2
2023-03-05 16:40:22,729:INFO:          scikitplot: 0.3.7
2023-03-05 16:40:22,729:INFO:         yellowbrick: 1.5
2023-03-05 16:40:22,729:INFO:              plotly: 5.9.0
2023-03-05 16:40:22,729:INFO:             kaleido: 0.2.1
2023-03-05 16:40:22,730:INFO:         statsmodels: 0.13.2
2023-03-05 16:40:22,730:INFO:              sktime: 0.16.1
2023-03-05 16:40:22,730:INFO:               tbats: 1.1.2
2023-03-05 16:40:22,730:INFO:            pmdarima: 2.0.2
2023-03-05 16:40:22,730:INFO:              psutil: 5.9.0
2023-03-05 16:40:22,730:INFO:PyCaret optional dependencies:
2023-03-05 16:40:22,730:INFO:                shap: Not installed
2023-03-05 16:40:22,730:INFO:           interpret: Not installed
2023-03-05 16:40:22,730:INFO:                umap: Not installed
2023-03-05 16:40:22,730:INFO:    pandas_profiling: Not installed
2023-03-05 16:40:22,730:INFO:  explainerdashboard: Not installed
2023-03-05 16:40:22,730:INFO:             autoviz: Not installed
2023-03-05 16:40:22,730:INFO:           fairlearn: Not installed
2023-03-05 16:40:22,730:INFO:             xgboost: 1.7.4
2023-03-05 16:40:22,730:INFO:            catboost: Not installed
2023-03-05 16:40:22,730:INFO:              kmodes: Not installed
2023-03-05 16:40:22,730:INFO:             mlxtend: Not installed
2023-03-05 16:40:22,730:INFO:       statsforecast: Not installed
2023-03-05 16:40:22,730:INFO:        tune_sklearn: Not installed
2023-03-05 16:40:22,730:INFO:                 ray: Not installed
2023-03-05 16:40:22,730:INFO:            hyperopt: Not installed
2023-03-05 16:40:22,730:INFO:              optuna: Not installed
2023-03-05 16:40:22,730:INFO:               skopt: Not installed
2023-03-05 16:40:22,731:INFO:              mlflow: Not installed
2023-03-05 16:40:22,731:INFO:              gradio: Not installed
2023-03-05 16:40:22,731:INFO:             fastapi: Not installed
2023-03-05 16:40:22,731:INFO:             uvicorn: Not installed
2023-03-05 16:40:22,731:INFO:              m2cgen: Not installed
2023-03-05 16:40:22,731:INFO:           evidently: Not installed
2023-03-05 16:40:22,731:INFO:               fugue: Not installed
2023-03-05 16:40:22,731:INFO:           streamlit: Not installed
2023-03-05 16:40:22,731:INFO:             prophet: Not installed
2023-03-05 16:40:22,731:INFO:None
2023-03-05 16:40:22,731:INFO:Set up data.
2023-03-05 16:40:22,738:INFO:Set up train/test split.
2023-03-05 16:40:22,742:INFO:Set up index.
2023-03-05 16:40:22,742:INFO:Set up folding strategy.
2023-03-05 16:40:22,742:INFO:Assigning column types.
2023-03-05 16:40:22,745:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-05 16:40:22,745:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-05 16:40:22,751:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 16:40:22,757:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:40:22,814:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:40:22,858:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:40:22,859:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:40:22,862:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:40:22,862:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-05 16:40:22,867:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 16:40:22,871:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:40:22,922:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:40:22,963:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:40:22,964:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:40:22,967:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:40:22,967:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-03-05 16:40:22,972:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 16:40:22,976:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:40:23,027:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:40:23,068:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:40:23,068:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:40:23,070:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:40:23,075:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 16:40:23,078:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:40:23,139:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:40:23,185:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:40:23,186:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:40:23,188:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:40:23,189:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-03-05 16:40:23,197:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:40:23,252:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:40:23,292:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:40:23,293:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:40:23,295:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:40:23,304:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:40:23,357:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:40:23,398:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:40:23,398:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:40:23,401:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:40:23,401:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-03-05 16:40:23,464:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:40:23,504:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:40:23,504:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:40:23,507:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:40:23,569:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:40:23,613:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:40:23,613:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:40:23,616:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:40:23,616:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-05 16:40:23,677:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:40:23,718:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:40:23,721:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:40:23,783:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:40:23,824:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:40:23,826:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:40:23,827:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-03-05 16:40:23,940:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:40:23,943:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:40:24,044:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:40:24,046:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:40:24,052:INFO:Preparing preprocessing pipeline...
2023-03-05 16:40:24,052:INFO:Set up simple imputation.
2023-03-05 16:40:24,054:INFO:Set up encoding of categorical features.
2023-03-05 16:40:24,131:INFO:Finished creating preprocessing pipeline.
2023-03-05 16:40:24,139:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Alvaro\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=[], transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['experience_level',
                                             'employment_type', 'job_title',
                                             'employee_residence',
                                             'company_size'],
                                    transformer=SimpleImputer(strategy='most_frequ...
                                             'company_size'],
                                    transformer=OneHotEncoder(cols=['experience_level',
                                                                    'employment_type',
                                                                    'company_size'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['job_title', 'employee_residence'],
                                    transformer=LeaveOneOutEncoder(cols=['job_title',
                                                                         'employee_residence'],
                                                                   handle_missing='return_nan',
                                                                   random_state=4126)))])
2023-03-05 16:40:24,139:INFO:Creating final display dataframe.
2023-03-05 16:40:24,490:INFO:Setup _display_container:                     Description             Value
0                    Session id              4126
1                        Target     salary_in_usd
2                   Target type        Regression
3           Original data shape          (500, 6)
4        Transformed data shape         (500, 14)
5   Transformed train set shape         (350, 14)
6    Transformed test set shape         (150, 14)
7          Categorical features                 5
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12     Maximum one-hot encoding                25
13              Encoding method              None
14               Fold Generator             KFold
15                  Fold Number                10
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  reg-default-name
20                          USI              000e
2023-03-05 16:40:24,612:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:40:24,614:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:40:24,718:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:40:24,721:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:40:24,721:INFO:setup() successfully completed in 2.0s...............
2023-03-05 16:40:24,729:INFO:Initializing compare_models()
2023-03-05 16:40:24,729:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7E535FD0>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000013B7E535FD0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-03-05 16:40:24,729:INFO:Checking exceptions
2023-03-05 16:40:24,732:INFO:Preparing display monitor
2023-03-05 16:40:24,772:INFO:Initializing Linear Regression
2023-03-05 16:40:24,773:INFO:Total runtime is 1.5803178151448567e-05 minutes
2023-03-05 16:40:24,781:INFO:SubProcess create_model() called ==================================
2023-03-05 16:40:24,781:INFO:Initializing create_model()
2023-03-05 16:40:24,781:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7E535FD0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7EC4D490>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:40:24,781:INFO:Checking exceptions
2023-03-05 16:40:24,781:INFO:Importing libraries
2023-03-05 16:40:24,781:INFO:Copying training dataset
2023-03-05 16:40:24,787:INFO:Defining folds
2023-03-05 16:40:24,788:INFO:Declaring metric variables
2023-03-05 16:40:24,794:INFO:Importing untrained model
2023-03-05 16:40:24,797:INFO:Linear Regression Imported successfully
2023-03-05 16:40:24,805:INFO:Starting cross validation
2023-03-05 16:40:24,807:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:40:25,391:INFO:Calculating mean and std
2023-03-05 16:40:25,392:INFO:Creating metrics dataframe
2023-03-05 16:40:25,394:INFO:Uploading results into container
2023-03-05 16:40:25,395:INFO:Uploading model into container now
2023-03-05 16:40:25,395:INFO:_master_model_container: 1
2023-03-05 16:40:25,395:INFO:_display_container: 2
2023-03-05 16:40:25,395:INFO:LinearRegression(n_jobs=-1)
2023-03-05 16:40:25,395:INFO:create_model() successfully completed......................................
2023-03-05 16:40:25,473:INFO:SubProcess create_model() end ==================================
2023-03-05 16:40:25,473:INFO:Creating metrics dataframe
2023-03-05 16:40:25,480:INFO:Initializing Lasso Regression
2023-03-05 16:40:25,480:INFO:Total runtime is 0.011797618865966798 minutes
2023-03-05 16:40:25,483:INFO:SubProcess create_model() called ==================================
2023-03-05 16:40:25,484:INFO:Initializing create_model()
2023-03-05 16:40:25,484:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7E535FD0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7EC4D490>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:40:25,484:INFO:Checking exceptions
2023-03-05 16:40:25,484:INFO:Importing libraries
2023-03-05 16:40:25,484:INFO:Copying training dataset
2023-03-05 16:40:25,488:INFO:Defining folds
2023-03-05 16:40:25,488:INFO:Declaring metric variables
2023-03-05 16:40:25,491:INFO:Importing untrained model
2023-03-05 16:40:25,495:INFO:Lasso Regression Imported successfully
2023-03-05 16:40:25,504:INFO:Starting cross validation
2023-03-05 16:40:25,507:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:40:25,851:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.110e+09, tolerance: 1.482e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-05 16:40:26,135:INFO:Calculating mean and std
2023-03-05 16:40:26,137:INFO:Creating metrics dataframe
2023-03-05 16:40:26,139:INFO:Uploading results into container
2023-03-05 16:40:26,140:INFO:Uploading model into container now
2023-03-05 16:40:26,140:INFO:_master_model_container: 2
2023-03-05 16:40:26,140:INFO:_display_container: 2
2023-03-05 16:40:26,140:INFO:Lasso(random_state=4126)
2023-03-05 16:40:26,140:INFO:create_model() successfully completed......................................
2023-03-05 16:40:26,217:INFO:SubProcess create_model() end ==================================
2023-03-05 16:40:26,217:INFO:Creating metrics dataframe
2023-03-05 16:40:26,229:INFO:Initializing Ridge Regression
2023-03-05 16:40:26,230:INFO:Total runtime is 0.02429998715718587 minutes
2023-03-05 16:40:26,234:INFO:SubProcess create_model() called ==================================
2023-03-05 16:40:26,234:INFO:Initializing create_model()
2023-03-05 16:40:26,234:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7E535FD0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7EC4D490>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:40:26,235:INFO:Checking exceptions
2023-03-05 16:40:26,235:INFO:Importing libraries
2023-03-05 16:40:26,235:INFO:Copying training dataset
2023-03-05 16:40:26,242:INFO:Defining folds
2023-03-05 16:40:26,242:INFO:Declaring metric variables
2023-03-05 16:40:26,247:INFO:Importing untrained model
2023-03-05 16:40:26,252:INFO:Ridge Regression Imported successfully
2023-03-05 16:40:26,263:INFO:Starting cross validation
2023-03-05 16:40:26,266:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:40:26,843:INFO:Calculating mean and std
2023-03-05 16:40:26,845:INFO:Creating metrics dataframe
2023-03-05 16:40:26,848:INFO:Uploading results into container
2023-03-05 16:40:26,849:INFO:Uploading model into container now
2023-03-05 16:40:26,849:INFO:_master_model_container: 3
2023-03-05 16:40:26,849:INFO:_display_container: 2
2023-03-05 16:40:26,850:INFO:Ridge(random_state=4126)
2023-03-05 16:40:26,850:INFO:create_model() successfully completed......................................
2023-03-05 16:40:26,939:INFO:SubProcess create_model() end ==================================
2023-03-05 16:40:26,939:INFO:Creating metrics dataframe
2023-03-05 16:40:26,948:INFO:Initializing Elastic Net
2023-03-05 16:40:26,948:INFO:Total runtime is 0.036271329720815024 minutes
2023-03-05 16:40:26,952:INFO:SubProcess create_model() called ==================================
2023-03-05 16:40:26,952:INFO:Initializing create_model()
2023-03-05 16:40:26,952:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7E535FD0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7EC4D490>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:40:26,952:INFO:Checking exceptions
2023-03-05 16:40:26,952:INFO:Importing libraries
2023-03-05 16:40:26,953:INFO:Copying training dataset
2023-03-05 16:40:26,958:INFO:Defining folds
2023-03-05 16:40:26,958:INFO:Declaring metric variables
2023-03-05 16:40:26,961:INFO:Importing untrained model
2023-03-05 16:40:26,966:INFO:Elastic Net Imported successfully
2023-03-05 16:40:26,973:INFO:Starting cross validation
2023-03-05 16:40:26,975:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:40:27,571:INFO:Calculating mean and std
2023-03-05 16:40:27,573:INFO:Creating metrics dataframe
2023-03-05 16:40:27,576:INFO:Uploading results into container
2023-03-05 16:40:27,577:INFO:Uploading model into container now
2023-03-05 16:40:27,578:INFO:_master_model_container: 4
2023-03-05 16:40:27,578:INFO:_display_container: 2
2023-03-05 16:40:27,578:INFO:ElasticNet(random_state=4126)
2023-03-05 16:40:27,578:INFO:create_model() successfully completed......................................
2023-03-05 16:40:27,660:INFO:SubProcess create_model() end ==================================
2023-03-05 16:40:27,660:INFO:Creating metrics dataframe
2023-03-05 16:40:27,669:INFO:Initializing Least Angle Regression
2023-03-05 16:40:27,669:INFO:Total runtime is 0.048291027545928955 minutes
2023-03-05 16:40:27,674:INFO:SubProcess create_model() called ==================================
2023-03-05 16:40:27,674:INFO:Initializing create_model()
2023-03-05 16:40:27,675:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7E535FD0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7EC4D490>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:40:27,675:INFO:Checking exceptions
2023-03-05 16:40:27,675:INFO:Importing libraries
2023-03-05 16:40:27,675:INFO:Copying training dataset
2023-03-05 16:40:27,680:INFO:Defining folds
2023-03-05 16:40:27,680:INFO:Declaring metric variables
2023-03-05 16:40:27,685:INFO:Importing untrained model
2023-03-05 16:40:27,688:INFO:Least Angle Regression Imported successfully
2023-03-05 16:40:27,699:INFO:Starting cross validation
2023-03-05 16:40:27,701:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:40:27,953:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:40:27,958:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:40:27,993:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:40:28,014:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:40:28,020:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:40:28,039:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:40:28,043:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:40:28,056:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:40:28,326:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:40:28,335:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:40:28,407:INFO:Calculating mean and std
2023-03-05 16:40:28,408:INFO:Creating metrics dataframe
2023-03-05 16:40:28,412:INFO:Uploading results into container
2023-03-05 16:40:28,413:INFO:Uploading model into container now
2023-03-05 16:40:28,414:INFO:_master_model_container: 5
2023-03-05 16:40:28,414:INFO:_display_container: 2
2023-03-05 16:40:28,414:INFO:Lars(random_state=4126)
2023-03-05 16:40:28,414:INFO:create_model() successfully completed......................................
2023-03-05 16:40:28,508:INFO:SubProcess create_model() end ==================================
2023-03-05 16:40:28,508:INFO:Creating metrics dataframe
2023-03-05 16:40:28,517:INFO:Initializing Lasso Least Angle Regression
2023-03-05 16:40:28,517:INFO:Total runtime is 0.062421031792958576 minutes
2023-03-05 16:40:28,521:INFO:SubProcess create_model() called ==================================
2023-03-05 16:40:28,521:INFO:Initializing create_model()
2023-03-05 16:40:28,521:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7E535FD0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7EC4D490>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:40:28,521:INFO:Checking exceptions
2023-03-05 16:40:28,521:INFO:Importing libraries
2023-03-05 16:40:28,521:INFO:Copying training dataset
2023-03-05 16:40:28,525:INFO:Defining folds
2023-03-05 16:40:28,525:INFO:Declaring metric variables
2023-03-05 16:40:28,529:INFO:Importing untrained model
2023-03-05 16:40:28,532:INFO:Lasso Least Angle Regression Imported successfully
2023-03-05 16:40:28,538:INFO:Starting cross validation
2023-03-05 16:40:28,539:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:40:28,831:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:40:28,848:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:40:28,870:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:40:28,875:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:40:28,917:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:40:28,918:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:40:28,950:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:40:28,954:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:40:29,229:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:40:29,238:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:40:29,298:INFO:Calculating mean and std
2023-03-05 16:40:29,300:INFO:Creating metrics dataframe
2023-03-05 16:40:29,303:INFO:Uploading results into container
2023-03-05 16:40:29,304:INFO:Uploading model into container now
2023-03-05 16:40:29,304:INFO:_master_model_container: 6
2023-03-05 16:40:29,304:INFO:_display_container: 2
2023-03-05 16:40:29,304:INFO:LassoLars(random_state=4126)
2023-03-05 16:40:29,305:INFO:create_model() successfully completed......................................
2023-03-05 16:40:29,384:INFO:SubProcess create_model() end ==================================
2023-03-05 16:40:29,384:INFO:Creating metrics dataframe
2023-03-05 16:40:29,394:INFO:Initializing Orthogonal Matching Pursuit
2023-03-05 16:40:29,394:INFO:Total runtime is 0.07702929178873698 minutes
2023-03-05 16:40:29,397:INFO:SubProcess create_model() called ==================================
2023-03-05 16:40:29,397:INFO:Initializing create_model()
2023-03-05 16:40:29,397:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7E535FD0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7EC4D490>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:40:29,397:INFO:Checking exceptions
2023-03-05 16:40:29,397:INFO:Importing libraries
2023-03-05 16:40:29,397:INFO:Copying training dataset
2023-03-05 16:40:29,403:INFO:Defining folds
2023-03-05 16:40:29,403:INFO:Declaring metric variables
2023-03-05 16:40:29,406:INFO:Importing untrained model
2023-03-05 16:40:29,410:INFO:Orthogonal Matching Pursuit Imported successfully
2023-03-05 16:40:29,419:INFO:Starting cross validation
2023-03-05 16:40:29,420:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:40:29,687:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:40:29,692:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:40:29,723:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:40:29,733:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:40:29,750:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:40:29,751:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:40:29,752:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:40:29,799:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:40:29,975:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:40:29,976:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:40:30,038:INFO:Calculating mean and std
2023-03-05 16:40:30,039:INFO:Creating metrics dataframe
2023-03-05 16:40:30,042:INFO:Uploading results into container
2023-03-05 16:40:30,043:INFO:Uploading model into container now
2023-03-05 16:40:30,043:INFO:_master_model_container: 7
2023-03-05 16:40:30,043:INFO:_display_container: 2
2023-03-05 16:40:30,044:INFO:OrthogonalMatchingPursuit()
2023-03-05 16:40:30,044:INFO:create_model() successfully completed......................................
2023-03-05 16:40:30,124:INFO:SubProcess create_model() end ==================================
2023-03-05 16:40:30,124:INFO:Creating metrics dataframe
2023-03-05 16:40:30,134:INFO:Initializing Bayesian Ridge
2023-03-05 16:40:30,134:INFO:Total runtime is 0.08936302661895752 minutes
2023-03-05 16:40:30,138:INFO:SubProcess create_model() called ==================================
2023-03-05 16:40:30,138:INFO:Initializing create_model()
2023-03-05 16:40:30,138:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7E535FD0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7EC4D490>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:40:30,139:INFO:Checking exceptions
2023-03-05 16:40:30,139:INFO:Importing libraries
2023-03-05 16:40:30,139:INFO:Copying training dataset
2023-03-05 16:40:30,145:INFO:Defining folds
2023-03-05 16:40:30,145:INFO:Declaring metric variables
2023-03-05 16:40:30,149:INFO:Importing untrained model
2023-03-05 16:40:30,154:INFO:Bayesian Ridge Imported successfully
2023-03-05 16:40:30,161:INFO:Starting cross validation
2023-03-05 16:40:30,163:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:40:30,773:INFO:Calculating mean and std
2023-03-05 16:40:30,774:INFO:Creating metrics dataframe
2023-03-05 16:40:30,778:INFO:Uploading results into container
2023-03-05 16:40:30,779:INFO:Uploading model into container now
2023-03-05 16:40:30,780:INFO:_master_model_container: 8
2023-03-05 16:40:30,780:INFO:_display_container: 2
2023-03-05 16:40:30,781:INFO:BayesianRidge()
2023-03-05 16:40:30,781:INFO:create_model() successfully completed......................................
2023-03-05 16:40:30,886:INFO:SubProcess create_model() end ==================================
2023-03-05 16:40:30,887:INFO:Creating metrics dataframe
2023-03-05 16:40:30,896:INFO:Initializing Passive Aggressive Regressor
2023-03-05 16:40:30,896:INFO:Total runtime is 0.10206818183263143 minutes
2023-03-05 16:40:30,899:INFO:SubProcess create_model() called ==================================
2023-03-05 16:40:30,900:INFO:Initializing create_model()
2023-03-05 16:40:30,900:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7E535FD0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7EC4D490>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:40:30,900:INFO:Checking exceptions
2023-03-05 16:40:30,900:INFO:Importing libraries
2023-03-05 16:40:30,900:INFO:Copying training dataset
2023-03-05 16:40:30,907:INFO:Defining folds
2023-03-05 16:40:30,907:INFO:Declaring metric variables
2023-03-05 16:40:30,911:INFO:Importing untrained model
2023-03-05 16:40:30,915:INFO:Passive Aggressive Regressor Imported successfully
2023-03-05 16:40:30,924:INFO:Starting cross validation
2023-03-05 16:40:30,926:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:40:31,503:INFO:Calculating mean and std
2023-03-05 16:40:31,504:INFO:Creating metrics dataframe
2023-03-05 16:40:31,507:INFO:Uploading results into container
2023-03-05 16:40:31,508:INFO:Uploading model into container now
2023-03-05 16:40:31,508:INFO:_master_model_container: 9
2023-03-05 16:40:31,508:INFO:_display_container: 2
2023-03-05 16:40:31,509:INFO:PassiveAggressiveRegressor(random_state=4126)
2023-03-05 16:40:31,509:INFO:create_model() successfully completed......................................
2023-03-05 16:40:31,588:INFO:SubProcess create_model() end ==================================
2023-03-05 16:40:31,589:INFO:Creating metrics dataframe
2023-03-05 16:40:31,598:INFO:Initializing Huber Regressor
2023-03-05 16:40:31,598:INFO:Total runtime is 0.11376323302586873 minutes
2023-03-05 16:40:31,602:INFO:SubProcess create_model() called ==================================
2023-03-05 16:40:31,602:INFO:Initializing create_model()
2023-03-05 16:40:31,602:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7E535FD0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7EC4D490>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:40:31,602:INFO:Checking exceptions
2023-03-05 16:40:31,602:INFO:Importing libraries
2023-03-05 16:40:31,602:INFO:Copying training dataset
2023-03-05 16:40:31,607:INFO:Defining folds
2023-03-05 16:40:31,607:INFO:Declaring metric variables
2023-03-05 16:40:31,612:INFO:Importing untrained model
2023-03-05 16:40:31,615:INFO:Huber Regressor Imported successfully
2023-03-05 16:40:31,624:INFO:Starting cross validation
2023-03-05 16:40:31,626:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:40:32,484:INFO:Calculating mean and std
2023-03-05 16:40:32,485:INFO:Creating metrics dataframe
2023-03-05 16:40:32,488:INFO:Uploading results into container
2023-03-05 16:40:32,489:INFO:Uploading model into container now
2023-03-05 16:40:32,489:INFO:_master_model_container: 10
2023-03-05 16:40:32,489:INFO:_display_container: 2
2023-03-05 16:40:32,490:INFO:HuberRegressor()
2023-03-05 16:40:32,490:INFO:create_model() successfully completed......................................
2023-03-05 16:40:32,594:INFO:SubProcess create_model() end ==================================
2023-03-05 16:40:32,595:INFO:Creating metrics dataframe
2023-03-05 16:40:32,609:INFO:Initializing K Neighbors Regressor
2023-03-05 16:40:32,609:INFO:Total runtime is 0.1306207776069641 minutes
2023-03-05 16:40:32,613:INFO:SubProcess create_model() called ==================================
2023-03-05 16:40:32,613:INFO:Initializing create_model()
2023-03-05 16:40:32,613:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7E535FD0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7EC4D490>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:40:32,613:INFO:Checking exceptions
2023-03-05 16:40:32,613:INFO:Importing libraries
2023-03-05 16:40:32,613:INFO:Copying training dataset
2023-03-05 16:40:32,619:INFO:Defining folds
2023-03-05 16:40:32,619:INFO:Declaring metric variables
2023-03-05 16:40:32,625:INFO:Importing untrained model
2023-03-05 16:40:32,631:INFO:K Neighbors Regressor Imported successfully
2023-03-05 16:40:32,643:INFO:Starting cross validation
2023-03-05 16:40:32,646:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:40:34,307:INFO:Calculating mean and std
2023-03-05 16:40:34,310:INFO:Creating metrics dataframe
2023-03-05 16:40:34,315:INFO:Uploading results into container
2023-03-05 16:40:34,316:INFO:Uploading model into container now
2023-03-05 16:40:34,318:INFO:_master_model_container: 11
2023-03-05 16:40:34,318:INFO:_display_container: 2
2023-03-05 16:40:34,318:INFO:KNeighborsRegressor(n_jobs=-1)
2023-03-05 16:40:34,319:INFO:create_model() successfully completed......................................
2023-03-05 16:40:34,439:INFO:SubProcess create_model() end ==================================
2023-03-05 16:40:34,439:INFO:Creating metrics dataframe
2023-03-05 16:40:34,459:INFO:Initializing Decision Tree Regressor
2023-03-05 16:40:34,459:INFO:Total runtime is 0.161447803179423 minutes
2023-03-05 16:40:34,465:INFO:SubProcess create_model() called ==================================
2023-03-05 16:40:34,465:INFO:Initializing create_model()
2023-03-05 16:40:34,466:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7E535FD0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7EC4D490>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:40:34,466:INFO:Checking exceptions
2023-03-05 16:40:34,467:INFO:Importing libraries
2023-03-05 16:40:34,468:INFO:Copying training dataset
2023-03-05 16:40:34,477:INFO:Defining folds
2023-03-05 16:40:34,478:INFO:Declaring metric variables
2023-03-05 16:40:34,489:INFO:Importing untrained model
2023-03-05 16:40:34,497:INFO:Decision Tree Regressor Imported successfully
2023-03-05 16:40:34,516:INFO:Starting cross validation
2023-03-05 16:40:34,522:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:40:35,491:INFO:Calculating mean and std
2023-03-05 16:40:35,492:INFO:Creating metrics dataframe
2023-03-05 16:40:35,496:INFO:Uploading results into container
2023-03-05 16:40:35,497:INFO:Uploading model into container now
2023-03-05 16:40:35,497:INFO:_master_model_container: 12
2023-03-05 16:40:35,497:INFO:_display_container: 2
2023-03-05 16:40:35,498:INFO:DecisionTreeRegressor(random_state=4126)
2023-03-05 16:40:35,498:INFO:create_model() successfully completed......................................
2023-03-05 16:40:35,600:INFO:SubProcess create_model() end ==================================
2023-03-05 16:40:35,601:INFO:Creating metrics dataframe
2023-03-05 16:40:35,614:INFO:Initializing Random Forest Regressor
2023-03-05 16:40:35,615:INFO:Total runtime is 0.18071107467015585 minutes
2023-03-05 16:40:35,619:INFO:SubProcess create_model() called ==================================
2023-03-05 16:40:35,619:INFO:Initializing create_model()
2023-03-05 16:40:35,619:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7E535FD0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7EC4D490>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:40:35,620:INFO:Checking exceptions
2023-03-05 16:40:35,620:INFO:Importing libraries
2023-03-05 16:40:35,620:INFO:Copying training dataset
2023-03-05 16:40:35,627:INFO:Defining folds
2023-03-05 16:40:35,628:INFO:Declaring metric variables
2023-03-05 16:40:35,633:INFO:Importing untrained model
2023-03-05 16:40:35,641:INFO:Random Forest Regressor Imported successfully
2023-03-05 16:40:35,654:INFO:Starting cross validation
2023-03-05 16:40:35,658:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:40:37,508:INFO:Calculating mean and std
2023-03-05 16:40:37,510:INFO:Creating metrics dataframe
2023-03-05 16:40:37,514:INFO:Uploading results into container
2023-03-05 16:40:37,515:INFO:Uploading model into container now
2023-03-05 16:40:37,515:INFO:_master_model_container: 13
2023-03-05 16:40:37,515:INFO:_display_container: 2
2023-03-05 16:40:37,516:INFO:RandomForestRegressor(n_jobs=-1, random_state=4126)
2023-03-05 16:40:37,517:INFO:create_model() successfully completed......................................
2023-03-05 16:40:37,605:INFO:SubProcess create_model() end ==================================
2023-03-05 16:40:37,605:INFO:Creating metrics dataframe
2023-03-05 16:40:37,619:INFO:Initializing Extra Trees Regressor
2023-03-05 16:40:37,620:INFO:Total runtime is 0.214139727751414 minutes
2023-03-05 16:40:37,624:INFO:SubProcess create_model() called ==================================
2023-03-05 16:40:37,625:INFO:Initializing create_model()
2023-03-05 16:40:37,625:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7E535FD0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7EC4D490>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:40:37,625:INFO:Checking exceptions
2023-03-05 16:40:37,625:INFO:Importing libraries
2023-03-05 16:40:37,625:INFO:Copying training dataset
2023-03-05 16:40:37,631:INFO:Defining folds
2023-03-05 16:40:37,631:INFO:Declaring metric variables
2023-03-05 16:40:37,637:INFO:Importing untrained model
2023-03-05 16:40:37,642:INFO:Extra Trees Regressor Imported successfully
2023-03-05 16:40:37,652:INFO:Starting cross validation
2023-03-05 16:40:37,654:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:40:39,807:INFO:Calculating mean and std
2023-03-05 16:40:39,809:INFO:Creating metrics dataframe
2023-03-05 16:40:39,813:INFO:Uploading results into container
2023-03-05 16:40:39,814:INFO:Uploading model into container now
2023-03-05 16:40:39,815:INFO:_master_model_container: 14
2023-03-05 16:40:39,815:INFO:_display_container: 2
2023-03-05 16:40:39,816:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=4126)
2023-03-05 16:40:39,816:INFO:create_model() successfully completed......................................
2023-03-05 16:40:39,904:INFO:SubProcess create_model() end ==================================
2023-03-05 16:40:39,904:INFO:Creating metrics dataframe
2023-03-05 16:40:39,918:INFO:Initializing AdaBoost Regressor
2023-03-05 16:40:39,918:INFO:Total runtime is 0.252428944905599 minutes
2023-03-05 16:40:39,922:INFO:SubProcess create_model() called ==================================
2023-03-05 16:40:39,922:INFO:Initializing create_model()
2023-03-05 16:40:39,922:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7E535FD0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7EC4D490>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:40:39,922:INFO:Checking exceptions
2023-03-05 16:40:39,922:INFO:Importing libraries
2023-03-05 16:40:39,922:INFO:Copying training dataset
2023-03-05 16:40:39,928:INFO:Defining folds
2023-03-05 16:40:39,928:INFO:Declaring metric variables
2023-03-05 16:40:39,934:INFO:Importing untrained model
2023-03-05 16:40:39,938:INFO:AdaBoost Regressor Imported successfully
2023-03-05 16:40:39,947:INFO:Starting cross validation
2023-03-05 16:40:39,949:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:40:41,012:INFO:Calculating mean and std
2023-03-05 16:40:41,014:INFO:Creating metrics dataframe
2023-03-05 16:40:41,017:INFO:Uploading results into container
2023-03-05 16:40:41,018:INFO:Uploading model into container now
2023-03-05 16:40:41,018:INFO:_master_model_container: 15
2023-03-05 16:40:41,018:INFO:_display_container: 2
2023-03-05 16:40:41,019:INFO:AdaBoostRegressor(random_state=4126)
2023-03-05 16:40:41,019:INFO:create_model() successfully completed......................................
2023-03-05 16:40:41,105:INFO:SubProcess create_model() end ==================================
2023-03-05 16:40:41,105:INFO:Creating metrics dataframe
2023-03-05 16:40:41,120:INFO:Initializing Gradient Boosting Regressor
2023-03-05 16:40:41,120:INFO:Total runtime is 0.2724681576093038 minutes
2023-03-05 16:40:41,125:INFO:SubProcess create_model() called ==================================
2023-03-05 16:40:41,125:INFO:Initializing create_model()
2023-03-05 16:40:41,125:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7E535FD0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7EC4D490>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:40:41,126:INFO:Checking exceptions
2023-03-05 16:40:41,126:INFO:Importing libraries
2023-03-05 16:40:41,126:INFO:Copying training dataset
2023-03-05 16:40:41,135:INFO:Defining folds
2023-03-05 16:40:41,136:INFO:Declaring metric variables
2023-03-05 16:40:41,142:INFO:Importing untrained model
2023-03-05 16:40:41,146:INFO:Gradient Boosting Regressor Imported successfully
2023-03-05 16:40:41,155:INFO:Starting cross validation
2023-03-05 16:40:41,157:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:40:42,388:INFO:Calculating mean and std
2023-03-05 16:40:42,390:INFO:Creating metrics dataframe
2023-03-05 16:40:42,395:INFO:Uploading results into container
2023-03-05 16:40:42,396:INFO:Uploading model into container now
2023-03-05 16:40:42,396:INFO:_master_model_container: 16
2023-03-05 16:40:42,396:INFO:_display_container: 2
2023-03-05 16:40:42,397:INFO:GradientBoostingRegressor(random_state=4126)
2023-03-05 16:40:42,397:INFO:create_model() successfully completed......................................
2023-03-05 16:40:42,483:INFO:SubProcess create_model() end ==================================
2023-03-05 16:40:42,483:INFO:Creating metrics dataframe
2023-03-05 16:40:42,500:INFO:Initializing Extreme Gradient Boosting
2023-03-05 16:40:42,501:INFO:Total runtime is 0.29548548062642416 minutes
2023-03-05 16:40:42,505:INFO:SubProcess create_model() called ==================================
2023-03-05 16:40:42,505:INFO:Initializing create_model()
2023-03-05 16:40:42,505:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7E535FD0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7EC4D490>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:40:42,506:INFO:Checking exceptions
2023-03-05 16:40:42,506:INFO:Importing libraries
2023-03-05 16:40:42,506:INFO:Copying training dataset
2023-03-05 16:40:42,514:INFO:Defining folds
2023-03-05 16:40:42,515:INFO:Declaring metric variables
2023-03-05 16:40:42,519:INFO:Importing untrained model
2023-03-05 16:40:42,524:INFO:Extreme Gradient Boosting Imported successfully
2023-03-05 16:40:42,534:INFO:Starting cross validation
2023-03-05 16:40:42,536:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:40:43,770:INFO:Calculating mean and std
2023-03-05 16:40:43,773:INFO:Creating metrics dataframe
2023-03-05 16:40:43,783:INFO:Uploading results into container
2023-03-05 16:40:43,784:INFO:Uploading model into container now
2023-03-05 16:40:43,785:INFO:_master_model_container: 17
2023-03-05 16:40:43,786:INFO:_display_container: 2
2023-03-05 16:40:43,786:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=4126, ...)
2023-03-05 16:40:43,787:INFO:create_model() successfully completed......................................
2023-03-05 16:40:43,896:INFO:SubProcess create_model() end ==================================
2023-03-05 16:40:43,896:INFO:Creating metrics dataframe
2023-03-05 16:40:43,912:INFO:Initializing Light Gradient Boosting Machine
2023-03-05 16:40:43,912:INFO:Total runtime is 0.31900777022043864 minutes
2023-03-05 16:40:43,916:INFO:SubProcess create_model() called ==================================
2023-03-05 16:40:43,917:INFO:Initializing create_model()
2023-03-05 16:40:43,917:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7E535FD0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7EC4D490>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:40:43,917:INFO:Checking exceptions
2023-03-05 16:40:43,917:INFO:Importing libraries
2023-03-05 16:40:43,917:INFO:Copying training dataset
2023-03-05 16:40:43,923:INFO:Defining folds
2023-03-05 16:40:43,924:INFO:Declaring metric variables
2023-03-05 16:40:43,928:INFO:Importing untrained model
2023-03-05 16:40:43,935:INFO:Light Gradient Boosting Machine Imported successfully
2023-03-05 16:40:43,945:INFO:Starting cross validation
2023-03-05 16:40:43,948:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:40:44,872:INFO:Calculating mean and std
2023-03-05 16:40:44,875:INFO:Creating metrics dataframe
2023-03-05 16:40:44,878:INFO:Uploading results into container
2023-03-05 16:40:44,879:INFO:Uploading model into container now
2023-03-05 16:40:44,879:INFO:_master_model_container: 18
2023-03-05 16:40:44,879:INFO:_display_container: 2
2023-03-05 16:40:44,880:INFO:LGBMRegressor(random_state=4126)
2023-03-05 16:40:44,880:INFO:create_model() successfully completed......................................
2023-03-05 16:40:44,967:INFO:SubProcess create_model() end ==================================
2023-03-05 16:40:44,967:INFO:Creating metrics dataframe
2023-03-05 16:40:44,982:INFO:Initializing Dummy Regressor
2023-03-05 16:40:44,982:INFO:Total runtime is 0.33683551947275797 minutes
2023-03-05 16:40:44,987:INFO:SubProcess create_model() called ==================================
2023-03-05 16:40:44,987:INFO:Initializing create_model()
2023-03-05 16:40:44,987:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7E535FD0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013B7EC4D490>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:40:44,988:INFO:Checking exceptions
2023-03-05 16:40:44,988:INFO:Importing libraries
2023-03-05 16:40:44,988:INFO:Copying training dataset
2023-03-05 16:40:44,996:INFO:Defining folds
2023-03-05 16:40:44,997:INFO:Declaring metric variables
2023-03-05 16:40:45,002:INFO:Importing untrained model
2023-03-05 16:40:45,007:INFO:Dummy Regressor Imported successfully
2023-03-05 16:40:45,017:INFO:Starting cross validation
2023-03-05 16:40:45,019:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:40:45,755:INFO:Calculating mean and std
2023-03-05 16:40:45,757:INFO:Creating metrics dataframe
2023-03-05 16:40:45,761:INFO:Uploading results into container
2023-03-05 16:40:45,762:INFO:Uploading model into container now
2023-03-05 16:40:45,762:INFO:_master_model_container: 19
2023-03-05 16:40:45,762:INFO:_display_container: 2
2023-03-05 16:40:45,762:INFO:DummyRegressor()
2023-03-05 16:40:45,762:INFO:create_model() successfully completed......................................
2023-03-05 16:40:45,850:INFO:SubProcess create_model() end ==================================
2023-03-05 16:40:45,850:INFO:Creating metrics dataframe
2023-03-05 16:40:45,882:INFO:Initializing create_model()
2023-03-05 16:40:45,883:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7E535FD0>, estimator=AdaBoostRegressor(random_state=4126), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:40:45,883:INFO:Checking exceptions
2023-03-05 16:40:45,886:INFO:Importing libraries
2023-03-05 16:40:45,886:INFO:Copying training dataset
2023-03-05 16:40:45,890:INFO:Defining folds
2023-03-05 16:40:45,890:INFO:Declaring metric variables
2023-03-05 16:40:45,891:INFO:Importing untrained model
2023-03-05 16:40:45,891:INFO:Declaring custom model
2023-03-05 16:40:45,892:INFO:AdaBoost Regressor Imported successfully
2023-03-05 16:40:45,894:INFO:Cross validation set to False
2023-03-05 16:40:45,895:INFO:Fitting Model
2023-03-05 16:40:46,046:INFO:AdaBoostRegressor(random_state=4126)
2023-03-05 16:40:46,046:INFO:create_model() successfully completed......................................
2023-03-05 16:40:46,162:INFO:_master_model_container: 19
2023-03-05 16:40:46,162:INFO:_display_container: 2
2023-03-05 16:40:46,163:INFO:AdaBoostRegressor(random_state=4126)
2023-03-05 16:40:46,163:INFO:compare_models() successfully completed......................................
2023-03-05 16:42:27,887:INFO:Initializing evaluate_model()
2023-03-05 16:42:27,887:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7E535FD0>, estimator=AdaBoostRegressor(random_state=4126), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-03-05 16:42:27,922:INFO:Initializing plot_model()
2023-03-05 16:42:27,922:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=AdaBoostRegressor(random_state=4126), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7E535FD0>, system=True)
2023-03-05 16:42:27,923:INFO:Checking exceptions
2023-03-05 16:42:27,926:INFO:Preloading libraries
2023-03-05 16:42:27,931:INFO:Copying training dataset
2023-03-05 16:42:27,932:INFO:Plot type: pipeline
2023-03-05 16:42:28,173:INFO:Visual Rendered Successfully
2023-03-05 16:42:28,249:INFO:plot_model() successfully completed......................................
2023-03-05 16:42:41,721:INFO:Initializing predict_model()
2023-03-05 16:42:41,722:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000013B7E535FD0>, estimator=AdaBoostRegressor(random_state=4126), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x0000013B7D9ED790>)
2023-03-05 16:42:41,722:INFO:Checking exceptions
2023-03-05 16:42:41,722:INFO:Preloading libraries
2023-03-05 16:42:41,724:INFO:Set up data.
2023-03-05 16:42:41,730:INFO:Set up index.
2023-03-05 16:44:39,216:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-05 16:44:39,216:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-05 16:44:39,216:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-05 16:44:39,216:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-05 16:44:39,816:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-03-05 16:44:40,006:INFO:PyCaret RegressionExperiment
2023-03-05 16:44:40,006:INFO:Logging name: reg-default-name
2023-03-05 16:44:40,006:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-05 16:44:40,006:INFO:version 3.0.0.rc9
2023-03-05 16:44:40,007:INFO:Initializing setup()
2023-03-05 16:44:40,007:INFO:self.USI: e49b
2023-03-05 16:44:40,007:INFO:self._variable_keys: {'fold_shuffle_param', '_available_plots', '_ml_usecase', 'exp_id', 'exp_name_log', 'X_train', 'html_param', 'fold_generator', 'n_jobs_param', 'logging_param', 'X_test', 'gpu_param', 'seed', 'y_train', 'data', 'pipeline', 'memory', 'log_plots_param', 'X', 'fold_groups_param', 'transform_target_param', 'USI', 'idx', 'y_test', 'target_param', 'y', 'gpu_n_jobs_param'}
2023-03-05 16:44:40,007:INFO:Checking environment
2023-03-05 16:44:40,007:INFO:python_version: 3.9.13
2023-03-05 16:44:40,007:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-03-05 16:44:40,007:INFO:machine: AMD64
2023-03-05 16:44:40,007:INFO:platform: Windows-10-10.0.19044-SP0
2023-03-05 16:44:40,007:INFO:Memory: svmem(total=17009516544, available=4383744000, percent=74.2, used=12625772544, free=4383744000)
2023-03-05 16:44:40,007:INFO:Physical Core: 4
2023-03-05 16:44:40,007:INFO:Logical Core: 8
2023-03-05 16:44:40,007:INFO:Checking libraries
2023-03-05 16:44:40,007:INFO:System:
2023-03-05 16:44:40,007:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-03-05 16:44:40,007:INFO:executable: C:\Users\Alvaro\anaconda3\python.exe
2023-03-05 16:44:40,007:INFO:   machine: Windows-10-10.0.19044-SP0
2023-03-05 16:44:40,007:INFO:PyCaret required dependencies:
2023-03-05 16:44:40,007:INFO:                 pip: 22.2.2
2023-03-05 16:44:40,007:INFO:          setuptools: 63.4.1
2023-03-05 16:44:40,007:INFO:             pycaret: 3.0.0rc9
2023-03-05 16:44:40,008:INFO:             IPython: 7.31.1
2023-03-05 16:44:40,008:INFO:          ipywidgets: 7.6.5
2023-03-05 16:44:40,008:INFO:                tqdm: 4.64.1
2023-03-05 16:44:40,008:INFO:               numpy: 1.21.5
2023-03-05 16:44:40,008:INFO:              pandas: 1.4.4
2023-03-05 16:44:40,008:INFO:              jinja2: 2.11.3
2023-03-05 16:44:40,008:INFO:               scipy: 1.9.1
2023-03-05 16:44:40,008:INFO:              joblib: 1.2.0
2023-03-05 16:44:40,008:INFO:             sklearn: 1.0.2
2023-03-05 16:44:40,008:INFO:                pyod: 1.0.7
2023-03-05 16:44:40,008:INFO:            imblearn: 0.10.1
2023-03-05 16:44:40,008:INFO:   category_encoders: 2.6.0
2023-03-05 16:44:40,008:INFO:            lightgbm: 3.3.5
2023-03-05 16:44:40,008:INFO:               numba: 0.55.1
2023-03-05 16:44:40,008:INFO:            requests: 2.28.1
2023-03-05 16:44:40,008:INFO:          matplotlib: 3.5.2
2023-03-05 16:44:40,008:INFO:          scikitplot: 0.3.7
2023-03-05 16:44:40,008:INFO:         yellowbrick: 1.5
2023-03-05 16:44:40,008:INFO:              plotly: 5.9.0
2023-03-05 16:44:40,008:INFO:             kaleido: 0.2.1
2023-03-05 16:44:40,008:INFO:         statsmodels: 0.13.2
2023-03-05 16:44:40,008:INFO:              sktime: 0.16.1
2023-03-05 16:44:40,008:INFO:               tbats: 1.1.2
2023-03-05 16:44:40,008:INFO:            pmdarima: 2.0.2
2023-03-05 16:44:40,008:INFO:              psutil: 5.9.0
2023-03-05 16:44:40,008:INFO:PyCaret optional dependencies:
2023-03-05 16:44:40,027:INFO:                shap: Not installed
2023-03-05 16:44:40,027:INFO:           interpret: Not installed
2023-03-05 16:44:40,027:INFO:                umap: Not installed
2023-03-05 16:44:40,027:INFO:    pandas_profiling: Not installed
2023-03-05 16:44:40,027:INFO:  explainerdashboard: Not installed
2023-03-05 16:44:40,027:INFO:             autoviz: Not installed
2023-03-05 16:44:40,027:INFO:           fairlearn: Not installed
2023-03-05 16:44:40,027:INFO:             xgboost: 1.7.4
2023-03-05 16:44:40,027:INFO:            catboost: Not installed
2023-03-05 16:44:40,027:INFO:              kmodes: Not installed
2023-03-05 16:44:40,027:INFO:             mlxtend: Not installed
2023-03-05 16:44:40,027:INFO:       statsforecast: Not installed
2023-03-05 16:44:40,027:INFO:        tune_sklearn: Not installed
2023-03-05 16:44:40,028:INFO:                 ray: Not installed
2023-03-05 16:44:40,028:INFO:            hyperopt: Not installed
2023-03-05 16:44:40,028:INFO:              optuna: Not installed
2023-03-05 16:44:40,028:INFO:               skopt: Not installed
2023-03-05 16:44:40,028:INFO:              mlflow: Not installed
2023-03-05 16:44:40,028:INFO:              gradio: Not installed
2023-03-05 16:44:40,028:INFO:             fastapi: Not installed
2023-03-05 16:44:40,028:INFO:             uvicorn: Not installed
2023-03-05 16:44:40,028:INFO:              m2cgen: Not installed
2023-03-05 16:44:40,028:INFO:           evidently: Not installed
2023-03-05 16:44:40,028:INFO:               fugue: Not installed
2023-03-05 16:44:40,028:INFO:           streamlit: Not installed
2023-03-05 16:44:40,028:INFO:             prophet: Not installed
2023-03-05 16:44:40,028:INFO:None
2023-03-05 16:44:40,028:INFO:Set up data.
2023-03-05 16:44:40,034:INFO:Set up train/test split.
2023-03-05 16:44:40,040:INFO:Set up index.
2023-03-05 16:44:40,040:INFO:Set up folding strategy.
2023-03-05 16:44:40,040:INFO:Assigning column types.
2023-03-05 16:44:40,043:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-05 16:44:40,043:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-05 16:44:40,047:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 16:44:40,051:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:44:40,105:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:44:40,145:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:44:40,217:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:44:40,252:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:44:40,252:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-05 16:44:40,256:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 16:44:40,261:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:44:40,314:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:44:40,358:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:44:40,359:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:44:40,361:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:44:40,362:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-03-05 16:44:40,367:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 16:44:40,371:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:44:40,423:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:44:40,463:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:44:40,464:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:44:40,467:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:44:40,471:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 16:44:40,475:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:44:40,527:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:44:40,568:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:44:40,568:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:44:40,572:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:44:40,572:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-03-05 16:44:40,581:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:44:40,633:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:44:40,673:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:44:40,674:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:44:40,676:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:44:40,685:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:44:40,736:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:44:40,780:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:44:40,781:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:44:40,783:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:44:40,783:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-03-05 16:44:40,844:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:44:40,886:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:44:40,886:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:44:40,888:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:44:40,949:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:44:40,988:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:44:40,989:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:44:40,991:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:44:40,991:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-05 16:44:41,051:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:44:41,091:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:44:41,093:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:44:41,153:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:44:41,193:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:44:41,196:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:44:41,196:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-03-05 16:44:41,300:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:44:41,302:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:44:41,405:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:44:41,407:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:44:41,409:INFO:Preparing preprocessing pipeline...
2023-03-05 16:44:41,409:INFO:Set up simple imputation.
2023-03-05 16:44:41,412:INFO:Set up encoding of categorical features.
2023-03-05 16:44:41,514:INFO:Finished creating preprocessing pipeline.
2023-03-05 16:44:41,524:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Alvaro\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['work_year', 'remote_ratio'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['experience_level',
                                             'employment_type', 'job_title',
                                             'employee_residence',
                                             'company_location',
                                             'company_size'],
                                    t...
                                    transformer=OneHotEncoder(cols=['experience_level',
                                                                    'employment_type',
                                                                    'company_size'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['job_title', 'employee_residence',
                                             'company_location'],
                                    transformer=LeaveOneOutEncoder(cols=['job_title',
                                                                         'employee_residence',
                                                                         'company_location'],
                                                                   handle_missing='return_nan',
                                                                   random_state=7671)))])
2023-03-05 16:44:41,524:INFO:Creating final display dataframe.
2023-03-05 16:44:41,978:INFO:Setup _display_container:                     Description             Value
0                    Session id              7671
1                        Target     salary_in_usd
2                   Target type        Regression
3           Original data shape          (500, 9)
4        Transformed data shape         (500, 17)
5   Transformed train set shape         (350, 17)
6    Transformed test set shape         (150, 17)
7              Numeric features                 2
8          Categorical features                 6
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator             KFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  reg-default-name
21                          USI              e49b
2023-03-05 16:44:42,100:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:44:42,103:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:44:42,204:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:44:42,206:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:44:42,207:INFO:setup() successfully completed in 2.21s...............
2023-03-05 16:44:42,227:INFO:Initializing compare_models()
2023-03-05 16:44:42,228:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A4A69BB80>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000023A4A69BB80>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-03-05 16:44:42,228:INFO:Checking exceptions
2023-03-05 16:44:42,231:INFO:Preparing display monitor
2023-03-05 16:44:42,288:INFO:Initializing Linear Regression
2023-03-05 16:44:42,288:INFO:Total runtime is 1.085201899210612e-05 minutes
2023-03-05 16:44:42,293:INFO:SubProcess create_model() called ==================================
2023-03-05 16:44:42,294:INFO:Initializing create_model()
2023-03-05 16:44:42,294:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A4A69BB80>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A4A671430>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:44:42,294:INFO:Checking exceptions
2023-03-05 16:44:42,294:INFO:Importing libraries
2023-03-05 16:44:42,294:INFO:Copying training dataset
2023-03-05 16:44:42,302:INFO:Defining folds
2023-03-05 16:44:42,302:INFO:Declaring metric variables
2023-03-05 16:44:42,305:INFO:Importing untrained model
2023-03-05 16:44:42,310:INFO:Linear Regression Imported successfully
2023-03-05 16:44:42,320:INFO:Starting cross validation
2023-03-05 16:44:42,327:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:44:51,720:INFO:Calculating mean and std
2023-03-05 16:44:51,721:INFO:Creating metrics dataframe
2023-03-05 16:44:51,724:INFO:Uploading results into container
2023-03-05 16:44:51,725:INFO:Uploading model into container now
2023-03-05 16:44:51,725:INFO:_master_model_container: 1
2023-03-05 16:44:51,726:INFO:_display_container: 2
2023-03-05 16:44:51,726:INFO:LinearRegression(n_jobs=-1)
2023-03-05 16:44:51,726:INFO:create_model() successfully completed......................................
2023-03-05 16:44:51,809:INFO:SubProcess create_model() end ==================================
2023-03-05 16:44:51,809:INFO:Creating metrics dataframe
2023-03-05 16:44:51,817:INFO:Initializing Lasso Regression
2023-03-05 16:44:51,817:INFO:Total runtime is 0.15881518125534058 minutes
2023-03-05 16:44:51,821:INFO:SubProcess create_model() called ==================================
2023-03-05 16:44:51,821:INFO:Initializing create_model()
2023-03-05 16:44:51,821:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A4A69BB80>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A4A671430>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:44:51,822:INFO:Checking exceptions
2023-03-05 16:44:51,822:INFO:Importing libraries
2023-03-05 16:44:51,822:INFO:Copying training dataset
2023-03-05 16:44:51,827:INFO:Defining folds
2023-03-05 16:44:51,827:INFO:Declaring metric variables
2023-03-05 16:44:51,831:INFO:Importing untrained model
2023-03-05 16:44:51,835:INFO:Lasso Regression Imported successfully
2023-03-05 16:44:51,842:INFO:Starting cross validation
2023-03-05 16:44:51,845:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:44:52,151:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.025e+09, tolerance: 1.313e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-05 16:44:52,661:INFO:Calculating mean and std
2023-03-05 16:44:52,662:INFO:Creating metrics dataframe
2023-03-05 16:44:52,665:INFO:Uploading results into container
2023-03-05 16:44:52,666:INFO:Uploading model into container now
2023-03-05 16:44:52,666:INFO:_master_model_container: 2
2023-03-05 16:44:52,666:INFO:_display_container: 2
2023-03-05 16:44:52,666:INFO:Lasso(random_state=7671)
2023-03-05 16:44:52,666:INFO:create_model() successfully completed......................................
2023-03-05 16:44:52,742:INFO:SubProcess create_model() end ==================================
2023-03-05 16:44:52,742:INFO:Creating metrics dataframe
2023-03-05 16:44:52,751:INFO:Initializing Ridge Regression
2023-03-05 16:44:52,751:INFO:Total runtime is 0.174395751953125 minutes
2023-03-05 16:44:52,755:INFO:SubProcess create_model() called ==================================
2023-03-05 16:44:52,756:INFO:Initializing create_model()
2023-03-05 16:44:52,756:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A4A69BB80>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A4A671430>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:44:52,756:INFO:Checking exceptions
2023-03-05 16:44:52,756:INFO:Importing libraries
2023-03-05 16:44:52,756:INFO:Copying training dataset
2023-03-05 16:44:52,760:INFO:Defining folds
2023-03-05 16:44:52,761:INFO:Declaring metric variables
2023-03-05 16:44:52,764:INFO:Importing untrained model
2023-03-05 16:44:52,767:INFO:Ridge Regression Imported successfully
2023-03-05 16:44:52,777:INFO:Starting cross validation
2023-03-05 16:44:52,779:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:44:53,665:INFO:Calculating mean and std
2023-03-05 16:44:53,665:INFO:Creating metrics dataframe
2023-03-05 16:44:53,669:INFO:Uploading results into container
2023-03-05 16:44:53,669:INFO:Uploading model into container now
2023-03-05 16:44:53,670:INFO:_master_model_container: 3
2023-03-05 16:44:53,670:INFO:_display_container: 2
2023-03-05 16:44:53,670:INFO:Ridge(random_state=7671)
2023-03-05 16:44:53,670:INFO:create_model() successfully completed......................................
2023-03-05 16:44:53,749:INFO:SubProcess create_model() end ==================================
2023-03-05 16:44:53,749:INFO:Creating metrics dataframe
2023-03-05 16:44:53,759:INFO:Initializing Elastic Net
2023-03-05 16:44:53,759:INFO:Total runtime is 0.19118071397145592 minutes
2023-03-05 16:44:53,763:INFO:SubProcess create_model() called ==================================
2023-03-05 16:44:53,763:INFO:Initializing create_model()
2023-03-05 16:44:53,763:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A4A69BB80>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A4A671430>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:44:53,763:INFO:Checking exceptions
2023-03-05 16:44:53,764:INFO:Importing libraries
2023-03-05 16:44:53,764:INFO:Copying training dataset
2023-03-05 16:44:53,769:INFO:Defining folds
2023-03-05 16:44:53,769:INFO:Declaring metric variables
2023-03-05 16:44:53,772:INFO:Importing untrained model
2023-03-05 16:44:53,776:INFO:Elastic Net Imported successfully
2023-03-05 16:44:53,785:INFO:Starting cross validation
2023-03-05 16:44:53,787:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:44:54,707:INFO:Calculating mean and std
2023-03-05 16:44:54,707:INFO:Creating metrics dataframe
2023-03-05 16:44:54,713:INFO:Uploading results into container
2023-03-05 16:44:54,713:INFO:Uploading model into container now
2023-03-05 16:44:54,714:INFO:_master_model_container: 4
2023-03-05 16:44:54,714:INFO:_display_container: 2
2023-03-05 16:44:54,714:INFO:ElasticNet(random_state=7671)
2023-03-05 16:44:54,714:INFO:create_model() successfully completed......................................
2023-03-05 16:44:54,804:INFO:SubProcess create_model() end ==================================
2023-03-05 16:44:54,804:INFO:Creating metrics dataframe
2023-03-05 16:44:54,817:INFO:Initializing Least Angle Regression
2023-03-05 16:44:54,817:INFO:Total runtime is 0.20882306893666588 minutes
2023-03-05 16:44:54,824:INFO:SubProcess create_model() called ==================================
2023-03-05 16:44:54,825:INFO:Initializing create_model()
2023-03-05 16:44:54,825:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A4A69BB80>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A4A671430>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:44:54,825:INFO:Checking exceptions
2023-03-05 16:44:54,825:INFO:Importing libraries
2023-03-05 16:44:54,826:INFO:Copying training dataset
2023-03-05 16:44:54,835:INFO:Defining folds
2023-03-05 16:44:54,836:INFO:Declaring metric variables
2023-03-05 16:44:54,840:INFO:Importing untrained model
2023-03-05 16:44:54,845:INFO:Least Angle Regression Imported successfully
2023-03-05 16:44:54,855:INFO:Starting cross validation
2023-03-05 16:44:54,856:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:44:55,133:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:44:55,225:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:44:55,262:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:44:55,292:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:44:55,322:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:44:55,360:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:44:55,372:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:44:55,388:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:44:55,902:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:44:55,916:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:44:56,025:INFO:Calculating mean and std
2023-03-05 16:44:56,027:INFO:Creating metrics dataframe
2023-03-05 16:44:56,031:INFO:Uploading results into container
2023-03-05 16:44:56,032:INFO:Uploading model into container now
2023-03-05 16:44:56,032:INFO:_master_model_container: 5
2023-03-05 16:44:56,032:INFO:_display_container: 2
2023-03-05 16:44:56,033:INFO:Lars(random_state=7671)
2023-03-05 16:44:56,033:INFO:create_model() successfully completed......................................
2023-03-05 16:44:56,129:INFO:SubProcess create_model() end ==================================
2023-03-05 16:44:56,129:INFO:Creating metrics dataframe
2023-03-05 16:44:56,150:INFO:Initializing Lasso Least Angle Regression
2023-03-05 16:44:56,150:INFO:Total runtime is 0.23103757699330651 minutes
2023-03-05 16:44:56,154:INFO:SubProcess create_model() called ==================================
2023-03-05 16:44:56,155:INFO:Initializing create_model()
2023-03-05 16:44:56,155:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A4A69BB80>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A4A671430>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:44:56,155:INFO:Checking exceptions
2023-03-05 16:44:56,156:INFO:Importing libraries
2023-03-05 16:44:56,156:INFO:Copying training dataset
2023-03-05 16:44:56,163:INFO:Defining folds
2023-03-05 16:44:56,163:INFO:Declaring metric variables
2023-03-05 16:44:56,167:INFO:Importing untrained model
2023-03-05 16:44:56,173:INFO:Lasso Least Angle Regression Imported successfully
2023-03-05 16:44:56,182:INFO:Starting cross validation
2023-03-05 16:44:56,185:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:44:56,465:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:44:56,541:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:44:56,555:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:44:56,572:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:44:56,596:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:44:56,600:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:44:56,625:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:44:56,634:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:44:57,184:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:44:57,203:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 16:44:57,309:INFO:Calculating mean and std
2023-03-05 16:44:57,311:INFO:Creating metrics dataframe
2023-03-05 16:44:57,315:INFO:Uploading results into container
2023-03-05 16:44:57,316:INFO:Uploading model into container now
2023-03-05 16:44:57,316:INFO:_master_model_container: 6
2023-03-05 16:44:57,316:INFO:_display_container: 2
2023-03-05 16:44:57,317:INFO:LassoLars(random_state=7671)
2023-03-05 16:44:57,317:INFO:create_model() successfully completed......................................
2023-03-05 16:44:57,408:INFO:SubProcess create_model() end ==================================
2023-03-05 16:44:57,408:INFO:Creating metrics dataframe
2023-03-05 16:44:57,422:INFO:Initializing Orthogonal Matching Pursuit
2023-03-05 16:44:57,422:INFO:Total runtime is 0.25223547617594405 minutes
2023-03-05 16:44:57,426:INFO:SubProcess create_model() called ==================================
2023-03-05 16:44:57,426:INFO:Initializing create_model()
2023-03-05 16:44:57,428:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A4A69BB80>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A4A671430>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:44:57,428:INFO:Checking exceptions
2023-03-05 16:44:57,428:INFO:Importing libraries
2023-03-05 16:44:57,428:INFO:Copying training dataset
2023-03-05 16:44:57,436:INFO:Defining folds
2023-03-05 16:44:57,436:INFO:Declaring metric variables
2023-03-05 16:44:57,441:INFO:Importing untrained model
2023-03-05 16:44:57,450:INFO:Orthogonal Matching Pursuit Imported successfully
2023-03-05 16:44:57,460:INFO:Starting cross validation
2023-03-05 16:44:57,462:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:44:57,725:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:44:57,807:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:44:57,833:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:44:57,843:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:44:57,858:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:44:57,885:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:44:57,899:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:44:58,372:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:44:58,432:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 16:44:58,524:INFO:Calculating mean and std
2023-03-05 16:44:58,526:INFO:Creating metrics dataframe
2023-03-05 16:44:58,529:INFO:Uploading results into container
2023-03-05 16:44:58,530:INFO:Uploading model into container now
2023-03-05 16:44:58,530:INFO:_master_model_container: 7
2023-03-05 16:44:58,530:INFO:_display_container: 2
2023-03-05 16:44:58,531:INFO:OrthogonalMatchingPursuit()
2023-03-05 16:44:58,531:INFO:create_model() successfully completed......................................
2023-03-05 16:44:58,615:INFO:SubProcess create_model() end ==================================
2023-03-05 16:44:58,615:INFO:Creating metrics dataframe
2023-03-05 16:44:58,626:INFO:Initializing Bayesian Ridge
2023-03-05 16:44:58,627:INFO:Total runtime is 0.2723215540250143 minutes
2023-03-05 16:44:58,632:INFO:SubProcess create_model() called ==================================
2023-03-05 16:44:58,632:INFO:Initializing create_model()
2023-03-05 16:44:58,632:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A4A69BB80>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A4A671430>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:44:58,632:INFO:Checking exceptions
2023-03-05 16:44:58,632:INFO:Importing libraries
2023-03-05 16:44:58,632:INFO:Copying training dataset
2023-03-05 16:44:58,639:INFO:Defining folds
2023-03-05 16:44:58,639:INFO:Declaring metric variables
2023-03-05 16:44:58,643:INFO:Importing untrained model
2023-03-05 16:44:58,648:INFO:Bayesian Ridge Imported successfully
2023-03-05 16:44:58,657:INFO:Starting cross validation
2023-03-05 16:44:58,658:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:44:59,808:INFO:Calculating mean and std
2023-03-05 16:44:59,809:INFO:Creating metrics dataframe
2023-03-05 16:44:59,815:INFO:Uploading results into container
2023-03-05 16:44:59,816:INFO:Uploading model into container now
2023-03-05 16:44:59,818:INFO:_master_model_container: 8
2023-03-05 16:44:59,818:INFO:_display_container: 2
2023-03-05 16:44:59,819:INFO:BayesianRidge()
2023-03-05 16:44:59,819:INFO:create_model() successfully completed......................................
2023-03-05 16:44:59,905:INFO:SubProcess create_model() end ==================================
2023-03-05 16:44:59,905:INFO:Creating metrics dataframe
2023-03-05 16:44:59,918:INFO:Initializing Passive Aggressive Regressor
2023-03-05 16:44:59,918:INFO:Total runtime is 0.2938374559084575 minutes
2023-03-05 16:44:59,922:INFO:SubProcess create_model() called ==================================
2023-03-05 16:44:59,922:INFO:Initializing create_model()
2023-03-05 16:44:59,922:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A4A69BB80>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A4A671430>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:44:59,923:INFO:Checking exceptions
2023-03-05 16:44:59,923:INFO:Importing libraries
2023-03-05 16:44:59,923:INFO:Copying training dataset
2023-03-05 16:44:59,931:INFO:Defining folds
2023-03-05 16:44:59,932:INFO:Declaring metric variables
2023-03-05 16:44:59,936:INFO:Importing untrained model
2023-03-05 16:44:59,941:INFO:Passive Aggressive Regressor Imported successfully
2023-03-05 16:44:59,949:INFO:Starting cross validation
2023-03-05 16:44:59,951:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:45:01,044:INFO:Calculating mean and std
2023-03-05 16:45:01,046:INFO:Creating metrics dataframe
2023-03-05 16:45:01,054:INFO:Uploading results into container
2023-03-05 16:45:01,056:INFO:Uploading model into container now
2023-03-05 16:45:01,056:INFO:_master_model_container: 9
2023-03-05 16:45:01,056:INFO:_display_container: 2
2023-03-05 16:45:01,057:INFO:PassiveAggressiveRegressor(random_state=7671)
2023-03-05 16:45:01,057:INFO:create_model() successfully completed......................................
2023-03-05 16:45:01,171:INFO:SubProcess create_model() end ==================================
2023-03-05 16:45:01,172:INFO:Creating metrics dataframe
2023-03-05 16:45:01,186:INFO:Initializing Huber Regressor
2023-03-05 16:45:01,186:INFO:Total runtime is 0.31497344175974534 minutes
2023-03-05 16:45:01,191:INFO:SubProcess create_model() called ==================================
2023-03-05 16:45:01,191:INFO:Initializing create_model()
2023-03-05 16:45:01,191:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A4A69BB80>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A4A671430>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:45:01,191:INFO:Checking exceptions
2023-03-05 16:45:01,191:INFO:Importing libraries
2023-03-05 16:45:01,192:INFO:Copying training dataset
2023-03-05 16:45:01,198:INFO:Defining folds
2023-03-05 16:45:01,198:INFO:Declaring metric variables
2023-03-05 16:45:01,204:INFO:Importing untrained model
2023-03-05 16:45:01,210:INFO:Huber Regressor Imported successfully
2023-03-05 16:45:01,219:INFO:Starting cross validation
2023-03-05 16:45:01,221:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:45:02,413:INFO:Calculating mean and std
2023-03-05 16:45:02,415:INFO:Creating metrics dataframe
2023-03-05 16:45:02,419:INFO:Uploading results into container
2023-03-05 16:45:02,420:INFO:Uploading model into container now
2023-03-05 16:45:02,420:INFO:_master_model_container: 10
2023-03-05 16:45:02,420:INFO:_display_container: 2
2023-03-05 16:45:02,421:INFO:HuberRegressor()
2023-03-05 16:45:02,421:INFO:create_model() successfully completed......................................
2023-03-05 16:45:02,505:INFO:SubProcess create_model() end ==================================
2023-03-05 16:45:02,505:INFO:Creating metrics dataframe
2023-03-05 16:45:02,518:INFO:Initializing K Neighbors Regressor
2023-03-05 16:45:02,518:INFO:Total runtime is 0.3371724088986715 minutes
2023-03-05 16:45:02,523:INFO:SubProcess create_model() called ==================================
2023-03-05 16:45:02,523:INFO:Initializing create_model()
2023-03-05 16:45:02,524:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A4A69BB80>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A4A671430>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:45:02,524:INFO:Checking exceptions
2023-03-05 16:45:02,524:INFO:Importing libraries
2023-03-05 16:45:02,524:INFO:Copying training dataset
2023-03-05 16:45:02,531:INFO:Defining folds
2023-03-05 16:45:02,531:INFO:Declaring metric variables
2023-03-05 16:45:02,536:INFO:Importing untrained model
2023-03-05 16:45:02,541:INFO:K Neighbors Regressor Imported successfully
2023-03-05 16:45:02,550:INFO:Starting cross validation
2023-03-05 16:45:02,552:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:45:03,676:INFO:Calculating mean and std
2023-03-05 16:45:03,678:INFO:Creating metrics dataframe
2023-03-05 16:45:03,682:INFO:Uploading results into container
2023-03-05 16:45:03,683:INFO:Uploading model into container now
2023-03-05 16:45:03,684:INFO:_master_model_container: 11
2023-03-05 16:45:03,684:INFO:_display_container: 2
2023-03-05 16:45:03,684:INFO:KNeighborsRegressor(n_jobs=-1)
2023-03-05 16:45:03,685:INFO:create_model() successfully completed......................................
2023-03-05 16:45:03,769:INFO:SubProcess create_model() end ==================================
2023-03-05 16:45:03,769:INFO:Creating metrics dataframe
2023-03-05 16:45:03,783:INFO:Initializing Decision Tree Regressor
2023-03-05 16:45:03,784:INFO:Total runtime is 0.3582662582397461 minutes
2023-03-05 16:45:03,788:INFO:SubProcess create_model() called ==================================
2023-03-05 16:45:03,789:INFO:Initializing create_model()
2023-03-05 16:45:03,789:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A4A69BB80>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A4A671430>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:45:03,789:INFO:Checking exceptions
2023-03-05 16:45:03,789:INFO:Importing libraries
2023-03-05 16:45:03,789:INFO:Copying training dataset
2023-03-05 16:45:03,795:INFO:Defining folds
2023-03-05 16:45:03,796:INFO:Declaring metric variables
2023-03-05 16:45:03,800:INFO:Importing untrained model
2023-03-05 16:45:03,806:INFO:Decision Tree Regressor Imported successfully
2023-03-05 16:45:03,814:INFO:Starting cross validation
2023-03-05 16:45:03,817:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:45:04,860:INFO:Calculating mean and std
2023-03-05 16:45:04,862:INFO:Creating metrics dataframe
2023-03-05 16:45:04,866:INFO:Uploading results into container
2023-03-05 16:45:04,866:INFO:Uploading model into container now
2023-03-05 16:45:04,868:INFO:_master_model_container: 12
2023-03-05 16:45:04,868:INFO:_display_container: 2
2023-03-05 16:45:04,868:INFO:DecisionTreeRegressor(random_state=7671)
2023-03-05 16:45:04,868:INFO:create_model() successfully completed......................................
2023-03-05 16:45:04,955:INFO:SubProcess create_model() end ==================================
2023-03-05 16:45:04,956:INFO:Creating metrics dataframe
2023-03-05 16:45:04,972:INFO:Initializing Random Forest Regressor
2023-03-05 16:45:04,973:INFO:Total runtime is 0.3780842502911886 minutes
2023-03-05 16:45:04,976:INFO:SubProcess create_model() called ==================================
2023-03-05 16:45:04,977:INFO:Initializing create_model()
2023-03-05 16:45:04,978:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A4A69BB80>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A4A671430>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:45:04,978:INFO:Checking exceptions
2023-03-05 16:45:04,978:INFO:Importing libraries
2023-03-05 16:45:04,978:INFO:Copying training dataset
2023-03-05 16:45:04,983:INFO:Defining folds
2023-03-05 16:45:04,983:INFO:Declaring metric variables
2023-03-05 16:45:04,992:INFO:Importing untrained model
2023-03-05 16:45:04,997:INFO:Random Forest Regressor Imported successfully
2023-03-05 16:45:05,010:INFO:Starting cross validation
2023-03-05 16:45:05,012:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:45:07,713:INFO:Calculating mean and std
2023-03-05 16:45:07,715:INFO:Creating metrics dataframe
2023-03-05 16:45:07,719:INFO:Uploading results into container
2023-03-05 16:45:07,720:INFO:Uploading model into container now
2023-03-05 16:45:07,720:INFO:_master_model_container: 13
2023-03-05 16:45:07,720:INFO:_display_container: 2
2023-03-05 16:45:07,720:INFO:RandomForestRegressor(n_jobs=-1, random_state=7671)
2023-03-05 16:45:07,721:INFO:create_model() successfully completed......................................
2023-03-05 16:45:07,808:INFO:SubProcess create_model() end ==================================
2023-03-05 16:45:07,808:INFO:Creating metrics dataframe
2023-03-05 16:45:07,822:INFO:Initializing Extra Trees Regressor
2023-03-05 16:45:07,822:INFO:Total runtime is 0.4255656798680624 minutes
2023-03-05 16:45:07,825:INFO:SubProcess create_model() called ==================================
2023-03-05 16:45:07,826:INFO:Initializing create_model()
2023-03-05 16:45:07,826:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A4A69BB80>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A4A671430>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:45:07,826:INFO:Checking exceptions
2023-03-05 16:45:07,826:INFO:Importing libraries
2023-03-05 16:45:07,826:INFO:Copying training dataset
2023-03-05 16:45:07,833:INFO:Defining folds
2023-03-05 16:45:07,833:INFO:Declaring metric variables
2023-03-05 16:45:07,838:INFO:Importing untrained model
2023-03-05 16:45:07,842:INFO:Extra Trees Regressor Imported successfully
2023-03-05 16:45:07,850:INFO:Starting cross validation
2023-03-05 16:45:07,853:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:45:10,322:INFO:Calculating mean and std
2023-03-05 16:45:10,323:INFO:Creating metrics dataframe
2023-03-05 16:45:10,327:INFO:Uploading results into container
2023-03-05 16:45:10,328:INFO:Uploading model into container now
2023-03-05 16:45:10,329:INFO:_master_model_container: 14
2023-03-05 16:45:10,329:INFO:_display_container: 2
2023-03-05 16:45:10,329:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=7671)
2023-03-05 16:45:10,330:INFO:create_model() successfully completed......................................
2023-03-05 16:45:10,411:INFO:SubProcess create_model() end ==================================
2023-03-05 16:45:10,411:INFO:Creating metrics dataframe
2023-03-05 16:45:10,423:INFO:Initializing AdaBoost Regressor
2023-03-05 16:45:10,423:INFO:Total runtime is 0.4689266403516134 minutes
2023-03-05 16:45:10,427:INFO:SubProcess create_model() called ==================================
2023-03-05 16:45:10,427:INFO:Initializing create_model()
2023-03-05 16:45:10,427:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A4A69BB80>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A4A671430>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:45:10,427:INFO:Checking exceptions
2023-03-05 16:45:10,428:INFO:Importing libraries
2023-03-05 16:45:10,429:INFO:Copying training dataset
2023-03-05 16:45:10,436:INFO:Defining folds
2023-03-05 16:45:10,436:INFO:Declaring metric variables
2023-03-05 16:45:10,441:INFO:Importing untrained model
2023-03-05 16:45:10,446:INFO:AdaBoost Regressor Imported successfully
2023-03-05 16:45:10,454:INFO:Starting cross validation
2023-03-05 16:45:10,455:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:45:12,166:INFO:Calculating mean and std
2023-03-05 16:45:12,168:INFO:Creating metrics dataframe
2023-03-05 16:45:12,172:INFO:Uploading results into container
2023-03-05 16:45:12,172:INFO:Uploading model into container now
2023-03-05 16:45:12,173:INFO:_master_model_container: 15
2023-03-05 16:45:12,173:INFO:_display_container: 2
2023-03-05 16:45:12,173:INFO:AdaBoostRegressor(random_state=7671)
2023-03-05 16:45:12,173:INFO:create_model() successfully completed......................................
2023-03-05 16:45:12,255:INFO:SubProcess create_model() end ==================================
2023-03-05 16:45:12,255:INFO:Creating metrics dataframe
2023-03-05 16:45:12,269:INFO:Initializing Gradient Boosting Regressor
2023-03-05 16:45:12,270:INFO:Total runtime is 0.4996990283330282 minutes
2023-03-05 16:45:12,274:INFO:SubProcess create_model() called ==================================
2023-03-05 16:45:12,274:INFO:Initializing create_model()
2023-03-05 16:45:12,275:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A4A69BB80>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A4A671430>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:45:12,275:INFO:Checking exceptions
2023-03-05 16:45:12,275:INFO:Importing libraries
2023-03-05 16:45:12,275:INFO:Copying training dataset
2023-03-05 16:45:12,280:INFO:Defining folds
2023-03-05 16:45:12,281:INFO:Declaring metric variables
2023-03-05 16:45:12,285:INFO:Importing untrained model
2023-03-05 16:45:12,291:INFO:Gradient Boosting Regressor Imported successfully
2023-03-05 16:45:12,300:INFO:Starting cross validation
2023-03-05 16:45:12,302:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:45:14,122:INFO:Calculating mean and std
2023-03-05 16:45:14,124:INFO:Creating metrics dataframe
2023-03-05 16:45:14,127:INFO:Uploading results into container
2023-03-05 16:45:14,128:INFO:Uploading model into container now
2023-03-05 16:45:14,128:INFO:_master_model_container: 16
2023-03-05 16:45:14,128:INFO:_display_container: 2
2023-03-05 16:45:14,129:INFO:GradientBoostingRegressor(random_state=7671)
2023-03-05 16:45:14,129:INFO:create_model() successfully completed......................................
2023-03-05 16:45:14,209:INFO:SubProcess create_model() end ==================================
2023-03-05 16:45:14,209:INFO:Creating metrics dataframe
2023-03-05 16:45:14,224:INFO:Initializing Extreme Gradient Boosting
2023-03-05 16:45:14,225:INFO:Total runtime is 0.5322797338167827 minutes
2023-03-05 16:45:14,229:INFO:SubProcess create_model() called ==================================
2023-03-05 16:45:14,229:INFO:Initializing create_model()
2023-03-05 16:45:14,229:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A4A69BB80>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A4A671430>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:45:14,229:INFO:Checking exceptions
2023-03-05 16:45:14,229:INFO:Importing libraries
2023-03-05 16:45:14,229:INFO:Copying training dataset
2023-03-05 16:45:14,235:INFO:Defining folds
2023-03-05 16:45:14,235:INFO:Declaring metric variables
2023-03-05 16:45:14,240:INFO:Importing untrained model
2023-03-05 16:45:14,245:INFO:Extreme Gradient Boosting Imported successfully
2023-03-05 16:45:14,253:INFO:Starting cross validation
2023-03-05 16:45:14,256:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:45:16,086:INFO:Calculating mean and std
2023-03-05 16:45:16,088:INFO:Creating metrics dataframe
2023-03-05 16:45:16,091:INFO:Uploading results into container
2023-03-05 16:45:16,092:INFO:Uploading model into container now
2023-03-05 16:45:16,092:INFO:_master_model_container: 17
2023-03-05 16:45:16,092:INFO:_display_container: 2
2023-03-05 16:45:16,093:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=7671, ...)
2023-03-05 16:45:16,093:INFO:create_model() successfully completed......................................
2023-03-05 16:45:16,174:INFO:SubProcess create_model() end ==================================
2023-03-05 16:45:16,175:INFO:Creating metrics dataframe
2023-03-05 16:45:16,189:INFO:Initializing Light Gradient Boosting Machine
2023-03-05 16:45:16,189:INFO:Total runtime is 0.5650271852811178 minutes
2023-03-05 16:45:16,194:INFO:SubProcess create_model() called ==================================
2023-03-05 16:45:16,194:INFO:Initializing create_model()
2023-03-05 16:45:16,195:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A4A69BB80>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A4A671430>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:45:16,195:INFO:Checking exceptions
2023-03-05 16:45:16,195:INFO:Importing libraries
2023-03-05 16:45:16,195:INFO:Copying training dataset
2023-03-05 16:45:16,200:INFO:Defining folds
2023-03-05 16:45:16,201:INFO:Declaring metric variables
2023-03-05 16:45:16,206:INFO:Importing untrained model
2023-03-05 16:45:16,212:INFO:Light Gradient Boosting Machine Imported successfully
2023-03-05 16:45:16,220:INFO:Starting cross validation
2023-03-05 16:45:16,222:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:45:19,508:INFO:Calculating mean and std
2023-03-05 16:45:19,509:INFO:Creating metrics dataframe
2023-03-05 16:45:19,514:INFO:Uploading results into container
2023-03-05 16:45:19,515:INFO:Uploading model into container now
2023-03-05 16:45:19,516:INFO:_master_model_container: 18
2023-03-05 16:45:19,516:INFO:_display_container: 2
2023-03-05 16:45:19,516:INFO:LGBMRegressor(random_state=7671)
2023-03-05 16:45:19,517:INFO:create_model() successfully completed......................................
2023-03-05 16:45:19,601:INFO:SubProcess create_model() end ==================================
2023-03-05 16:45:19,602:INFO:Creating metrics dataframe
2023-03-05 16:45:19,616:INFO:Initializing Dummy Regressor
2023-03-05 16:45:19,617:INFO:Total runtime is 0.6221305052439372 minutes
2023-03-05 16:45:19,621:INFO:SubProcess create_model() called ==================================
2023-03-05 16:45:19,621:INFO:Initializing create_model()
2023-03-05 16:45:19,622:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A4A69BB80>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A4A671430>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:45:19,622:INFO:Checking exceptions
2023-03-05 16:45:19,622:INFO:Importing libraries
2023-03-05 16:45:19,622:INFO:Copying training dataset
2023-03-05 16:45:19,630:INFO:Defining folds
2023-03-05 16:45:19,631:INFO:Declaring metric variables
2023-03-05 16:45:19,635:INFO:Importing untrained model
2023-03-05 16:45:19,640:INFO:Dummy Regressor Imported successfully
2023-03-05 16:45:19,648:INFO:Starting cross validation
2023-03-05 16:45:19,650:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:45:20,640:INFO:Calculating mean and std
2023-03-05 16:45:20,642:INFO:Creating metrics dataframe
2023-03-05 16:45:20,646:INFO:Uploading results into container
2023-03-05 16:45:20,647:INFO:Uploading model into container now
2023-03-05 16:45:20,647:INFO:_master_model_container: 19
2023-03-05 16:45:20,648:INFO:_display_container: 2
2023-03-05 16:45:20,648:INFO:DummyRegressor()
2023-03-05 16:45:20,648:INFO:create_model() successfully completed......................................
2023-03-05 16:45:20,736:INFO:SubProcess create_model() end ==================================
2023-03-05 16:45:20,736:INFO:Creating metrics dataframe
2023-03-05 16:45:20,763:INFO:Initializing create_model()
2023-03-05 16:45:20,764:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A4A69BB80>, estimator=Lasso(random_state=7671), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:45:20,764:INFO:Checking exceptions
2023-03-05 16:45:20,766:INFO:Importing libraries
2023-03-05 16:45:20,766:INFO:Copying training dataset
2023-03-05 16:45:20,771:INFO:Defining folds
2023-03-05 16:45:20,771:INFO:Declaring metric variables
2023-03-05 16:45:20,771:INFO:Importing untrained model
2023-03-05 16:45:20,771:INFO:Declaring custom model
2023-03-05 16:45:20,772:INFO:Lasso Regression Imported successfully
2023-03-05 16:45:20,774:INFO:Cross validation set to False
2023-03-05 16:45:20,774:INFO:Fitting Model
2023-03-05 16:45:20,906:INFO:Lasso(random_state=7671)
2023-03-05 16:45:20,907:INFO:create_model() successfully completed......................................
2023-03-05 16:45:21,022:INFO:_master_model_container: 19
2023-03-05 16:45:21,022:INFO:_display_container: 2
2023-03-05 16:45:21,023:INFO:Lasso(random_state=7671)
2023-03-05 16:45:21,023:INFO:compare_models() successfully completed......................................
2023-03-05 16:45:21,034:INFO:Initializing evaluate_model()
2023-03-05 16:45:21,034:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A4A69BB80>, estimator=Lasso(random_state=7671), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-03-05 16:45:21,058:INFO:Initializing plot_model()
2023-03-05 16:45:21,059:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=Lasso(random_state=7671), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A4A69BB80>, system=True)
2023-03-05 16:45:21,059:INFO:Checking exceptions
2023-03-05 16:45:21,062:INFO:Preloading libraries
2023-03-05 16:45:21,062:INFO:Copying training dataset
2023-03-05 16:45:21,062:INFO:Plot type: pipeline
2023-03-05 16:45:21,218:INFO:Visual Rendered Successfully
2023-03-05 16:45:21,300:INFO:plot_model() successfully completed......................................
2023-03-05 16:45:21,313:INFO:Initializing predict_model()
2023-03-05 16:45:21,314:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A4A69BB80>, estimator=Lasso(random_state=7671), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x0000023A55AC34C0>)
2023-03-05 16:45:21,314:INFO:Checking exceptions
2023-03-05 16:45:21,314:INFO:Preloading libraries
2023-03-05 16:45:21,316:INFO:Set up data.
2023-03-05 16:45:21,321:INFO:Set up index.
2023-03-05 16:54:53,538:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-05 16:54:53,538:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-05 16:54:53,538:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-05 16:54:53,538:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-05 16:54:54,135:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-03-05 16:55:09,958:INFO:PyCaret RegressionExperiment
2023-03-05 16:55:09,958:INFO:Logging name: reg-default-name
2023-03-05 16:55:09,958:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-05 16:55:09,958:INFO:version 3.0.0.rc9
2023-03-05 16:55:09,958:INFO:Initializing setup()
2023-03-05 16:55:09,958:INFO:self.USI: 735c
2023-03-05 16:55:09,958:INFO:self._variable_keys: {'USI', 'data', 'log_plots_param', '_ml_usecase', 'memory', 'transform_target_param', 'html_param', 'X_train', 'pipeline', 'fold_shuffle_param', 'n_jobs_param', 'fold_generator', 'exp_id', 'X_test', 'exp_name_log', 'logging_param', 'y_train', '_available_plots', 'X', 'gpu_param', 'idx', 'gpu_n_jobs_param', 'fold_groups_param', 'target_param', 'y_test', 'seed', 'y'}
2023-03-05 16:55:09,958:INFO:Checking environment
2023-03-05 16:55:09,958:INFO:python_version: 3.9.13
2023-03-05 16:55:09,958:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-03-05 16:55:09,958:INFO:machine: AMD64
2023-03-05 16:55:09,958:INFO:platform: Windows-10-10.0.19044-SP0
2023-03-05 16:55:09,958:INFO:Memory: svmem(total=17009516544, available=5296369664, percent=68.9, used=11713146880, free=5296369664)
2023-03-05 16:55:09,958:INFO:Physical Core: 4
2023-03-05 16:55:09,959:INFO:Logical Core: 8
2023-03-05 16:55:09,959:INFO:Checking libraries
2023-03-05 16:55:09,959:INFO:System:
2023-03-05 16:55:09,959:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-03-05 16:55:09,959:INFO:executable: C:\Users\Alvaro\anaconda3\python.exe
2023-03-05 16:55:09,959:INFO:   machine: Windows-10-10.0.19044-SP0
2023-03-05 16:55:09,959:INFO:PyCaret required dependencies:
2023-03-05 16:55:09,959:INFO:                 pip: 22.2.2
2023-03-05 16:55:09,959:INFO:          setuptools: 63.4.1
2023-03-05 16:55:09,959:INFO:             pycaret: 3.0.0rc9
2023-03-05 16:55:09,959:INFO:             IPython: 7.31.1
2023-03-05 16:55:09,959:INFO:          ipywidgets: 7.6.5
2023-03-05 16:55:09,959:INFO:                tqdm: 4.64.1
2023-03-05 16:55:09,959:INFO:               numpy: 1.21.5
2023-03-05 16:55:09,959:INFO:              pandas: 1.4.4
2023-03-05 16:55:09,959:INFO:              jinja2: 2.11.3
2023-03-05 16:55:09,959:INFO:               scipy: 1.9.1
2023-03-05 16:55:09,959:INFO:              joblib: 1.2.0
2023-03-05 16:55:09,959:INFO:             sklearn: 1.0.2
2023-03-05 16:55:09,959:INFO:                pyod: 1.0.7
2023-03-05 16:55:09,959:INFO:            imblearn: 0.10.1
2023-03-05 16:55:09,959:INFO:   category_encoders: 2.6.0
2023-03-05 16:55:09,959:INFO:            lightgbm: 3.3.5
2023-03-05 16:55:09,959:INFO:               numba: 0.55.1
2023-03-05 16:55:09,959:INFO:            requests: 2.28.1
2023-03-05 16:55:09,959:INFO:          matplotlib: 3.5.2
2023-03-05 16:55:09,960:INFO:          scikitplot: 0.3.7
2023-03-05 16:55:09,960:INFO:         yellowbrick: 1.5
2023-03-05 16:55:09,960:INFO:              plotly: 5.9.0
2023-03-05 16:55:09,960:INFO:             kaleido: 0.2.1
2023-03-05 16:55:09,960:INFO:         statsmodels: 0.13.2
2023-03-05 16:55:09,960:INFO:              sktime: 0.16.1
2023-03-05 16:55:09,960:INFO:               tbats: 1.1.2
2023-03-05 16:55:09,960:INFO:            pmdarima: 2.0.2
2023-03-05 16:55:09,960:INFO:              psutil: 5.9.0
2023-03-05 16:55:09,960:INFO:PyCaret optional dependencies:
2023-03-05 16:55:09,986:INFO:                shap: Not installed
2023-03-05 16:55:09,986:INFO:           interpret: Not installed
2023-03-05 16:55:09,986:INFO:                umap: Not installed
2023-03-05 16:55:09,986:INFO:    pandas_profiling: Not installed
2023-03-05 16:55:09,986:INFO:  explainerdashboard: Not installed
2023-03-05 16:55:09,986:INFO:             autoviz: Not installed
2023-03-05 16:55:09,987:INFO:           fairlearn: Not installed
2023-03-05 16:55:09,987:INFO:             xgboost: 1.7.4
2023-03-05 16:55:09,987:INFO:            catboost: Not installed
2023-03-05 16:55:09,987:INFO:              kmodes: Not installed
2023-03-05 16:55:09,987:INFO:             mlxtend: Not installed
2023-03-05 16:55:09,987:INFO:       statsforecast: Not installed
2023-03-05 16:55:09,987:INFO:        tune_sklearn: Not installed
2023-03-05 16:55:09,987:INFO:                 ray: Not installed
2023-03-05 16:55:09,987:INFO:            hyperopt: Not installed
2023-03-05 16:55:09,987:INFO:              optuna: Not installed
2023-03-05 16:55:09,987:INFO:               skopt: Not installed
2023-03-05 16:55:09,987:INFO:              mlflow: Not installed
2023-03-05 16:55:09,987:INFO:              gradio: Not installed
2023-03-05 16:55:09,987:INFO:             fastapi: Not installed
2023-03-05 16:55:09,987:INFO:             uvicorn: Not installed
2023-03-05 16:55:09,987:INFO:              m2cgen: Not installed
2023-03-05 16:55:09,987:INFO:           evidently: Not installed
2023-03-05 16:55:09,987:INFO:               fugue: Not installed
2023-03-05 16:55:09,987:INFO:           streamlit: Not installed
2023-03-05 16:55:09,987:INFO:             prophet: Not installed
2023-03-05 16:55:09,987:INFO:None
2023-03-05 16:55:09,987:INFO:Set up data.
2023-03-05 16:55:09,997:INFO:Set up train/test split.
2023-03-05 16:55:10,002:INFO:Set up index.
2023-03-05 16:55:10,002:INFO:Set up folding strategy.
2023-03-05 16:55:10,002:INFO:Assigning column types.
2023-03-05 16:55:10,005:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-05 16:55:10,005:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-05 16:55:10,010:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 16:55:10,014:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:55:10,068:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:55:10,109:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:55:10,110:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:55:10,146:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:55:10,147:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-05 16:55:10,151:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 16:55:10,155:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:55:10,207:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:55:10,247:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:55:10,248:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:55:10,250:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:55:10,250:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-03-05 16:55:10,255:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 16:55:10,259:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:55:10,312:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:55:10,354:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:55:10,354:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:55:10,357:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:55:10,361:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 16:55:10,366:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:55:10,417:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:55:10,457:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:55:10,458:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:55:10,460:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:55:10,461:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-03-05 16:55:10,469:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:55:10,521:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:55:10,561:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:55:10,561:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:55:10,564:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:55:10,573:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:55:10,626:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:55:10,668:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:55:10,669:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:55:10,671:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:55:10,672:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-03-05 16:55:10,734:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:55:10,775:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:55:10,776:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:55:10,778:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:55:10,843:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:55:10,885:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:55:10,886:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:55:10,888:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:55:10,888:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-05 16:55:10,962:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:55:11,008:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:55:11,010:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:55:11,089:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:55:11,145:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:55:11,148:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:55:11,148:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-03-05 16:55:11,272:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:55:11,275:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:55:11,391:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:55:11,394:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:55:11,395:INFO:Preparing preprocessing pipeline...
2023-03-05 16:55:11,396:INFO:Set up simple imputation.
2023-03-05 16:55:11,399:INFO:Set up encoding of categorical features.
2023-03-05 16:55:11,497:INFO:Finished creating preprocessing pipeline.
2023-03-05 16:55:11,506:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Alvaro\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['work_year', 'remote_ratio'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['experience_level',
                                             'employment_type', 'job_title',
                                             'employee_residence',
                                             'company_location',
                                             'company_size'],
                                    t...
                                    transformer=OneHotEncoder(cols=['experience_level',
                                                                    'employment_type',
                                                                    'company_size'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['job_title', 'employee_residence',
                                             'company_location'],
                                    transformer=LeaveOneOutEncoder(cols=['job_title',
                                                                         'employee_residence',
                                                                         'company_location'],
                                                                   handle_missing='return_nan',
                                                                   random_state=1658)))])
2023-03-05 16:55:11,506:INFO:Creating final display dataframe.
2023-03-05 16:55:11,977:INFO:Setup _display_container:                     Description             Value
0                    Session id              1658
1                        Target     salary_in_usd
2                   Target type        Regression
3           Original data shape          (500, 9)
4        Transformed data shape         (500, 17)
5   Transformed train set shape         (350, 17)
6    Transformed test set shape         (150, 17)
7              Numeric features                 2
8          Categorical features                 6
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator             KFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  reg-default-name
21                          USI              735c
2023-03-05 16:55:12,103:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:55:12,106:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:55:12,210:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:55:12,213:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:55:12,213:INFO:setup() successfully completed in 2.26s...............
2023-03-05 16:56:05,250:INFO:PyCaret RegressionExperiment
2023-03-05 16:56:05,250:INFO:Logging name: reg-default-name
2023-03-05 16:56:05,250:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-05 16:56:05,250:INFO:version 3.0.0.rc9
2023-03-05 16:56:05,250:INFO:Initializing setup()
2023-03-05 16:56:05,250:INFO:self.USI: 4424
2023-03-05 16:56:05,250:INFO:self._variable_keys: {'USI', 'data', 'log_plots_param', '_ml_usecase', 'memory', 'transform_target_param', 'html_param', 'X_train', 'pipeline', 'fold_shuffle_param', 'n_jobs_param', 'fold_generator', 'exp_id', 'X_test', 'exp_name_log', 'logging_param', 'y_train', '_available_plots', 'X', 'gpu_param', 'idx', 'gpu_n_jobs_param', 'fold_groups_param', 'target_param', 'y_test', 'seed', 'y'}
2023-03-05 16:56:05,250:INFO:Checking environment
2023-03-05 16:56:05,250:INFO:python_version: 3.9.13
2023-03-05 16:56:05,250:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-03-05 16:56:05,250:INFO:machine: AMD64
2023-03-05 16:56:05,250:INFO:platform: Windows-10-10.0.19044-SP0
2023-03-05 16:56:05,250:INFO:Memory: svmem(total=17009516544, available=5121593344, percent=69.9, used=11887923200, free=5121593344)
2023-03-05 16:56:05,250:INFO:Physical Core: 4
2023-03-05 16:56:05,250:INFO:Logical Core: 8
2023-03-05 16:56:05,250:INFO:Checking libraries
2023-03-05 16:56:05,250:INFO:System:
2023-03-05 16:56:05,250:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-03-05 16:56:05,250:INFO:executable: C:\Users\Alvaro\anaconda3\python.exe
2023-03-05 16:56:05,251:INFO:   machine: Windows-10-10.0.19044-SP0
2023-03-05 16:56:05,251:INFO:PyCaret required dependencies:
2023-03-05 16:56:05,251:INFO:                 pip: 22.2.2
2023-03-05 16:56:05,251:INFO:          setuptools: 63.4.1
2023-03-05 16:56:05,251:INFO:             pycaret: 3.0.0rc9
2023-03-05 16:56:05,251:INFO:             IPython: 7.31.1
2023-03-05 16:56:05,251:INFO:          ipywidgets: 7.6.5
2023-03-05 16:56:05,251:INFO:                tqdm: 4.64.1
2023-03-05 16:56:05,251:INFO:               numpy: 1.21.5
2023-03-05 16:56:05,251:INFO:              pandas: 1.4.4
2023-03-05 16:56:05,251:INFO:              jinja2: 2.11.3
2023-03-05 16:56:05,251:INFO:               scipy: 1.9.1
2023-03-05 16:56:05,251:INFO:              joblib: 1.2.0
2023-03-05 16:56:05,251:INFO:             sklearn: 1.0.2
2023-03-05 16:56:05,251:INFO:                pyod: 1.0.7
2023-03-05 16:56:05,251:INFO:            imblearn: 0.10.1
2023-03-05 16:56:05,251:INFO:   category_encoders: 2.6.0
2023-03-05 16:56:05,251:INFO:            lightgbm: 3.3.5
2023-03-05 16:56:05,251:INFO:               numba: 0.55.1
2023-03-05 16:56:05,251:INFO:            requests: 2.28.1
2023-03-05 16:56:05,251:INFO:          matplotlib: 3.5.2
2023-03-05 16:56:05,252:INFO:          scikitplot: 0.3.7
2023-03-05 16:56:05,252:INFO:         yellowbrick: 1.5
2023-03-05 16:56:05,252:INFO:              plotly: 5.9.0
2023-03-05 16:56:05,252:INFO:             kaleido: 0.2.1
2023-03-05 16:56:05,252:INFO:         statsmodels: 0.13.2
2023-03-05 16:56:05,252:INFO:              sktime: 0.16.1
2023-03-05 16:56:05,252:INFO:               tbats: 1.1.2
2023-03-05 16:56:05,252:INFO:            pmdarima: 2.0.2
2023-03-05 16:56:05,252:INFO:              psutil: 5.9.0
2023-03-05 16:56:05,252:INFO:PyCaret optional dependencies:
2023-03-05 16:56:05,252:INFO:                shap: Not installed
2023-03-05 16:56:05,252:INFO:           interpret: Not installed
2023-03-05 16:56:05,252:INFO:                umap: Not installed
2023-03-05 16:56:05,252:INFO:    pandas_profiling: Not installed
2023-03-05 16:56:05,252:INFO:  explainerdashboard: Not installed
2023-03-05 16:56:05,252:INFO:             autoviz: Not installed
2023-03-05 16:56:05,252:INFO:           fairlearn: Not installed
2023-03-05 16:56:05,252:INFO:             xgboost: 1.7.4
2023-03-05 16:56:05,252:INFO:            catboost: Not installed
2023-03-05 16:56:05,252:INFO:              kmodes: Not installed
2023-03-05 16:56:05,252:INFO:             mlxtend: Not installed
2023-03-05 16:56:05,252:INFO:       statsforecast: Not installed
2023-03-05 16:56:05,252:INFO:        tune_sklearn: Not installed
2023-03-05 16:56:05,252:INFO:                 ray: Not installed
2023-03-05 16:56:05,253:INFO:            hyperopt: Not installed
2023-03-05 16:56:05,253:INFO:              optuna: Not installed
2023-03-05 16:56:05,253:INFO:               skopt: Not installed
2023-03-05 16:56:05,253:INFO:              mlflow: Not installed
2023-03-05 16:56:05,253:INFO:              gradio: Not installed
2023-03-05 16:56:05,253:INFO:             fastapi: Not installed
2023-03-05 16:56:05,253:INFO:             uvicorn: Not installed
2023-03-05 16:56:05,253:INFO:              m2cgen: Not installed
2023-03-05 16:56:05,253:INFO:           evidently: Not installed
2023-03-05 16:56:05,253:INFO:               fugue: Not installed
2023-03-05 16:56:05,253:INFO:           streamlit: Not installed
2023-03-05 16:56:05,253:INFO:             prophet: Not installed
2023-03-05 16:56:05,253:INFO:None
2023-03-05 16:56:05,253:INFO:Set up data.
2023-03-05 16:56:05,262:INFO:Set up train/test split.
2023-03-05 16:56:05,268:INFO:Set up index.
2023-03-05 16:56:05,268:INFO:Set up folding strategy.
2023-03-05 16:56:05,268:INFO:Assigning column types.
2023-03-05 16:56:05,271:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-05 16:56:05,271:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-05 16:56:05,276:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 16:56:05,282:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:56:05,339:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:56:05,381:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:56:05,381:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:56:05,383:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:56:05,384:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-05 16:56:05,388:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 16:56:05,392:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:56:05,446:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:56:05,486:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:56:05,487:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:56:05,489:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:56:05,489:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-03-05 16:56:05,493:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 16:56:05,498:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:56:05,550:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:56:05,590:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:56:05,590:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:56:05,593:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:56:05,597:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 16:56:05,601:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:56:05,653:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:56:05,694:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:56:05,694:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:56:05,697:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:56:05,697:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-03-05 16:56:05,706:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:56:05,758:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:56:05,799:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:56:05,800:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:56:05,802:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:56:05,810:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:56:05,863:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:56:05,904:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:56:05,904:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:56:05,907:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:56:05,907:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-03-05 16:56:05,968:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:56:06,008:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:56:06,009:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:56:06,011:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:56:06,072:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:56:06,112:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:56:06,112:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:56:06,115:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:56:06,115:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-05 16:56:06,175:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:56:06,215:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:56:06,218:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:56:06,278:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:56:06,318:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:56:06,321:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:56:06,321:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-03-05 16:56:06,426:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:56:06,428:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:56:06,535:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:56:06,538:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:56:06,539:INFO:Preparing preprocessing pipeline...
2023-03-05 16:56:06,540:INFO:Set up simple imputation.
2023-03-05 16:56:06,542:INFO:Set up encoding of categorical features.
2023-03-05 16:56:06,634:INFO:Finished creating preprocessing pipeline.
2023-03-05 16:56:06,643:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Alvaro\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['work_year', 'remote_ratio'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['experience_level',
                                             'employment_type', 'job_title',
                                             'employee_residence',
                                             'company_location',
                                             'company_size'],
                                    t...
                                    transformer=OneHotEncoder(cols=['experience_level',
                                                                    'employment_type',
                                                                    'company_size'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['job_title', 'employee_residence',
                                             'company_location'],
                                    transformer=LeaveOneOutEncoder(cols=['job_title',
                                                                         'employee_residence',
                                                                         'company_location'],
                                                                   handle_missing='return_nan',
                                                                   random_state=7690)))])
2023-03-05 16:56:06,643:INFO:Creating final display dataframe.
2023-03-05 16:56:07,091:INFO:Setup _display_container:                     Description             Value
0                    Session id              7690
1                        Target     salary_in_usd
2                   Target type        Regression
3           Original data shape          (500, 9)
4        Transformed data shape         (500, 17)
5   Transformed train set shape         (350, 17)
6    Transformed test set shape         (150, 17)
7              Numeric features                 2
8          Categorical features                 6
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator             KFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  reg-default-name
21                          USI              4424
2023-03-05 16:56:07,206:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:56:07,209:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:56:07,311:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:56:07,313:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:56:07,314:INFO:setup() successfully completed in 2.07s...............
2023-03-05 16:56:07,314:INFO:Initializing create_model()
2023-03-05 16:56:07,314:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537C7D1370>, estimator=dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:56:07,314:INFO:Checking exceptions
2023-03-05 16:56:07,344:INFO:Importing libraries
2023-03-05 16:56:07,345:INFO:Copying training dataset
2023-03-05 16:56:07,354:INFO:Defining folds
2023-03-05 16:56:07,354:INFO:Declaring metric variables
2023-03-05 16:56:07,359:INFO:Importing untrained model
2023-03-05 16:56:07,364:INFO:Decision Tree Regressor Imported successfully
2023-03-05 16:56:07,374:INFO:Starting cross validation
2023-03-05 16:56:07,381:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:56:16,071:INFO:Calculating mean and std
2023-03-05 16:56:16,073:INFO:Creating metrics dataframe
2023-03-05 16:56:16,079:INFO:Finalizing model
2023-03-05 16:56:16,204:INFO:Uploading results into container
2023-03-05 16:56:16,205:INFO:Uploading model into container now
2023-03-05 16:56:16,217:INFO:_master_model_container: 1
2023-03-05 16:56:16,217:INFO:_display_container: 2
2023-03-05 16:56:16,217:INFO:DecisionTreeRegressor(random_state=7690)
2023-03-05 16:56:16,218:INFO:create_model() successfully completed......................................
2023-03-05 16:56:16,305:INFO:Initializing tune_model()
2023-03-05 16:56:16,306:INFO:tune_model(estimator=DecisionTreeRegressor(random_state=7690), fold=None, round=4, n_iter=10, custom_grid=None, optimize=R2, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537C7D1370>)
2023-03-05 16:56:16,306:INFO:Checking exceptions
2023-03-05 16:56:16,332:INFO:Copying training dataset
2023-03-05 16:56:16,335:INFO:Checking base model
2023-03-05 16:56:16,336:INFO:Base model : Decision Tree Regressor
2023-03-05 16:56:16,340:INFO:Declaring metric variables
2023-03-05 16:56:16,349:INFO:Defining Hyperparameters
2023-03-05 16:56:16,443:INFO:Tuning with n_jobs=-1
2023-03-05 16:56:16,443:INFO:Initializing RandomizedSearchCV
2023-03-05 16:56:24,771:INFO:best_params: {'actual_estimator__min_samples_split': 5, 'actual_estimator__min_samples_leaf': 4, 'actual_estimator__min_impurity_decrease': 0.5, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 5, 'actual_estimator__criterion': 'absolute_error'}
2023-03-05 16:56:24,773:INFO:Hyperparameter search completed
2023-03-05 16:56:24,773:INFO:SubProcess create_model() called ==================================
2023-03-05 16:56:24,774:INFO:Initializing create_model()
2023-03-05 16:56:24,774:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537C7D1370>, estimator=DecisionTreeRegressor(random_state=7690), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015371652A30>, model_only=True, return_train_score=False, kwargs={'min_samples_split': 5, 'min_samples_leaf': 4, 'min_impurity_decrease': 0.5, 'max_features': 1.0, 'max_depth': 5, 'criterion': 'absolute_error'})
2023-03-05 16:56:24,774:INFO:Checking exceptions
2023-03-05 16:56:24,774:INFO:Importing libraries
2023-03-05 16:56:24,774:INFO:Copying training dataset
2023-03-05 16:56:24,782:INFO:Defining folds
2023-03-05 16:56:24,782:INFO:Declaring metric variables
2023-03-05 16:56:24,789:INFO:Importing untrained model
2023-03-05 16:56:24,789:INFO:Declaring custom model
2023-03-05 16:56:24,796:INFO:Decision Tree Regressor Imported successfully
2023-03-05 16:56:24,809:INFO:Starting cross validation
2023-03-05 16:56:24,812:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:56:25,468:INFO:Calculating mean and std
2023-03-05 16:56:25,470:INFO:Creating metrics dataframe
2023-03-05 16:56:25,477:INFO:Finalizing model
2023-03-05 16:56:25,641:INFO:Uploading results into container
2023-03-05 16:56:25,643:INFO:Uploading model into container now
2023-03-05 16:56:25,644:INFO:_master_model_container: 2
2023-03-05 16:56:25,644:INFO:_display_container: 3
2023-03-05 16:56:25,644:INFO:DecisionTreeRegressor(criterion='absolute_error', max_depth=5, max_features=1.0,
                      min_impurity_decrease=0.5, min_samples_leaf=4,
                      min_samples_split=5, random_state=7690)
2023-03-05 16:56:25,645:INFO:create_model() successfully completed......................................
2023-03-05 16:56:25,731:INFO:SubProcess create_model() end ==================================
2023-03-05 16:56:25,731:INFO:choose_better activated
2023-03-05 16:56:25,735:INFO:SubProcess create_model() called ==================================
2023-03-05 16:56:25,735:INFO:Initializing create_model()
2023-03-05 16:56:25,735:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537C7D1370>, estimator=DecisionTreeRegressor(random_state=7690), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:56:25,736:INFO:Checking exceptions
2023-03-05 16:56:25,737:INFO:Importing libraries
2023-03-05 16:56:25,738:INFO:Copying training dataset
2023-03-05 16:56:25,742:INFO:Defining folds
2023-03-05 16:56:25,743:INFO:Declaring metric variables
2023-03-05 16:56:25,743:INFO:Importing untrained model
2023-03-05 16:56:25,743:INFO:Declaring custom model
2023-03-05 16:56:25,743:INFO:Decision Tree Regressor Imported successfully
2023-03-05 16:56:25,744:INFO:Starting cross validation
2023-03-05 16:56:25,745:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:56:26,411:INFO:Calculating mean and std
2023-03-05 16:56:26,412:INFO:Creating metrics dataframe
2023-03-05 16:56:26,414:INFO:Finalizing model
2023-03-05 16:56:26,556:INFO:Uploading results into container
2023-03-05 16:56:26,557:INFO:Uploading model into container now
2023-03-05 16:56:26,557:INFO:_master_model_container: 3
2023-03-05 16:56:26,557:INFO:_display_container: 4
2023-03-05 16:56:26,557:INFO:DecisionTreeRegressor(random_state=7690)
2023-03-05 16:56:26,558:INFO:create_model() successfully completed......................................
2023-03-05 16:56:26,634:INFO:SubProcess create_model() end ==================================
2023-03-05 16:56:26,634:INFO:DecisionTreeRegressor(random_state=7690) result for R2 is 0.3343
2023-03-05 16:56:26,635:INFO:DecisionTreeRegressor(criterion='absolute_error', max_depth=5, max_features=1.0,
                      min_impurity_decrease=0.5, min_samples_leaf=4,
                      min_samples_split=5, random_state=7690) result for R2 is 0.3564
2023-03-05 16:56:26,635:INFO:DecisionTreeRegressor(criterion='absolute_error', max_depth=5, max_features=1.0,
                      min_impurity_decrease=0.5, min_samples_leaf=4,
                      min_samples_split=5, random_state=7690) is best model
2023-03-05 16:56:26,635:INFO:choose_better completed
2023-03-05 16:56:26,644:INFO:_master_model_container: 3
2023-03-05 16:56:26,644:INFO:_display_container: 3
2023-03-05 16:56:26,645:INFO:DecisionTreeRegressor(criterion='absolute_error', max_depth=5, max_features=1.0,
                      min_impurity_decrease=0.5, min_samples_leaf=4,
                      min_samples_split=5, random_state=7690)
2023-03-05 16:56:26,645:INFO:tune_model() successfully completed......................................
2023-03-05 16:57:04,019:INFO:PyCaret RegressionExperiment
2023-03-05 16:57:04,019:INFO:Logging name: reg-default-name
2023-03-05 16:57:04,019:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-05 16:57:04,019:INFO:version 3.0.0.rc9
2023-03-05 16:57:04,019:INFO:Initializing setup()
2023-03-05 16:57:04,019:INFO:self.USI: 810a
2023-03-05 16:57:04,019:INFO:self._variable_keys: {'USI', 'data', 'log_plots_param', '_ml_usecase', 'memory', 'transform_target_param', 'html_param', 'X_train', 'pipeline', 'fold_shuffle_param', 'n_jobs_param', 'fold_generator', 'exp_id', 'X_test', 'exp_name_log', 'logging_param', 'y_train', '_available_plots', 'X', 'gpu_param', 'idx', 'gpu_n_jobs_param', 'fold_groups_param', 'target_param', 'y_test', 'seed', 'y'}
2023-03-05 16:57:04,019:INFO:Checking environment
2023-03-05 16:57:04,019:INFO:python_version: 3.9.13
2023-03-05 16:57:04,019:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-03-05 16:57:04,020:INFO:machine: AMD64
2023-03-05 16:57:04,020:INFO:platform: Windows-10-10.0.19044-SP0
2023-03-05 16:57:04,020:INFO:Memory: svmem(total=17009516544, available=4349280256, percent=74.4, used=12660236288, free=4349280256)
2023-03-05 16:57:04,020:INFO:Physical Core: 4
2023-03-05 16:57:04,020:INFO:Logical Core: 8
2023-03-05 16:57:04,020:INFO:Checking libraries
2023-03-05 16:57:04,020:INFO:System:
2023-03-05 16:57:04,020:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-03-05 16:57:04,020:INFO:executable: C:\Users\Alvaro\anaconda3\python.exe
2023-03-05 16:57:04,020:INFO:   machine: Windows-10-10.0.19044-SP0
2023-03-05 16:57:04,020:INFO:PyCaret required dependencies:
2023-03-05 16:57:04,020:INFO:                 pip: 22.2.2
2023-03-05 16:57:04,020:INFO:          setuptools: 63.4.1
2023-03-05 16:57:04,020:INFO:             pycaret: 3.0.0rc9
2023-03-05 16:57:04,020:INFO:             IPython: 7.31.1
2023-03-05 16:57:04,020:INFO:          ipywidgets: 7.6.5
2023-03-05 16:57:04,020:INFO:                tqdm: 4.64.1
2023-03-05 16:57:04,020:INFO:               numpy: 1.21.5
2023-03-05 16:57:04,020:INFO:              pandas: 1.4.4
2023-03-05 16:57:04,020:INFO:              jinja2: 2.11.3
2023-03-05 16:57:04,021:INFO:               scipy: 1.9.1
2023-03-05 16:57:04,021:INFO:              joblib: 1.2.0
2023-03-05 16:57:04,021:INFO:             sklearn: 1.0.2
2023-03-05 16:57:04,021:INFO:                pyod: 1.0.7
2023-03-05 16:57:04,021:INFO:            imblearn: 0.10.1
2023-03-05 16:57:04,021:INFO:   category_encoders: 2.6.0
2023-03-05 16:57:04,021:INFO:            lightgbm: 3.3.5
2023-03-05 16:57:04,021:INFO:               numba: 0.55.1
2023-03-05 16:57:04,021:INFO:            requests: 2.28.1
2023-03-05 16:57:04,021:INFO:          matplotlib: 3.5.2
2023-03-05 16:57:04,021:INFO:          scikitplot: 0.3.7
2023-03-05 16:57:04,021:INFO:         yellowbrick: 1.5
2023-03-05 16:57:04,021:INFO:              plotly: 5.9.0
2023-03-05 16:57:04,021:INFO:             kaleido: 0.2.1
2023-03-05 16:57:04,021:INFO:         statsmodels: 0.13.2
2023-03-05 16:57:04,021:INFO:              sktime: 0.16.1
2023-03-05 16:57:04,021:INFO:               tbats: 1.1.2
2023-03-05 16:57:04,021:INFO:            pmdarima: 2.0.2
2023-03-05 16:57:04,021:INFO:              psutil: 5.9.0
2023-03-05 16:57:04,021:INFO:PyCaret optional dependencies:
2023-03-05 16:57:04,021:INFO:                shap: Not installed
2023-03-05 16:57:04,021:INFO:           interpret: Not installed
2023-03-05 16:57:04,021:INFO:                umap: Not installed
2023-03-05 16:57:04,022:INFO:    pandas_profiling: Not installed
2023-03-05 16:57:04,022:INFO:  explainerdashboard: Not installed
2023-03-05 16:57:04,022:INFO:             autoviz: Not installed
2023-03-05 16:57:04,022:INFO:           fairlearn: Not installed
2023-03-05 16:57:04,022:INFO:             xgboost: 1.7.4
2023-03-05 16:57:04,022:INFO:            catboost: Not installed
2023-03-05 16:57:04,022:INFO:              kmodes: Not installed
2023-03-05 16:57:04,022:INFO:             mlxtend: Not installed
2023-03-05 16:57:04,022:INFO:       statsforecast: Not installed
2023-03-05 16:57:04,022:INFO:        tune_sklearn: Not installed
2023-03-05 16:57:04,022:INFO:                 ray: Not installed
2023-03-05 16:57:04,022:INFO:            hyperopt: Not installed
2023-03-05 16:57:04,022:INFO:              optuna: Not installed
2023-03-05 16:57:04,022:INFO:               skopt: Not installed
2023-03-05 16:57:04,022:INFO:              mlflow: Not installed
2023-03-05 16:57:04,022:INFO:              gradio: Not installed
2023-03-05 16:57:04,022:INFO:             fastapi: Not installed
2023-03-05 16:57:04,022:INFO:             uvicorn: Not installed
2023-03-05 16:57:04,022:INFO:              m2cgen: Not installed
2023-03-05 16:57:04,022:INFO:           evidently: Not installed
2023-03-05 16:57:04,022:INFO:               fugue: Not installed
2023-03-05 16:57:04,022:INFO:           streamlit: Not installed
2023-03-05 16:57:04,022:INFO:             prophet: Not installed
2023-03-05 16:57:04,023:INFO:None
2023-03-05 16:57:04,023:INFO:Set up data.
2023-03-05 16:57:04,029:INFO:Set up train/test split.
2023-03-05 16:57:04,033:INFO:Set up index.
2023-03-05 16:57:04,034:INFO:Set up folding strategy.
2023-03-05 16:57:04,034:INFO:Assigning column types.
2023-03-05 16:57:04,037:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-05 16:57:04,037:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-05 16:57:04,043:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 16:57:04,048:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:57:04,106:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:57:04,150:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:57:04,151:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:57:04,153:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:57:04,153:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-05 16:57:04,158:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 16:57:04,162:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:57:04,212:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:57:04,253:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:57:04,253:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:57:04,256:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:57:04,256:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-03-05 16:57:04,260:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 16:57:04,264:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:57:04,314:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:57:04,353:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:57:04,353:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:57:04,356:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:57:04,360:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 16:57:04,364:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:57:04,416:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:57:04,455:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:57:04,456:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:57:04,458:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:57:04,459:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-03-05 16:57:04,467:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:57:04,517:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:57:04,556:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:57:04,557:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:57:04,559:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:57:04,567:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:57:04,617:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:57:04,656:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:57:04,656:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:57:04,659:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:57:04,659:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-03-05 16:57:04,724:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:57:04,765:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:57:04,765:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:57:04,767:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:57:04,827:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:57:04,870:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:57:04,870:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:57:04,873:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:57:04,873:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-05 16:57:04,935:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:57:04,991:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:57:04,993:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:57:05,054:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:57:05,093:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:57:05,095:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:57:05,096:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-03-05 16:57:05,197:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:57:05,199:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:57:05,307:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:57:05,309:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:57:05,311:INFO:Preparing preprocessing pipeline...
2023-03-05 16:57:05,311:INFO:Set up simple imputation.
2023-03-05 16:57:05,313:INFO:Set up encoding of categorical features.
2023-03-05 16:57:05,415:INFO:Finished creating preprocessing pipeline.
2023-03-05 16:57:05,425:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Alvaro\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['work_year', 'remote_ratio'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['experience_level',
                                             'employment_type', 'job_title',
                                             'employee_residence',
                                             'company_location',
                                             'company_size'],
                                    t...
                                    transformer=OneHotEncoder(cols=['experience_level',
                                                                    'employment_type',
                                                                    'company_size'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['job_title', 'employee_residence',
                                             'company_location'],
                                    transformer=LeaveOneOutEncoder(cols=['job_title',
                                                                         'employee_residence',
                                                                         'company_location'],
                                                                   handle_missing='return_nan',
                                                                   random_state=4071)))])
2023-03-05 16:57:05,425:INFO:Creating final display dataframe.
2023-03-05 16:57:05,956:INFO:Setup _display_container:                     Description             Value
0                    Session id              4071
1                        Target     salary_in_usd
2                   Target type        Regression
3           Original data shape          (500, 9)
4        Transformed data shape         (500, 16)
5   Transformed train set shape         (350, 16)
6    Transformed test set shape         (150, 16)
7              Numeric features                 2
8          Categorical features                 6
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator             KFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  reg-default-name
21                          USI              810a
2023-03-05 16:57:06,068:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:57:06,070:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:57:06,166:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:57:06,169:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:57:06,169:INFO:setup() successfully completed in 2.15s...............
2023-03-05 16:57:06,169:INFO:Initializing create_model()
2023-03-05 16:57:06,170:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537D5E29D0>, estimator=dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:57:06,170:INFO:Checking exceptions
2023-03-05 16:57:06,203:INFO:Importing libraries
2023-03-05 16:57:06,203:INFO:Copying training dataset
2023-03-05 16:57:06,216:INFO:Defining folds
2023-03-05 16:57:06,217:INFO:Declaring metric variables
2023-03-05 16:57:06,225:INFO:Importing untrained model
2023-03-05 16:57:06,232:INFO:Decision Tree Regressor Imported successfully
2023-03-05 16:57:06,241:INFO:Starting cross validation
2023-03-05 16:57:06,244:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:57:07,066:INFO:Calculating mean and std
2023-03-05 16:57:07,067:INFO:Creating metrics dataframe
2023-03-05 16:57:07,071:INFO:Finalizing model
2023-03-05 16:57:07,186:INFO:Uploading results into container
2023-03-05 16:57:07,186:INFO:Uploading model into container now
2023-03-05 16:57:07,198:INFO:_master_model_container: 1
2023-03-05 16:57:07,199:INFO:_display_container: 2
2023-03-05 16:57:07,199:INFO:DecisionTreeRegressor(random_state=4071)
2023-03-05 16:57:07,199:INFO:create_model() successfully completed......................................
2023-03-05 16:57:07,286:INFO:Initializing tune_model()
2023-03-05 16:57:07,286:INFO:tune_model(estimator=DecisionTreeRegressor(random_state=4071), fold=None, round=4, n_iter=100, custom_grid=None, optimize=R2, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537D5E29D0>)
2023-03-05 16:57:07,286:INFO:Checking exceptions
2023-03-05 16:57:07,320:INFO:Copying training dataset
2023-03-05 16:57:07,324:INFO:Checking base model
2023-03-05 16:57:07,324:INFO:Base model : Decision Tree Regressor
2023-03-05 16:57:07,341:INFO:Declaring metric variables
2023-03-05 16:57:07,352:INFO:Defining Hyperparameters
2023-03-05 16:57:07,504:INFO:Tuning with n_jobs=-1
2023-03-05 16:57:07,504:INFO:Initializing RandomizedSearchCV
2023-03-05 16:58:38,538:INFO:best_params: {'actual_estimator__min_samples_split': 7, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0.001, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 5, 'actual_estimator__criterion': 'squared_error'}
2023-03-05 16:58:38,539:INFO:Hyperparameter search completed
2023-03-05 16:58:38,539:INFO:SubProcess create_model() called ==================================
2023-03-05 16:58:38,540:INFO:Initializing create_model()
2023-03-05 16:58:38,540:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537D5E29D0>, estimator=DecisionTreeRegressor(random_state=4071), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CA64E20>, model_only=True, return_train_score=False, kwargs={'min_samples_split': 7, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.001, 'max_features': 1.0, 'max_depth': 5, 'criterion': 'squared_error'})
2023-03-05 16:58:38,540:INFO:Checking exceptions
2023-03-05 16:58:38,540:INFO:Importing libraries
2023-03-05 16:58:38,540:INFO:Copying training dataset
2023-03-05 16:58:38,546:INFO:Defining folds
2023-03-05 16:58:38,546:INFO:Declaring metric variables
2023-03-05 16:58:38,551:INFO:Importing untrained model
2023-03-05 16:58:38,551:INFO:Declaring custom model
2023-03-05 16:58:38,558:INFO:Decision Tree Regressor Imported successfully
2023-03-05 16:58:38,567:INFO:Starting cross validation
2023-03-05 16:58:38,569:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:58:39,136:INFO:Calculating mean and std
2023-03-05 16:58:39,138:INFO:Creating metrics dataframe
2023-03-05 16:58:39,144:INFO:Finalizing model
2023-03-05 16:58:39,298:INFO:Uploading results into container
2023-03-05 16:58:39,299:INFO:Uploading model into container now
2023-03-05 16:58:39,299:INFO:_master_model_container: 2
2023-03-05 16:58:39,300:INFO:_display_container: 3
2023-03-05 16:58:39,300:INFO:DecisionTreeRegressor(max_depth=5, max_features=1.0,
                      min_impurity_decrease=0.001, min_samples_leaf=6,
                      min_samples_split=7, random_state=4071)
2023-03-05 16:58:39,300:INFO:create_model() successfully completed......................................
2023-03-05 16:58:39,382:INFO:SubProcess create_model() end ==================================
2023-03-05 16:58:39,382:INFO:choose_better activated
2023-03-05 16:58:39,386:INFO:SubProcess create_model() called ==================================
2023-03-05 16:58:39,386:INFO:Initializing create_model()
2023-03-05 16:58:39,386:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537D5E29D0>, estimator=DecisionTreeRegressor(random_state=4071), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:58:39,387:INFO:Checking exceptions
2023-03-05 16:58:39,388:INFO:Importing libraries
2023-03-05 16:58:39,389:INFO:Copying training dataset
2023-03-05 16:58:39,394:INFO:Defining folds
2023-03-05 16:58:39,394:INFO:Declaring metric variables
2023-03-05 16:58:39,394:INFO:Importing untrained model
2023-03-05 16:58:39,394:INFO:Declaring custom model
2023-03-05 16:58:39,395:INFO:Decision Tree Regressor Imported successfully
2023-03-05 16:58:39,395:INFO:Starting cross validation
2023-03-05 16:58:39,396:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:58:40,249:INFO:Calculating mean and std
2023-03-05 16:58:40,250:INFO:Creating metrics dataframe
2023-03-05 16:58:40,252:INFO:Finalizing model
2023-03-05 16:58:40,394:INFO:Uploading results into container
2023-03-05 16:58:40,395:INFO:Uploading model into container now
2023-03-05 16:58:40,395:INFO:_master_model_container: 3
2023-03-05 16:58:40,395:INFO:_display_container: 4
2023-03-05 16:58:40,395:INFO:DecisionTreeRegressor(random_state=4071)
2023-03-05 16:58:40,395:INFO:create_model() successfully completed......................................
2023-03-05 16:58:40,472:INFO:SubProcess create_model() end ==================================
2023-03-05 16:58:40,473:INFO:DecisionTreeRegressor(random_state=4071) result for R2 is 0.3911
2023-03-05 16:58:40,473:INFO:DecisionTreeRegressor(max_depth=5, max_features=1.0,
                      min_impurity_decrease=0.001, min_samples_leaf=6,
                      min_samples_split=7, random_state=4071) result for R2 is 0.4161
2023-03-05 16:58:40,474:INFO:DecisionTreeRegressor(max_depth=5, max_features=1.0,
                      min_impurity_decrease=0.001, min_samples_leaf=6,
                      min_samples_split=7, random_state=4071) is best model
2023-03-05 16:58:40,474:INFO:choose_better completed
2023-03-05 16:58:40,483:INFO:_master_model_container: 3
2023-03-05 16:58:40,483:INFO:_display_container: 3
2023-03-05 16:58:40,484:INFO:DecisionTreeRegressor(max_depth=5, max_features=1.0,
                      min_impurity_decrease=0.001, min_samples_leaf=6,
                      min_samples_split=7, random_state=4071)
2023-03-05 16:58:40,484:INFO:tune_model() successfully completed......................................
2023-03-05 16:58:40,584:INFO:PyCaret RegressionExperiment
2023-03-05 16:58:40,584:INFO:Logging name: reg-default-name
2023-03-05 16:58:40,584:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-05 16:58:40,584:INFO:version 3.0.0.rc9
2023-03-05 16:58:40,584:INFO:Initializing setup()
2023-03-05 16:58:40,585:INFO:self.USI: d5f4
2023-03-05 16:58:40,585:INFO:self._variable_keys: {'USI', 'data', 'log_plots_param', '_ml_usecase', 'memory', 'transform_target_param', 'html_param', 'X_train', 'pipeline', 'fold_shuffle_param', 'n_jobs_param', 'fold_generator', 'exp_id', 'X_test', 'exp_name_log', 'logging_param', 'y_train', '_available_plots', 'X', 'gpu_param', 'idx', 'gpu_n_jobs_param', 'fold_groups_param', 'target_param', 'y_test', 'seed', 'y'}
2023-03-05 16:58:40,585:INFO:Checking environment
2023-03-05 16:58:40,585:INFO:python_version: 3.9.13
2023-03-05 16:58:40,585:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-03-05 16:58:40,585:INFO:machine: AMD64
2023-03-05 16:58:40,585:INFO:platform: Windows-10-10.0.19044-SP0
2023-03-05 16:58:40,585:INFO:Memory: svmem(total=17009516544, available=4565073920, percent=73.2, used=12444442624, free=4565073920)
2023-03-05 16:58:40,585:INFO:Physical Core: 4
2023-03-05 16:58:40,585:INFO:Logical Core: 8
2023-03-05 16:58:40,585:INFO:Checking libraries
2023-03-05 16:58:40,585:INFO:System:
2023-03-05 16:58:40,585:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-03-05 16:58:40,585:INFO:executable: C:\Users\Alvaro\anaconda3\python.exe
2023-03-05 16:58:40,585:INFO:   machine: Windows-10-10.0.19044-SP0
2023-03-05 16:58:40,585:INFO:PyCaret required dependencies:
2023-03-05 16:58:40,585:INFO:                 pip: 22.2.2
2023-03-05 16:58:40,586:INFO:          setuptools: 63.4.1
2023-03-05 16:58:40,586:INFO:             pycaret: 3.0.0rc9
2023-03-05 16:58:40,586:INFO:             IPython: 7.31.1
2023-03-05 16:58:40,586:INFO:          ipywidgets: 7.6.5
2023-03-05 16:58:40,586:INFO:                tqdm: 4.64.1
2023-03-05 16:58:40,586:INFO:               numpy: 1.21.5
2023-03-05 16:58:40,586:INFO:              pandas: 1.4.4
2023-03-05 16:58:40,586:INFO:              jinja2: 2.11.3
2023-03-05 16:58:40,586:INFO:               scipy: 1.9.1
2023-03-05 16:58:40,586:INFO:              joblib: 1.2.0
2023-03-05 16:58:40,586:INFO:             sklearn: 1.0.2
2023-03-05 16:58:40,586:INFO:                pyod: 1.0.7
2023-03-05 16:58:40,586:INFO:            imblearn: 0.10.1
2023-03-05 16:58:40,586:INFO:   category_encoders: 2.6.0
2023-03-05 16:58:40,586:INFO:            lightgbm: 3.3.5
2023-03-05 16:58:40,586:INFO:               numba: 0.55.1
2023-03-05 16:58:40,586:INFO:            requests: 2.28.1
2023-03-05 16:58:40,586:INFO:          matplotlib: 3.5.2
2023-03-05 16:58:40,586:INFO:          scikitplot: 0.3.7
2023-03-05 16:58:40,586:INFO:         yellowbrick: 1.5
2023-03-05 16:58:40,586:INFO:              plotly: 5.9.0
2023-03-05 16:58:40,586:INFO:             kaleido: 0.2.1
2023-03-05 16:58:40,587:INFO:         statsmodels: 0.13.2
2023-03-05 16:58:40,587:INFO:              sktime: 0.16.1
2023-03-05 16:58:40,587:INFO:               tbats: 1.1.2
2023-03-05 16:58:40,587:INFO:            pmdarima: 2.0.2
2023-03-05 16:58:40,587:INFO:              psutil: 5.9.0
2023-03-05 16:58:40,587:INFO:PyCaret optional dependencies:
2023-03-05 16:58:40,587:INFO:                shap: Not installed
2023-03-05 16:58:40,587:INFO:           interpret: Not installed
2023-03-05 16:58:40,587:INFO:                umap: Not installed
2023-03-05 16:58:40,587:INFO:    pandas_profiling: Not installed
2023-03-05 16:58:40,587:INFO:  explainerdashboard: Not installed
2023-03-05 16:58:40,587:INFO:             autoviz: Not installed
2023-03-05 16:58:40,587:INFO:           fairlearn: Not installed
2023-03-05 16:58:40,587:INFO:             xgboost: 1.7.4
2023-03-05 16:58:40,587:INFO:            catboost: Not installed
2023-03-05 16:58:40,587:INFO:              kmodes: Not installed
2023-03-05 16:58:40,587:INFO:             mlxtend: Not installed
2023-03-05 16:58:40,587:INFO:       statsforecast: Not installed
2023-03-05 16:58:40,587:INFO:        tune_sklearn: Not installed
2023-03-05 16:58:40,588:INFO:                 ray: Not installed
2023-03-05 16:58:40,588:INFO:            hyperopt: Not installed
2023-03-05 16:58:40,588:INFO:              optuna: Not installed
2023-03-05 16:58:40,588:INFO:               skopt: Not installed
2023-03-05 16:58:40,588:INFO:              mlflow: Not installed
2023-03-05 16:58:40,588:INFO:              gradio: Not installed
2023-03-05 16:58:40,588:INFO:             fastapi: Not installed
2023-03-05 16:58:40,588:INFO:             uvicorn: Not installed
2023-03-05 16:58:40,588:INFO:              m2cgen: Not installed
2023-03-05 16:58:40,588:INFO:           evidently: Not installed
2023-03-05 16:58:40,588:INFO:               fugue: Not installed
2023-03-05 16:58:40,588:INFO:           streamlit: Not installed
2023-03-05 16:58:40,588:INFO:             prophet: Not installed
2023-03-05 16:58:40,588:INFO:None
2023-03-05 16:58:40,588:INFO:Set up data.
2023-03-05 16:58:40,595:INFO:Set up train/test split.
2023-03-05 16:58:40,599:INFO:Set up index.
2023-03-05 16:58:40,600:INFO:Set up folding strategy.
2023-03-05 16:58:40,600:INFO:Assigning column types.
2023-03-05 16:58:40,605:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-05 16:58:40,605:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-05 16:58:40,610:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 16:58:40,614:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:58:40,673:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:58:40,721:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:58:40,721:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:58:40,724:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:58:40,724:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-05 16:58:40,729:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 16:58:40,734:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:58:40,790:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:58:40,833:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:58:40,834:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:58:40,837:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:58:40,837:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-03-05 16:58:40,841:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 16:58:40,846:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:58:40,901:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:58:40,942:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:58:40,942:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:58:40,945:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:58:40,950:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 16:58:40,954:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:58:41,007:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:58:41,048:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:58:41,049:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:58:41,052:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:58:41,053:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-03-05 16:58:41,061:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:58:41,122:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:58:41,164:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:58:41,165:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:58:41,167:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:58:41,176:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 16:58:41,230:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:58:41,270:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:58:41,271:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:58:41,273:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:58:41,274:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-03-05 16:58:41,335:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:58:41,375:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:58:41,376:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:58:41,378:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:58:41,440:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:58:41,481:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 16:58:41,482:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:58:41,484:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:58:41,485:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-05 16:58:41,546:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:58:41,586:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:58:41,589:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:58:41,648:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 16:58:41,691:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:58:41,693:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:58:41,694:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-03-05 16:58:41,798:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:58:41,801:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:58:41,904:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:58:41,907:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:58:41,908:INFO:Preparing preprocessing pipeline...
2023-03-05 16:58:41,909:INFO:Set up simple imputation.
2023-03-05 16:58:41,910:INFO:Set up encoding of categorical features.
2023-03-05 16:58:42,007:INFO:Finished creating preprocessing pipeline.
2023-03-05 16:58:42,015:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Alvaro\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['work_year', 'remote_ratio'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['experience_level',
                                             'employment_type', 'job_title',
                                             'employee_residence',
                                             'company_location',
                                             'company_size'],
                                    t...
                                    transformer=OneHotEncoder(cols=['experience_level',
                                                                    'employment_type',
                                                                    'company_size'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['job_title', 'employee_residence',
                                             'company_location'],
                                    transformer=LeaveOneOutEncoder(cols=['job_title',
                                                                         'employee_residence',
                                                                         'company_location'],
                                                                   handle_missing='return_nan',
                                                                   random_state=7311)))])
2023-03-05 16:58:42,015:INFO:Creating final display dataframe.
2023-03-05 16:58:42,444:INFO:Setup _display_container:                     Description             Value
0                    Session id              7311
1                        Target     salary_in_usd
2                   Target type        Regression
3           Original data shape          (500, 9)
4        Transformed data shape         (500, 17)
5   Transformed train set shape         (350, 17)
6    Transformed test set shape         (150, 17)
7              Numeric features                 2
8          Categorical features                 6
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator             KFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  reg-default-name
21                          USI              d5f4
2023-03-05 16:58:42,563:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:58:42,566:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:58:42,672:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 16:58:42,674:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 16:58:42,675:INFO:setup() successfully completed in 2.1s...............
2023-03-05 16:58:42,675:INFO:Initializing create_model()
2023-03-05 16:58:42,675:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537D200790>, estimator=dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-05 16:58:42,676:INFO:Checking exceptions
2023-03-05 16:58:42,700:INFO:Importing libraries
2023-03-05 16:58:42,700:INFO:Copying training dataset
2023-03-05 16:58:42,709:INFO:Defining folds
2023-03-05 16:58:42,710:INFO:Declaring metric variables
2023-03-05 16:58:42,715:INFO:Importing untrained model
2023-03-05 16:58:42,719:INFO:Decision Tree Regressor Imported successfully
2023-03-05 16:58:42,728:INFO:Starting cross validation
2023-03-05 16:58:42,730:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 16:58:43,684:INFO:Calculating mean and std
2023-03-05 16:58:43,686:INFO:Creating metrics dataframe
2023-03-05 16:58:43,692:INFO:Finalizing model
2023-03-05 16:58:43,850:INFO:Uploading results into container
2023-03-05 16:58:43,851:INFO:Uploading model into container now
2023-03-05 16:58:43,863:INFO:_master_model_container: 1
2023-03-05 16:58:43,863:INFO:_display_container: 2
2023-03-05 16:58:43,863:INFO:DecisionTreeRegressor(random_state=7311)
2023-03-05 16:58:43,864:INFO:create_model() successfully completed......................................
2023-03-05 16:58:43,952:INFO:Initializing tune_model()
2023-03-05 16:58:43,952:INFO:tune_model(estimator=DecisionTreeRegressor(random_state=7311), fold=None, round=4, n_iter=500, custom_grid=None, optimize=R2, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537D200790>)
2023-03-05 16:58:43,952:INFO:Checking exceptions
2023-03-05 16:58:43,983:INFO:Copying training dataset
2023-03-05 16:58:43,987:INFO:Checking base model
2023-03-05 16:58:43,987:INFO:Base model : Decision Tree Regressor
2023-03-05 16:58:43,996:INFO:Declaring metric variables
2023-03-05 16:58:44,000:INFO:Defining Hyperparameters
2023-03-05 16:58:44,100:INFO:Tuning with n_jobs=-1
2023-03-05 16:58:44,100:INFO:Initializing RandomizedSearchCV
2023-03-05 17:04:01,503:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:04:01,670:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:04:01,873:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:04:02,074:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:04:02,668:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:04:45,167:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:04:45,235:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:04:47,452:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:04:47,499:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:04:47,578:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:04:48,059:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:04:48,124:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:04:48,640:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:230: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-05 17:04:49,469:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:223: UserWarning: Persisting input arguments took 1.34s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-05 17:04:49,498:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 1.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:04:49,592:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 1.31s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:04:49,626:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 1.21s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:04:49,813:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:230: UserWarning: Persisting input arguments took 1.06s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-05 17:04:49,851:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 0.95s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:04:49,992:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:04:50,070:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-03-05 17:04:50,807:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:04:51,299:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:04:51,534:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-03-05 17:04:51,687:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:04:51,816:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:230: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-05 17:04:51,851:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:223: UserWarning: Persisting input arguments took 0.94s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-05 17:04:52,174:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:04:52,226:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:04:52,276:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:06:44,467:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:06:49,538:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:06:49,555:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:06:58,846:INFO:best_params: {'actual_estimator__min_samples_split': 7, 'actual_estimator__min_samples_leaf': 2, 'actual_estimator__min_impurity_decrease': 0.02, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 11, 'actual_estimator__criterion': 'absolute_error'}
2023-03-05 17:06:58,850:INFO:Hyperparameter search completed
2023-03-05 17:06:58,851:INFO:SubProcess create_model() called ==================================
2023-03-05 17:06:58,852:INFO:Initializing create_model()
2023-03-05 17:06:58,852:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537D200790>, estimator=DecisionTreeRegressor(random_state=7311), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CC5CA60>, model_only=True, return_train_score=False, kwargs={'min_samples_split': 7, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.02, 'max_features': 'sqrt', 'max_depth': 11, 'criterion': 'absolute_error'})
2023-03-05 17:06:58,853:INFO:Checking exceptions
2023-03-05 17:06:58,854:INFO:Importing libraries
2023-03-05 17:06:58,854:INFO:Copying training dataset
2023-03-05 17:06:58,865:INFO:Defining folds
2023-03-05 17:06:58,866:INFO:Declaring metric variables
2023-03-05 17:06:58,873:INFO:Importing untrained model
2023-03-05 17:06:58,874:INFO:Declaring custom model
2023-03-05 17:06:58,885:INFO:Decision Tree Regressor Imported successfully
2023-03-05 17:06:58,902:INFO:Starting cross validation
2023-03-05 17:06:58,906:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:07:00,337:INFO:Calculating mean and std
2023-03-05 17:07:00,340:INFO:Creating metrics dataframe
2023-03-05 17:07:00,354:INFO:Finalizing model
2023-03-05 17:07:00,692:INFO:Uploading results into container
2023-03-05 17:07:00,694:INFO:Uploading model into container now
2023-03-05 17:07:00,695:INFO:_master_model_container: 2
2023-03-05 17:07:00,696:INFO:_display_container: 3
2023-03-05 17:07:00,699:INFO:DecisionTreeRegressor(criterion='absolute_error', max_depth=11,
                      max_features='sqrt', min_impurity_decrease=0.02,
                      min_samples_leaf=2, min_samples_split=7,
                      random_state=7311)
2023-03-05 17:07:00,699:INFO:create_model() successfully completed......................................
2023-03-05 17:07:00,892:INFO:SubProcess create_model() end ==================================
2023-03-05 17:07:00,892:INFO:choose_better activated
2023-03-05 17:07:00,901:INFO:SubProcess create_model() called ==================================
2023-03-05 17:07:00,902:INFO:Initializing create_model()
2023-03-05 17:07:00,903:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537D200790>, estimator=DecisionTreeRegressor(random_state=7311), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:07:00,903:INFO:Checking exceptions
2023-03-05 17:07:00,908:INFO:Importing libraries
2023-03-05 17:07:00,908:INFO:Copying training dataset
2023-03-05 17:07:00,919:INFO:Defining folds
2023-03-05 17:07:00,919:INFO:Declaring metric variables
2023-03-05 17:07:00,919:INFO:Importing untrained model
2023-03-05 17:07:00,920:INFO:Declaring custom model
2023-03-05 17:07:00,921:INFO:Decision Tree Regressor Imported successfully
2023-03-05 17:07:00,922:INFO:Starting cross validation
2023-03-05 17:07:00,924:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:07:02,368:INFO:Calculating mean and std
2023-03-05 17:07:02,369:INFO:Creating metrics dataframe
2023-03-05 17:07:02,374:INFO:Finalizing model
2023-03-05 17:07:02,697:INFO:Uploading results into container
2023-03-05 17:07:02,698:INFO:Uploading model into container now
2023-03-05 17:07:02,699:INFO:_master_model_container: 3
2023-03-05 17:07:02,699:INFO:_display_container: 4
2023-03-05 17:07:02,700:INFO:DecisionTreeRegressor(random_state=7311)
2023-03-05 17:07:02,700:INFO:create_model() successfully completed......................................
2023-03-05 17:07:02,872:INFO:SubProcess create_model() end ==================================
2023-03-05 17:07:02,874:INFO:DecisionTreeRegressor(random_state=7311) result for R2 is 0.3371
2023-03-05 17:07:02,875:INFO:DecisionTreeRegressor(criterion='absolute_error', max_depth=11,
                      max_features='sqrt', min_impurity_decrease=0.02,
                      min_samples_leaf=2, min_samples_split=7,
                      random_state=7311) result for R2 is 0.3867
2023-03-05 17:07:02,877:INFO:DecisionTreeRegressor(criterion='absolute_error', max_depth=11,
                      max_features='sqrt', min_impurity_decrease=0.02,
                      min_samples_leaf=2, min_samples_split=7,
                      random_state=7311) is best model
2023-03-05 17:07:02,877:INFO:choose_better completed
2023-03-05 17:07:02,901:INFO:_master_model_container: 3
2023-03-05 17:07:02,901:INFO:_display_container: 3
2023-03-05 17:07:02,903:INFO:DecisionTreeRegressor(criterion='absolute_error', max_depth=11,
                      max_features='sqrt', min_impurity_decrease=0.02,
                      min_samples_leaf=2, min_samples_split=7,
                      random_state=7311)
2023-03-05 17:07:02,903:INFO:tune_model() successfully completed......................................
2023-03-05 17:07:03,092:INFO:PyCaret RegressionExperiment
2023-03-05 17:07:03,093:INFO:Logging name: reg-default-name
2023-03-05 17:07:03,093:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-05 17:07:03,093:INFO:version 3.0.0.rc9
2023-03-05 17:07:03,093:INFO:Initializing setup()
2023-03-05 17:07:03,094:INFO:self.USI: 36e8
2023-03-05 17:07:03,094:INFO:self._variable_keys: {'USI', 'data', 'log_plots_param', '_ml_usecase', 'memory', 'transform_target_param', 'html_param', 'X_train', 'pipeline', 'fold_shuffle_param', 'n_jobs_param', 'fold_generator', 'exp_id', 'X_test', 'exp_name_log', 'logging_param', 'y_train', '_available_plots', 'X', 'gpu_param', 'idx', 'gpu_n_jobs_param', 'fold_groups_param', 'target_param', 'y_test', 'seed', 'y'}
2023-03-05 17:07:03,094:INFO:Checking environment
2023-03-05 17:07:03,094:INFO:python_version: 3.9.13
2023-03-05 17:07:03,094:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-03-05 17:07:03,094:INFO:machine: AMD64
2023-03-05 17:07:03,094:INFO:platform: Windows-10-10.0.19044-SP0
2023-03-05 17:07:03,094:INFO:Memory: svmem(total=17009516544, available=4415012864, percent=74.0, used=12594503680, free=4415012864)
2023-03-05 17:07:03,095:INFO:Physical Core: 4
2023-03-05 17:07:03,095:INFO:Logical Core: 8
2023-03-05 17:07:03,095:INFO:Checking libraries
2023-03-05 17:07:03,095:INFO:System:
2023-03-05 17:07:03,096:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-03-05 17:07:03,096:INFO:executable: C:\Users\Alvaro\anaconda3\python.exe
2023-03-05 17:07:03,096:INFO:   machine: Windows-10-10.0.19044-SP0
2023-03-05 17:07:03,096:INFO:PyCaret required dependencies:
2023-03-05 17:07:03,096:INFO:                 pip: 22.2.2
2023-03-05 17:07:03,096:INFO:          setuptools: 63.4.1
2023-03-05 17:07:03,096:INFO:             pycaret: 3.0.0rc9
2023-03-05 17:07:03,096:INFO:             IPython: 7.31.1
2023-03-05 17:07:03,096:INFO:          ipywidgets: 7.6.5
2023-03-05 17:07:03,096:INFO:                tqdm: 4.64.1
2023-03-05 17:07:03,096:INFO:               numpy: 1.21.5
2023-03-05 17:07:03,097:INFO:              pandas: 1.4.4
2023-03-05 17:07:03,097:INFO:              jinja2: 2.11.3
2023-03-05 17:07:03,097:INFO:               scipy: 1.9.1
2023-03-05 17:07:03,097:INFO:              joblib: 1.2.0
2023-03-05 17:07:03,097:INFO:             sklearn: 1.0.2
2023-03-05 17:07:03,097:INFO:                pyod: 1.0.7
2023-03-05 17:07:03,097:INFO:            imblearn: 0.10.1
2023-03-05 17:07:03,098:INFO:   category_encoders: 2.6.0
2023-03-05 17:07:03,098:INFO:            lightgbm: 3.3.5
2023-03-05 17:07:03,098:INFO:               numba: 0.55.1
2023-03-05 17:07:03,098:INFO:            requests: 2.28.1
2023-03-05 17:07:03,098:INFO:          matplotlib: 3.5.2
2023-03-05 17:07:03,098:INFO:          scikitplot: 0.3.7
2023-03-05 17:07:03,098:INFO:         yellowbrick: 1.5
2023-03-05 17:07:03,098:INFO:              plotly: 5.9.0
2023-03-05 17:07:03,098:INFO:             kaleido: 0.2.1
2023-03-05 17:07:03,099:INFO:         statsmodels: 0.13.2
2023-03-05 17:07:03,100:INFO:              sktime: 0.16.1
2023-03-05 17:07:03,100:INFO:               tbats: 1.1.2
2023-03-05 17:07:03,100:INFO:            pmdarima: 2.0.2
2023-03-05 17:07:03,100:INFO:              psutil: 5.9.0
2023-03-05 17:07:03,100:INFO:PyCaret optional dependencies:
2023-03-05 17:07:03,100:INFO:                shap: Not installed
2023-03-05 17:07:03,101:INFO:           interpret: Not installed
2023-03-05 17:07:03,101:INFO:                umap: Not installed
2023-03-05 17:07:03,101:INFO:    pandas_profiling: Not installed
2023-03-05 17:07:03,101:INFO:  explainerdashboard: Not installed
2023-03-05 17:07:03,102:INFO:             autoviz: Not installed
2023-03-05 17:07:03,102:INFO:           fairlearn: Not installed
2023-03-05 17:07:03,102:INFO:             xgboost: 1.7.4
2023-03-05 17:07:03,102:INFO:            catboost: Not installed
2023-03-05 17:07:03,102:INFO:              kmodes: Not installed
2023-03-05 17:07:03,102:INFO:             mlxtend: Not installed
2023-03-05 17:07:03,102:INFO:       statsforecast: Not installed
2023-03-05 17:07:03,103:INFO:        tune_sklearn: Not installed
2023-03-05 17:07:03,103:INFO:                 ray: Not installed
2023-03-05 17:07:03,103:INFO:            hyperopt: Not installed
2023-03-05 17:07:03,103:INFO:              optuna: Not installed
2023-03-05 17:07:03,103:INFO:               skopt: Not installed
2023-03-05 17:07:03,103:INFO:              mlflow: Not installed
2023-03-05 17:07:03,103:INFO:              gradio: Not installed
2023-03-05 17:07:03,103:INFO:             fastapi: Not installed
2023-03-05 17:07:03,104:INFO:             uvicorn: Not installed
2023-03-05 17:07:03,104:INFO:              m2cgen: Not installed
2023-03-05 17:07:03,104:INFO:           evidently: Not installed
2023-03-05 17:07:03,104:INFO:               fugue: Not installed
2023-03-05 17:07:03,104:INFO:           streamlit: Not installed
2023-03-05 17:07:03,105:INFO:             prophet: Not installed
2023-03-05 17:07:03,105:INFO:None
2023-03-05 17:07:03,105:INFO:Set up data.
2023-03-05 17:07:03,122:INFO:Set up train/test split.
2023-03-05 17:07:03,132:INFO:Set up index.
2023-03-05 17:07:03,133:INFO:Set up folding strategy.
2023-03-05 17:07:03,133:INFO:Assigning column types.
2023-03-05 17:07:03,140:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-05 17:07:03,141:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-05 17:07:03,153:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 17:07:03,164:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:07:03,314:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:07:03,427:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:07:03,430:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:07:03,438:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:07:03,438:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-05 17:07:03,450:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 17:07:03,471:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:07:03,627:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:07:03,758:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:07:03,760:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:07:03,767:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:07:03,768:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-03-05 17:07:03,780:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 17:07:03,792:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:07:03,944:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:07:04,077:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:07:04,078:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:07:04,088:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:07:04,106:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 17:07:04,119:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:07:04,300:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:07:04,413:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:07:04,415:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:07:04,422:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:07:04,422:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-03-05 17:07:04,446:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:07:04,600:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:07:04,713:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:07:04,714:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:07:04,721:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:07:04,745:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:07:04,921:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:07:05,027:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:07:05,028:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:07:05,034:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:07:05,035:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-03-05 17:07:05,202:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:07:05,314:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:07:05,315:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:07:05,322:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:07:05,493:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:07:05,621:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:07:05,623:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:07:05,630:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:07:05,631:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-05 17:07:05,819:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:07:05,918:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:07:05,925:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:07:06,105:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:07:06,230:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:07:06,237:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:07:06,238:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-03-05 17:07:06,520:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:07:06,526:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:07:06,806:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:07:06,812:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:07:06,815:INFO:Preparing preprocessing pipeline...
2023-03-05 17:07:06,817:INFO:Set up simple imputation.
2023-03-05 17:07:06,823:INFO:Set up encoding of categorical features.
2023-03-05 17:07:07,092:INFO:Finished creating preprocessing pipeline.
2023-03-05 17:07:07,115:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Alvaro\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['work_year', 'remote_ratio'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['experience_level',
                                             'employment_type', 'job_title',
                                             'employee_residence',
                                             'company_location',
                                             'company_size'],
                                    t...
                                    transformer=OneHotEncoder(cols=['experience_level',
                                                                    'employment_type',
                                                                    'company_size'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['job_title', 'employee_residence',
                                             'company_location'],
                                    transformer=LeaveOneOutEncoder(cols=['job_title',
                                                                         'employee_residence',
                                                                         'company_location'],
                                                                   handle_missing='return_nan',
                                                                   random_state=4132)))])
2023-03-05 17:07:07,115:INFO:Creating final display dataframe.
2023-03-05 17:07:08,061:INFO:Setup _display_container:                     Description             Value
0                    Session id              4132
1                        Target     salary_in_usd
2                   Target type        Regression
3           Original data shape          (500, 9)
4        Transformed data shape         (500, 17)
5   Transformed train set shape         (350, 17)
6    Transformed test set shape         (150, 17)
7              Numeric features                 2
8          Categorical features                 6
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator             KFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  reg-default-name
21                          USI              36e8
2023-03-05 17:07:08,360:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:07:08,367:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:07:08,662:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:07:08,668:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:07:08,670:INFO:setup() successfully completed in 5.6s...............
2023-03-05 17:07:08,670:INFO:Initializing create_model()
2023-03-05 17:07:08,671:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CF251C0>, estimator=dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:07:08,671:INFO:Checking exceptions
2023-03-05 17:07:08,736:INFO:Importing libraries
2023-03-05 17:07:08,736:INFO:Copying training dataset
2023-03-05 17:07:08,751:INFO:Defining folds
2023-03-05 17:07:08,752:INFO:Declaring metric variables
2023-03-05 17:07:08,766:INFO:Importing untrained model
2023-03-05 17:07:08,779:INFO:Decision Tree Regressor Imported successfully
2023-03-05 17:07:08,795:INFO:Starting cross validation
2023-03-05 17:07:08,801:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:07:10,643:INFO:Calculating mean and std
2023-03-05 17:07:10,647:INFO:Creating metrics dataframe
2023-03-05 17:07:10,662:INFO:Finalizing model
2023-03-05 17:07:10,971:INFO:Uploading results into container
2023-03-05 17:07:10,973:INFO:Uploading model into container now
2023-03-05 17:07:10,998:INFO:_master_model_container: 1
2023-03-05 17:07:10,999:INFO:_display_container: 2
2023-03-05 17:07:11,001:INFO:DecisionTreeRegressor(random_state=4132)
2023-03-05 17:07:11,001:INFO:create_model() successfully completed......................................
2023-03-05 17:07:11,147:INFO:Initializing tune_model()
2023-03-05 17:07:11,147:INFO:tune_model(estimator=DecisionTreeRegressor(random_state=4132), fold=None, round=4, n_iter=500, custom_grid=None, optimize=RMSE, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CF251C0>)
2023-03-05 17:07:11,147:INFO:Checking exceptions
2023-03-05 17:07:11,211:INFO:Copying training dataset
2023-03-05 17:07:11,219:INFO:Checking base model
2023-03-05 17:07:11,220:INFO:Base model : Decision Tree Regressor
2023-03-05 17:07:11,230:INFO:Declaring metric variables
2023-03-05 17:07:11,239:INFO:Defining Hyperparameters
2023-03-05 17:07:11,398:INFO:Tuning with n_jobs=-1
2023-03-05 17:07:11,399:INFO:Initializing RandomizedSearchCV
2023-03-05 17:10:28,386:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:13:23,189:INFO:PyCaret RegressionExperiment
2023-03-05 17:13:23,189:INFO:Logging name: reg-default-name
2023-03-05 17:13:23,189:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-05 17:13:23,189:INFO:version 3.0.0.rc9
2023-03-05 17:13:23,189:INFO:Initializing setup()
2023-03-05 17:13:23,189:INFO:self.USI: d769
2023-03-05 17:13:23,189:INFO:self._variable_keys: {'USI', 'data', 'log_plots_param', '_ml_usecase', 'memory', 'transform_target_param', 'html_param', 'X_train', 'pipeline', 'fold_shuffle_param', 'n_jobs_param', 'fold_generator', 'exp_id', 'X_test', 'exp_name_log', 'logging_param', 'y_train', '_available_plots', 'X', 'gpu_param', 'idx', 'gpu_n_jobs_param', 'fold_groups_param', 'target_param', 'y_test', 'seed', 'y'}
2023-03-05 17:13:23,190:INFO:Checking environment
2023-03-05 17:13:23,190:INFO:python_version: 3.9.13
2023-03-05 17:13:23,190:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-03-05 17:13:23,190:INFO:machine: AMD64
2023-03-05 17:13:23,190:INFO:platform: Windows-10-10.0.19044-SP0
2023-03-05 17:13:23,190:INFO:Memory: svmem(total=17009516544, available=6122864640, percent=64.0, used=10886651904, free=6122864640)
2023-03-05 17:13:23,190:INFO:Physical Core: 4
2023-03-05 17:13:23,190:INFO:Logical Core: 8
2023-03-05 17:13:23,190:INFO:Checking libraries
2023-03-05 17:13:23,191:INFO:System:
2023-03-05 17:13:23,191:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-03-05 17:13:23,191:INFO:executable: C:\Users\Alvaro\anaconda3\python.exe
2023-03-05 17:13:23,191:INFO:   machine: Windows-10-10.0.19044-SP0
2023-03-05 17:13:23,191:INFO:PyCaret required dependencies:
2023-03-05 17:13:23,191:INFO:                 pip: 22.2.2
2023-03-05 17:13:23,191:INFO:          setuptools: 63.4.1
2023-03-05 17:13:23,191:INFO:             pycaret: 3.0.0rc9
2023-03-05 17:13:23,191:INFO:             IPython: 7.31.1
2023-03-05 17:13:23,191:INFO:          ipywidgets: 7.6.5
2023-03-05 17:13:23,192:INFO:                tqdm: 4.64.1
2023-03-05 17:13:23,192:INFO:               numpy: 1.21.5
2023-03-05 17:13:23,192:INFO:              pandas: 1.4.4
2023-03-05 17:13:23,192:INFO:              jinja2: 2.11.3
2023-03-05 17:13:23,192:INFO:               scipy: 1.9.1
2023-03-05 17:13:23,192:INFO:              joblib: 1.2.0
2023-03-05 17:13:23,192:INFO:             sklearn: 1.0.2
2023-03-05 17:13:23,192:INFO:                pyod: 1.0.7
2023-03-05 17:13:23,192:INFO:            imblearn: 0.10.1
2023-03-05 17:13:23,192:INFO:   category_encoders: 2.6.0
2023-03-05 17:13:23,192:INFO:            lightgbm: 3.3.5
2023-03-05 17:13:23,193:INFO:               numba: 0.55.1
2023-03-05 17:13:23,193:INFO:            requests: 2.28.1
2023-03-05 17:13:23,193:INFO:          matplotlib: 3.5.2
2023-03-05 17:13:23,193:INFO:          scikitplot: 0.3.7
2023-03-05 17:13:23,193:INFO:         yellowbrick: 1.5
2023-03-05 17:13:23,193:INFO:              plotly: 5.9.0
2023-03-05 17:13:23,193:INFO:             kaleido: 0.2.1
2023-03-05 17:13:23,194:INFO:         statsmodels: 0.13.2
2023-03-05 17:13:23,194:INFO:              sktime: 0.16.1
2023-03-05 17:13:23,194:INFO:               tbats: 1.1.2
2023-03-05 17:13:23,194:INFO:            pmdarima: 2.0.2
2023-03-05 17:13:23,194:INFO:              psutil: 5.9.0
2023-03-05 17:13:23,194:INFO:PyCaret optional dependencies:
2023-03-05 17:13:23,194:INFO:                shap: Not installed
2023-03-05 17:13:23,194:INFO:           interpret: Not installed
2023-03-05 17:13:23,195:INFO:                umap: Not installed
2023-03-05 17:13:23,195:INFO:    pandas_profiling: Not installed
2023-03-05 17:13:23,195:INFO:  explainerdashboard: Not installed
2023-03-05 17:13:23,195:INFO:             autoviz: Not installed
2023-03-05 17:13:23,195:INFO:           fairlearn: Not installed
2023-03-05 17:13:23,195:INFO:             xgboost: 1.7.4
2023-03-05 17:13:23,196:INFO:            catboost: Not installed
2023-03-05 17:13:23,196:INFO:              kmodes: Not installed
2023-03-05 17:13:23,196:INFO:             mlxtend: Not installed
2023-03-05 17:13:23,196:INFO:       statsforecast: Not installed
2023-03-05 17:13:23,196:INFO:        tune_sklearn: Not installed
2023-03-05 17:13:23,196:INFO:                 ray: Not installed
2023-03-05 17:13:23,196:INFO:            hyperopt: Not installed
2023-03-05 17:13:23,196:INFO:              optuna: Not installed
2023-03-05 17:13:23,197:INFO:               skopt: Not installed
2023-03-05 17:13:23,197:INFO:              mlflow: Not installed
2023-03-05 17:13:23,197:INFO:              gradio: Not installed
2023-03-05 17:13:23,197:INFO:             fastapi: Not installed
2023-03-05 17:13:23,197:INFO:             uvicorn: Not installed
2023-03-05 17:13:23,197:INFO:              m2cgen: Not installed
2023-03-05 17:13:23,197:INFO:           evidently: Not installed
2023-03-05 17:13:23,197:INFO:               fugue: Not installed
2023-03-05 17:13:23,198:INFO:           streamlit: Not installed
2023-03-05 17:13:23,198:INFO:             prophet: Not installed
2023-03-05 17:13:23,198:INFO:None
2023-03-05 17:13:23,198:INFO:Set up data.
2023-03-05 17:13:23,215:INFO:Set up train/test split.
2023-03-05 17:13:23,227:INFO:Set up index.
2023-03-05 17:13:23,228:INFO:Set up folding strategy.
2023-03-05 17:13:23,228:INFO:Assigning column types.
2023-03-05 17:13:23,236:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-05 17:13:23,237:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-05 17:13:23,252:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 17:13:23,264:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:13:23,402:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:13:23,531:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:13:23,532:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:13:23,539:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:13:23,540:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-05 17:13:23,551:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 17:13:23,563:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:13:23,739:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:13:23,860:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:13:23,862:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:13:23,872:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:13:23,872:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-03-05 17:13:23,889:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 17:13:23,905:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:13:24,063:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:13:24,193:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:13:24,194:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:13:24,202:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:13:24,215:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 17:13:24,229:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:13:24,402:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:13:24,516:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:13:24,518:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:13:24,525:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:13:24,525:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-03-05 17:13:24,549:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:13:24,695:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:13:24,806:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:13:24,808:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:13:24,814:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:13:24,840:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:13:24,990:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:13:25,103:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:13:25,105:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:13:25,111:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:13:25,112:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-03-05 17:13:25,280:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:13:25,391:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:13:25,392:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:13:25,399:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:13:25,578:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:13:25,691:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:13:25,692:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:13:25,698:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:13:25,699:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-05 17:13:25,877:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:13:25,995:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:13:26,004:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:13:26,169:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:13:26,288:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:13:26,294:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:13:26,295:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-03-05 17:13:26,586:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:13:26,594:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:13:26,873:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:13:26,880:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:13:26,884:INFO:Preparing preprocessing pipeline...
2023-03-05 17:13:26,885:INFO:Set up simple imputation.
2023-03-05 17:13:26,891:INFO:Set up encoding of categorical features.
2023-03-05 17:13:27,143:INFO:Finished creating preprocessing pipeline.
2023-03-05 17:13:27,167:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Alvaro\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['work_year', 'remote_ratio'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['experience_level',
                                             'employment_type', 'job_title',
                                             'employee_residence',
                                             'company_location',
                                             'company_size'],
                                    t...
                                    transformer=OneHotEncoder(cols=['experience_level',
                                                                    'employment_type',
                                                                    'company_size'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['job_title', 'employee_residence',
                                             'company_location'],
                                    transformer=LeaveOneOutEncoder(cols=['job_title',
                                                                         'employee_residence',
                                                                         'company_location'],
                                                                   handle_missing='return_nan',
                                                                   random_state=7152)))])
2023-03-05 17:13:27,167:INFO:Creating final display dataframe.
2023-03-05 17:13:28,137:INFO:Setup _display_container:                     Description             Value
0                    Session id              7152
1                        Target     salary_in_usd
2                   Target type        Regression
3           Original data shape          (500, 9)
4        Transformed data shape         (500, 17)
5   Transformed train set shape         (350, 17)
6    Transformed test set shape         (150, 17)
7              Numeric features                 2
8          Categorical features                 6
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator             KFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  reg-default-name
21                          USI              d769
2023-03-05 17:13:28,466:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:13:28,473:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:13:28,803:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:13:28,809:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:13:28,811:INFO:setup() successfully completed in 5.62s...............
2023-03-05 17:13:28,811:INFO:Initializing create_model()
2023-03-05 17:13:28,811:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000153716115B0>, estimator=dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:13:28,811:INFO:Checking exceptions
2023-03-05 17:13:28,877:INFO:Importing libraries
2023-03-05 17:13:28,878:INFO:Copying training dataset
2023-03-05 17:13:28,896:INFO:Defining folds
2023-03-05 17:13:28,896:INFO:Declaring metric variables
2023-03-05 17:13:28,910:INFO:Importing untrained model
2023-03-05 17:13:28,923:INFO:Decision Tree Regressor Imported successfully
2023-03-05 17:13:28,943:INFO:Starting cross validation
2023-03-05 17:13:28,947:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:13:43,755:INFO:Calculating mean and std
2023-03-05 17:13:43,759:INFO:Creating metrics dataframe
2023-03-05 17:13:43,773:INFO:Finalizing model
2023-03-05 17:13:44,131:INFO:Uploading results into container
2023-03-05 17:13:44,133:INFO:Uploading model into container now
2023-03-05 17:13:44,165:INFO:_master_model_container: 1
2023-03-05 17:13:44,165:INFO:_display_container: 2
2023-03-05 17:13:44,166:INFO:DecisionTreeRegressor(random_state=7152)
2023-03-05 17:13:44,167:INFO:create_model() successfully completed......................................
2023-03-05 17:13:44,334:INFO:Initializing tune_model()
2023-03-05 17:13:44,334:INFO:tune_model(estimator=DecisionTreeRegressor(random_state=7152), fold=None, round=4, n_iter=500, custom_grid=None, optimize=RMSE, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.regression.oop.RegressionExperiment object at 0x00000153716115B0>)
2023-03-05 17:13:44,334:INFO:Checking exceptions
2023-03-05 17:13:44,413:INFO:Copying training dataset
2023-03-05 17:13:44,425:INFO:Checking base model
2023-03-05 17:13:44,426:INFO:Base model : Decision Tree Regressor
2023-03-05 17:13:44,438:INFO:Declaring metric variables
2023-03-05 17:13:44,448:INFO:Defining Hyperparameters
2023-03-05 17:13:44,597:INFO:Tuning with n_jobs=-1
2023-03-05 17:13:44,597:INFO:Initializing RandomizedSearchCV
2023-03-05 17:16:29,303:INFO:Initializing compare_models()
2023-03-05 17:16:29,304:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000153716115B0>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x00000153716115B0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-03-05 17:16:29,304:INFO:Checking exceptions
2023-03-05 17:16:29,309:INFO:Preparing display monitor
2023-03-05 17:16:29,389:INFO:Initializing Linear Regression
2023-03-05 17:16:29,389:INFO:Total runtime is 0.0 minutes
2023-03-05 17:16:29,401:INFO:SubProcess create_model() called ==================================
2023-03-05 17:16:29,402:INFO:Initializing create_model()
2023-03-05 17:16:29,404:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000153716115B0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CEC5250>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:16:29,404:INFO:Checking exceptions
2023-03-05 17:16:29,407:INFO:Importing libraries
2023-03-05 17:16:29,407:INFO:Copying training dataset
2023-03-05 17:16:29,423:INFO:Defining folds
2023-03-05 17:16:29,424:INFO:Declaring metric variables
2023-03-05 17:16:29,434:INFO:Importing untrained model
2023-03-05 17:16:29,446:INFO:Linear Regression Imported successfully
2023-03-05 17:16:29,470:INFO:Starting cross validation
2023-03-05 17:16:29,475:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:16:46,141:INFO:Calculating mean and std
2023-03-05 17:16:46,146:INFO:Creating metrics dataframe
2023-03-05 17:16:46,155:INFO:Uploading results into container
2023-03-05 17:16:46,156:INFO:Uploading model into container now
2023-03-05 17:16:46,157:INFO:_master_model_container: 2
2023-03-05 17:16:46,157:INFO:_display_container: 3
2023-03-05 17:16:46,158:INFO:LinearRegression(n_jobs=-1)
2023-03-05 17:16:46,158:INFO:create_model() successfully completed......................................
2023-03-05 17:16:46,350:INFO:SubProcess create_model() end ==================================
2023-03-05 17:16:46,351:INFO:Creating metrics dataframe
2023-03-05 17:16:46,378:INFO:Initializing Lasso Regression
2023-03-05 17:16:46,379:INFO:Total runtime is 0.2831683913866679 minutes
2023-03-05 17:16:46,392:INFO:SubProcess create_model() called ==================================
2023-03-05 17:16:46,393:INFO:Initializing create_model()
2023-03-05 17:16:46,393:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000153716115B0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CEC5250>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:16:46,393:INFO:Checking exceptions
2023-03-05 17:16:46,394:INFO:Importing libraries
2023-03-05 17:16:46,394:INFO:Copying training dataset
2023-03-05 17:16:46,413:INFO:Defining folds
2023-03-05 17:16:46,414:INFO:Declaring metric variables
2023-03-05 17:16:46,426:INFO:Importing untrained model
2023-03-05 17:16:46,439:INFO:Lasso Regression Imported successfully
2023-03-05 17:16:46,456:INFO:Starting cross validation
2023-03-05 17:16:46,460:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:16:46,977:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.870e+09, tolerance: 1.575e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-05 17:16:47,071:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.663e+09, tolerance: 1.485e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-05 17:16:47,114:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.115e+09, tolerance: 1.548e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-05 17:16:47,190:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.123e+09, tolerance: 1.615e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-05 17:16:47,245:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.270e+09, tolerance: 1.407e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-05 17:16:47,261:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.175e+09, tolerance: 1.462e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-05 17:16:47,288:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.845e+09, tolerance: 1.615e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-05 17:16:47,292:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.567e+09, tolerance: 1.615e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-05 17:16:47,893:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.862e+09, tolerance: 1.580e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-05 17:16:48,056:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.342e+09, tolerance: 1.573e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-05 17:16:48,521:INFO:Calculating mean and std
2023-03-05 17:16:48,524:INFO:Creating metrics dataframe
2023-03-05 17:16:48,535:INFO:Uploading results into container
2023-03-05 17:16:48,536:INFO:Uploading model into container now
2023-03-05 17:16:48,537:INFO:_master_model_container: 3
2023-03-05 17:16:48,537:INFO:_display_container: 3
2023-03-05 17:16:48,539:INFO:Lasso(random_state=7152)
2023-03-05 17:16:48,539:INFO:create_model() successfully completed......................................
2023-03-05 17:16:48,694:INFO:SubProcess create_model() end ==================================
2023-03-05 17:16:48,694:INFO:Creating metrics dataframe
2023-03-05 17:16:48,727:INFO:Initializing Ridge Regression
2023-03-05 17:16:48,727:INFO:Total runtime is 0.322303307056427 minutes
2023-03-05 17:16:48,737:INFO:SubProcess create_model() called ==================================
2023-03-05 17:16:48,738:INFO:Initializing create_model()
2023-03-05 17:16:48,738:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000153716115B0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CEC5250>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:16:48,739:INFO:Checking exceptions
2023-03-05 17:16:48,740:INFO:Importing libraries
2023-03-05 17:16:48,740:INFO:Copying training dataset
2023-03-05 17:16:48,759:INFO:Defining folds
2023-03-05 17:16:48,760:INFO:Declaring metric variables
2023-03-05 17:16:48,772:INFO:Importing untrained model
2023-03-05 17:16:48,784:INFO:Ridge Regression Imported successfully
2023-03-05 17:16:48,804:INFO:Starting cross validation
2023-03-05 17:16:48,809:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:16:50,725:INFO:Calculating mean and std
2023-03-05 17:16:50,731:INFO:Creating metrics dataframe
2023-03-05 17:16:50,742:INFO:Uploading results into container
2023-03-05 17:16:50,744:INFO:Uploading model into container now
2023-03-05 17:16:50,745:INFO:_master_model_container: 4
2023-03-05 17:16:50,745:INFO:_display_container: 3
2023-03-05 17:16:50,746:INFO:Ridge(random_state=7152)
2023-03-05 17:16:50,746:INFO:create_model() successfully completed......................................
2023-03-05 17:16:50,914:INFO:SubProcess create_model() end ==================================
2023-03-05 17:16:50,915:INFO:Creating metrics dataframe
2023-03-05 17:16:50,947:INFO:Initializing Elastic Net
2023-03-05 17:16:50,948:INFO:Total runtime is 0.3593189160029093 minutes
2023-03-05 17:16:50,961:INFO:SubProcess create_model() called ==================================
2023-03-05 17:16:50,962:INFO:Initializing create_model()
2023-03-05 17:16:50,963:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000153716115B0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CEC5250>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:16:50,963:INFO:Checking exceptions
2023-03-05 17:16:50,963:INFO:Importing libraries
2023-03-05 17:16:50,964:INFO:Copying training dataset
2023-03-05 17:16:50,980:INFO:Defining folds
2023-03-05 17:16:50,980:INFO:Declaring metric variables
2023-03-05 17:16:50,991:INFO:Importing untrained model
2023-03-05 17:16:51,003:INFO:Elastic Net Imported successfully
2023-03-05 17:16:51,027:INFO:Starting cross validation
2023-03-05 17:16:51,031:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:16:52,979:INFO:Calculating mean and std
2023-03-05 17:16:52,983:INFO:Creating metrics dataframe
2023-03-05 17:16:52,991:INFO:Uploading results into container
2023-03-05 17:16:52,992:INFO:Uploading model into container now
2023-03-05 17:16:52,993:INFO:_master_model_container: 5
2023-03-05 17:16:52,993:INFO:_display_container: 3
2023-03-05 17:16:52,994:INFO:ElasticNet(random_state=7152)
2023-03-05 17:16:52,995:INFO:create_model() successfully completed......................................
2023-03-05 17:16:53,156:INFO:SubProcess create_model() end ==================================
2023-03-05 17:16:53,156:INFO:Creating metrics dataframe
2023-03-05 17:16:53,180:INFO:Initializing Least Angle Regression
2023-03-05 17:16:53,180:INFO:Total runtime is 0.3965177019437154 minutes
2023-03-05 17:16:53,191:INFO:SubProcess create_model() called ==================================
2023-03-05 17:16:53,192:INFO:Initializing create_model()
2023-03-05 17:16:53,192:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000153716115B0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CEC5250>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:16:53,192:INFO:Checking exceptions
2023-03-05 17:16:53,193:INFO:Importing libraries
2023-03-05 17:16:53,193:INFO:Copying training dataset
2023-03-05 17:16:53,213:INFO:Defining folds
2023-03-05 17:16:53,213:INFO:Declaring metric variables
2023-03-05 17:16:53,224:INFO:Importing untrained model
2023-03-05 17:16:53,237:INFO:Least Angle Regression Imported successfully
2023-03-05 17:16:53,259:INFO:Starting cross validation
2023-03-05 17:16:53,264:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:16:53,823:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:16:53,850:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:16:53,881:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:16:53,892:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.849e+01, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-05 17:16:53,903:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:16:53,909:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.253e+01, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-05 17:16:53,910:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=9.896e+00, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-05 17:16:53,914:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:16:53,927:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:16:54,043:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:16:54,119:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:16:54,771:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:16:54,781:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:16:55,206:INFO:Calculating mean and std
2023-03-05 17:16:55,212:INFO:Creating metrics dataframe
2023-03-05 17:16:55,223:INFO:Uploading results into container
2023-03-05 17:16:55,225:INFO:Uploading model into container now
2023-03-05 17:16:55,226:INFO:_master_model_container: 6
2023-03-05 17:16:55,226:INFO:_display_container: 3
2023-03-05 17:16:55,227:INFO:Lars(random_state=7152)
2023-03-05 17:16:55,228:INFO:create_model() successfully completed......................................
2023-03-05 17:16:55,401:INFO:SubProcess create_model() end ==================================
2023-03-05 17:16:55,401:INFO:Creating metrics dataframe
2023-03-05 17:16:55,426:INFO:Initializing Lasso Least Angle Regression
2023-03-05 17:16:55,427:INFO:Total runtime is 0.4339568416277567 minutes
2023-03-05 17:16:55,437:INFO:SubProcess create_model() called ==================================
2023-03-05 17:16:55,438:INFO:Initializing create_model()
2023-03-05 17:16:55,438:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000153716115B0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CEC5250>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:16:55,438:INFO:Checking exceptions
2023-03-05 17:16:55,439:INFO:Importing libraries
2023-03-05 17:16:55,439:INFO:Copying training dataset
2023-03-05 17:16:55,455:INFO:Defining folds
2023-03-05 17:16:55,456:INFO:Declaring metric variables
2023-03-05 17:16:55,467:INFO:Importing untrained model
2023-03-05 17:16:55,480:INFO:Lasso Least Angle Regression Imported successfully
2023-03-05 17:16:55,499:INFO:Starting cross validation
2023-03-05 17:16:55,504:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:16:56,059:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:16:56,070:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.849e+01, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-05 17:16:56,071:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.253e+01, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-05 17:16:56,071:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=9.896e+00, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-05 17:16:56,092:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:16:56,134:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:16:56,206:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:16:56,232:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:16:56,244:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:16:56,267:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:16:56,311:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:16:56,967:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:16:57,026:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:16:57,465:INFO:Calculating mean and std
2023-03-05 17:16:57,471:INFO:Creating metrics dataframe
2023-03-05 17:16:57,482:INFO:Uploading results into container
2023-03-05 17:16:57,483:INFO:Uploading model into container now
2023-03-05 17:16:57,484:INFO:_master_model_container: 7
2023-03-05 17:16:57,485:INFO:_display_container: 3
2023-03-05 17:16:57,486:INFO:LassoLars(random_state=7152)
2023-03-05 17:16:57,486:INFO:create_model() successfully completed......................................
2023-03-05 17:16:57,656:INFO:SubProcess create_model() end ==================================
2023-03-05 17:16:57,656:INFO:Creating metrics dataframe
2023-03-05 17:16:57,695:INFO:Initializing Orthogonal Matching Pursuit
2023-03-05 17:16:57,696:INFO:Total runtime is 0.4717822353045145 minutes
2023-03-05 17:16:57,707:INFO:SubProcess create_model() called ==================================
2023-03-05 17:16:57,708:INFO:Initializing create_model()
2023-03-05 17:16:57,708:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000153716115B0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CEC5250>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:16:57,710:INFO:Checking exceptions
2023-03-05 17:16:57,710:INFO:Importing libraries
2023-03-05 17:16:57,710:INFO:Copying training dataset
2023-03-05 17:16:57,722:INFO:Defining folds
2023-03-05 17:16:57,722:INFO:Declaring metric variables
2023-03-05 17:16:57,733:INFO:Importing untrained model
2023-03-05 17:16:57,742:INFO:Orthogonal Matching Pursuit Imported successfully
2023-03-05 17:16:57,765:INFO:Starting cross validation
2023-03-05 17:16:57,770:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:16:58,284:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:16:58,373:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:16:58,403:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:16:58,405:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:16:58,416:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:16:58,438:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:16:58,563:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:16:58,584:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:16:59,183:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:16:59,268:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:16:59,660:INFO:Calculating mean and std
2023-03-05 17:16:59,664:INFO:Creating metrics dataframe
2023-03-05 17:16:59,672:INFO:Uploading results into container
2023-03-05 17:16:59,673:INFO:Uploading model into container now
2023-03-05 17:16:59,674:INFO:_master_model_container: 8
2023-03-05 17:16:59,674:INFO:_display_container: 3
2023-03-05 17:16:59,675:INFO:OrthogonalMatchingPursuit()
2023-03-05 17:16:59,675:INFO:create_model() successfully completed......................................
2023-03-05 17:16:59,838:INFO:SubProcess create_model() end ==================================
2023-03-05 17:16:59,838:INFO:Creating metrics dataframe
2023-03-05 17:16:59,876:INFO:Initializing Bayesian Ridge
2023-03-05 17:16:59,876:INFO:Total runtime is 0.508113694190979 minutes
2023-03-05 17:16:59,887:INFO:SubProcess create_model() called ==================================
2023-03-05 17:16:59,888:INFO:Initializing create_model()
2023-03-05 17:16:59,889:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000153716115B0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CEC5250>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:16:59,889:INFO:Checking exceptions
2023-03-05 17:16:59,889:INFO:Importing libraries
2023-03-05 17:16:59,890:INFO:Copying training dataset
2023-03-05 17:16:59,905:INFO:Defining folds
2023-03-05 17:16:59,906:INFO:Declaring metric variables
2023-03-05 17:16:59,915:INFO:Importing untrained model
2023-03-05 17:16:59,926:INFO:Bayesian Ridge Imported successfully
2023-03-05 17:16:59,949:INFO:Starting cross validation
2023-03-05 17:16:59,954:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:17:01,908:INFO:Calculating mean and std
2023-03-05 17:17:01,912:INFO:Creating metrics dataframe
2023-03-05 17:17:01,920:INFO:Uploading results into container
2023-03-05 17:17:01,921:INFO:Uploading model into container now
2023-03-05 17:17:01,923:INFO:_master_model_container: 9
2023-03-05 17:17:01,923:INFO:_display_container: 3
2023-03-05 17:17:01,924:INFO:BayesianRidge()
2023-03-05 17:17:01,924:INFO:create_model() successfully completed......................................
2023-03-05 17:17:02,091:INFO:SubProcess create_model() end ==================================
2023-03-05 17:17:02,091:INFO:Creating metrics dataframe
2023-03-05 17:17:02,121:INFO:Initializing Passive Aggressive Regressor
2023-03-05 17:17:02,121:INFO:Total runtime is 0.5455358544985452 minutes
2023-03-05 17:17:02,130:INFO:SubProcess create_model() called ==================================
2023-03-05 17:17:02,131:INFO:Initializing create_model()
2023-03-05 17:17:02,131:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000153716115B0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CEC5250>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:17:02,131:INFO:Checking exceptions
2023-03-05 17:17:02,133:INFO:Importing libraries
2023-03-05 17:17:02,133:INFO:Copying training dataset
2023-03-05 17:17:02,148:INFO:Defining folds
2023-03-05 17:17:02,149:INFO:Declaring metric variables
2023-03-05 17:17:02,161:INFO:Importing untrained model
2023-03-05 17:17:02,174:INFO:Passive Aggressive Regressor Imported successfully
2023-03-05 17:17:02,199:INFO:Starting cross validation
2023-03-05 17:17:02,203:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:17:04,218:INFO:Calculating mean and std
2023-03-05 17:17:04,222:INFO:Creating metrics dataframe
2023-03-05 17:17:04,230:INFO:Uploading results into container
2023-03-05 17:17:04,231:INFO:Uploading model into container now
2023-03-05 17:17:04,232:INFO:_master_model_container: 10
2023-03-05 17:17:04,232:INFO:_display_container: 3
2023-03-05 17:17:04,233:INFO:PassiveAggressiveRegressor(random_state=7152)
2023-03-05 17:17:04,234:INFO:create_model() successfully completed......................................
2023-03-05 17:17:04,374:INFO:SubProcess create_model() end ==================================
2023-03-05 17:17:04,375:INFO:Creating metrics dataframe
2023-03-05 17:17:04,401:INFO:Initializing Huber Regressor
2023-03-05 17:17:04,402:INFO:Total runtime is 0.5835478623708088 minutes
2023-03-05 17:17:04,411:INFO:SubProcess create_model() called ==================================
2023-03-05 17:17:04,412:INFO:Initializing create_model()
2023-03-05 17:17:04,412:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000153716115B0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CEC5250>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:17:04,412:INFO:Checking exceptions
2023-03-05 17:17:04,413:INFO:Importing libraries
2023-03-05 17:17:04,413:INFO:Copying training dataset
2023-03-05 17:17:04,429:INFO:Defining folds
2023-03-05 17:17:04,430:INFO:Declaring metric variables
2023-03-05 17:17:04,441:INFO:Importing untrained model
2023-03-05 17:17:04,453:INFO:Huber Regressor Imported successfully
2023-03-05 17:17:04,475:INFO:Starting cross validation
2023-03-05 17:17:04,480:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:17:06,434:INFO:Calculating mean and std
2023-03-05 17:17:06,437:INFO:Creating metrics dataframe
2023-03-05 17:17:06,445:INFO:Uploading results into container
2023-03-05 17:17:06,446:INFO:Uploading model into container now
2023-03-05 17:17:06,447:INFO:_master_model_container: 11
2023-03-05 17:17:06,448:INFO:_display_container: 3
2023-03-05 17:17:06,448:INFO:HuberRegressor()
2023-03-05 17:17:06,449:INFO:create_model() successfully completed......................................
2023-03-05 17:17:06,606:INFO:SubProcess create_model() end ==================================
2023-03-05 17:17:06,606:INFO:Creating metrics dataframe
2023-03-05 17:17:06,635:INFO:Initializing K Neighbors Regressor
2023-03-05 17:17:06,636:INFO:Total runtime is 0.620781366030375 minutes
2023-03-05 17:17:06,644:INFO:SubProcess create_model() called ==================================
2023-03-05 17:17:06,645:INFO:Initializing create_model()
2023-03-05 17:17:06,645:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000153716115B0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CEC5250>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:17:06,645:INFO:Checking exceptions
2023-03-05 17:17:06,646:INFO:Importing libraries
2023-03-05 17:17:06,646:INFO:Copying training dataset
2023-03-05 17:17:06,660:INFO:Defining folds
2023-03-05 17:17:06,660:INFO:Declaring metric variables
2023-03-05 17:17:06,670:INFO:Importing untrained model
2023-03-05 17:17:06,681:INFO:K Neighbors Regressor Imported successfully
2023-03-05 17:17:06,701:INFO:Starting cross validation
2023-03-05 17:17:06,705:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:17:08,570:INFO:Calculating mean and std
2023-03-05 17:17:08,574:INFO:Creating metrics dataframe
2023-03-05 17:17:08,582:INFO:Uploading results into container
2023-03-05 17:17:08,583:INFO:Uploading model into container now
2023-03-05 17:17:08,584:INFO:_master_model_container: 12
2023-03-05 17:17:08,584:INFO:_display_container: 3
2023-03-05 17:17:08,585:INFO:KNeighborsRegressor(n_jobs=-1)
2023-03-05 17:17:08,585:INFO:create_model() successfully completed......................................
2023-03-05 17:17:08,720:INFO:SubProcess create_model() end ==================================
2023-03-05 17:17:08,720:INFO:Creating metrics dataframe
2023-03-05 17:17:08,749:INFO:Initializing Decision Tree Regressor
2023-03-05 17:17:08,749:INFO:Total runtime is 0.6559958457946776 minutes
2023-03-05 17:17:08,756:INFO:SubProcess create_model() called ==================================
2023-03-05 17:17:08,757:INFO:Initializing create_model()
2023-03-05 17:17:08,757:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000153716115B0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CEC5250>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:17:08,758:INFO:Checking exceptions
2023-03-05 17:17:08,758:INFO:Importing libraries
2023-03-05 17:17:08,758:INFO:Copying training dataset
2023-03-05 17:17:08,772:INFO:Defining folds
2023-03-05 17:17:08,772:INFO:Declaring metric variables
2023-03-05 17:17:08,781:INFO:Importing untrained model
2023-03-05 17:17:08,793:INFO:Decision Tree Regressor Imported successfully
2023-03-05 17:17:08,810:INFO:Starting cross validation
2023-03-05 17:17:08,815:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:17:10,604:INFO:Calculating mean and std
2023-03-05 17:17:10,607:INFO:Creating metrics dataframe
2023-03-05 17:17:10,615:INFO:Uploading results into container
2023-03-05 17:17:10,616:INFO:Uploading model into container now
2023-03-05 17:17:10,617:INFO:_master_model_container: 13
2023-03-05 17:17:10,617:INFO:_display_container: 3
2023-03-05 17:17:10,618:INFO:DecisionTreeRegressor(random_state=7152)
2023-03-05 17:17:10,618:INFO:create_model() successfully completed......................................
2023-03-05 17:17:10,754:INFO:SubProcess create_model() end ==================================
2023-03-05 17:17:10,754:INFO:Creating metrics dataframe
2023-03-05 17:17:10,785:INFO:Initializing Random Forest Regressor
2023-03-05 17:17:10,785:INFO:Total runtime is 0.6899253288904825 minutes
2023-03-05 17:17:10,796:INFO:SubProcess create_model() called ==================================
2023-03-05 17:17:10,798:INFO:Initializing create_model()
2023-03-05 17:17:10,798:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000153716115B0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CEC5250>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:17:10,798:INFO:Checking exceptions
2023-03-05 17:17:10,799:INFO:Importing libraries
2023-03-05 17:17:10,800:INFO:Copying training dataset
2023-03-05 17:17:10,816:INFO:Defining folds
2023-03-05 17:17:10,816:INFO:Declaring metric variables
2023-03-05 17:17:10,830:INFO:Importing untrained model
2023-03-05 17:17:10,842:INFO:Random Forest Regressor Imported successfully
2023-03-05 17:17:10,862:INFO:Starting cross validation
2023-03-05 17:17:10,866:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:17:15,472:INFO:Calculating mean and std
2023-03-05 17:17:15,478:INFO:Creating metrics dataframe
2023-03-05 17:17:15,491:INFO:Uploading results into container
2023-03-05 17:17:15,493:INFO:Uploading model into container now
2023-03-05 17:17:15,494:INFO:_master_model_container: 14
2023-03-05 17:17:15,494:INFO:_display_container: 3
2023-03-05 17:17:15,495:INFO:RandomForestRegressor(n_jobs=-1, random_state=7152)
2023-03-05 17:17:15,495:INFO:create_model() successfully completed......................................
2023-03-05 17:17:15,639:INFO:SubProcess create_model() end ==================================
2023-03-05 17:17:15,640:INFO:Creating metrics dataframe
2023-03-05 17:17:15,672:INFO:Initializing Extra Trees Regressor
2023-03-05 17:17:15,673:INFO:Total runtime is 0.7713985164960224 minutes
2023-03-05 17:17:15,683:INFO:SubProcess create_model() called ==================================
2023-03-05 17:17:15,684:INFO:Initializing create_model()
2023-03-05 17:17:15,684:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000153716115B0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CEC5250>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:17:15,685:INFO:Checking exceptions
2023-03-05 17:17:15,686:INFO:Importing libraries
2023-03-05 17:17:15,686:INFO:Copying training dataset
2023-03-05 17:17:15,699:INFO:Defining folds
2023-03-05 17:17:15,700:INFO:Declaring metric variables
2023-03-05 17:17:15,712:INFO:Importing untrained model
2023-03-05 17:17:15,722:INFO:Extra Trees Regressor Imported successfully
2023-03-05 17:17:15,738:INFO:Starting cross validation
2023-03-05 17:17:15,745:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:17:19,962:INFO:Calculating mean and std
2023-03-05 17:17:19,966:INFO:Creating metrics dataframe
2023-03-05 17:17:19,977:INFO:Uploading results into container
2023-03-05 17:17:19,979:INFO:Uploading model into container now
2023-03-05 17:17:19,981:INFO:_master_model_container: 15
2023-03-05 17:17:19,981:INFO:_display_container: 3
2023-03-05 17:17:19,982:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=7152)
2023-03-05 17:17:19,982:INFO:create_model() successfully completed......................................
2023-03-05 17:17:20,129:INFO:SubProcess create_model() end ==================================
2023-03-05 17:17:20,129:INFO:Creating metrics dataframe
2023-03-05 17:17:20,157:INFO:Initializing AdaBoost Regressor
2023-03-05 17:17:20,157:INFO:Total runtime is 0.846129544576009 minutes
2023-03-05 17:17:20,166:INFO:SubProcess create_model() called ==================================
2023-03-05 17:17:20,167:INFO:Initializing create_model()
2023-03-05 17:17:20,167:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000153716115B0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CEC5250>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:17:20,167:INFO:Checking exceptions
2023-03-05 17:17:20,168:INFO:Importing libraries
2023-03-05 17:17:20,168:INFO:Copying training dataset
2023-03-05 17:17:20,181:INFO:Defining folds
2023-03-05 17:17:20,182:INFO:Declaring metric variables
2023-03-05 17:17:20,195:INFO:Importing untrained model
2023-03-05 17:17:20,206:INFO:AdaBoost Regressor Imported successfully
2023-03-05 17:17:20,222:INFO:Starting cross validation
2023-03-05 17:17:20,227:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:17:23,294:INFO:Calculating mean and std
2023-03-05 17:17:23,297:INFO:Creating metrics dataframe
2023-03-05 17:17:23,305:INFO:Uploading results into container
2023-03-05 17:17:23,307:INFO:Uploading model into container now
2023-03-05 17:17:23,307:INFO:_master_model_container: 16
2023-03-05 17:17:23,307:INFO:_display_container: 3
2023-03-05 17:17:23,309:INFO:AdaBoostRegressor(random_state=7152)
2023-03-05 17:17:23,309:INFO:create_model() successfully completed......................................
2023-03-05 17:17:23,446:INFO:SubProcess create_model() end ==================================
2023-03-05 17:17:23,446:INFO:Creating metrics dataframe
2023-03-05 17:17:23,483:INFO:Initializing Gradient Boosting Regressor
2023-03-05 17:17:23,484:INFO:Total runtime is 0.9015839060147602 minutes
2023-03-05 17:17:23,493:INFO:SubProcess create_model() called ==================================
2023-03-05 17:17:23,495:INFO:Initializing create_model()
2023-03-05 17:17:23,495:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000153716115B0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CEC5250>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:17:23,495:INFO:Checking exceptions
2023-03-05 17:17:23,496:INFO:Importing libraries
2023-03-05 17:17:23,496:INFO:Copying training dataset
2023-03-05 17:17:23,511:INFO:Defining folds
2023-03-05 17:17:23,511:INFO:Declaring metric variables
2023-03-05 17:17:23,525:INFO:Importing untrained model
2023-03-05 17:17:23,534:INFO:Gradient Boosting Regressor Imported successfully
2023-03-05 17:17:23,553:INFO:Starting cross validation
2023-03-05 17:17:23,558:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:17:27,212:INFO:Calculating mean and std
2023-03-05 17:17:27,217:INFO:Creating metrics dataframe
2023-03-05 17:17:27,225:INFO:Uploading results into container
2023-03-05 17:17:27,227:INFO:Uploading model into container now
2023-03-05 17:17:27,228:INFO:_master_model_container: 17
2023-03-05 17:17:27,228:INFO:_display_container: 3
2023-03-05 17:17:27,230:INFO:GradientBoostingRegressor(random_state=7152)
2023-03-05 17:17:27,230:INFO:create_model() successfully completed......................................
2023-03-05 17:17:27,373:INFO:SubProcess create_model() end ==================================
2023-03-05 17:17:27,374:INFO:Creating metrics dataframe
2023-03-05 17:17:27,411:INFO:Initializing Extreme Gradient Boosting
2023-03-05 17:17:27,412:INFO:Total runtime is 0.9670401374499003 minutes
2023-03-05 17:17:27,424:INFO:SubProcess create_model() called ==================================
2023-03-05 17:17:27,425:INFO:Initializing create_model()
2023-03-05 17:17:27,425:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000153716115B0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CEC5250>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:17:27,425:INFO:Checking exceptions
2023-03-05 17:17:27,425:INFO:Importing libraries
2023-03-05 17:17:27,426:INFO:Copying training dataset
2023-03-05 17:17:27,439:INFO:Defining folds
2023-03-05 17:17:27,439:INFO:Declaring metric variables
2023-03-05 17:17:27,448:INFO:Importing untrained model
2023-03-05 17:17:27,457:INFO:Extreme Gradient Boosting Imported successfully
2023-03-05 17:17:27,474:INFO:Starting cross validation
2023-03-05 17:17:27,478:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:17:31,103:INFO:Calculating mean and std
2023-03-05 17:17:31,106:INFO:Creating metrics dataframe
2023-03-05 17:17:31,114:INFO:Uploading results into container
2023-03-05 17:17:31,115:INFO:Uploading model into container now
2023-03-05 17:17:31,116:INFO:_master_model_container: 18
2023-03-05 17:17:31,116:INFO:_display_container: 3
2023-03-05 17:17:31,118:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=7152, ...)
2023-03-05 17:17:31,119:INFO:create_model() successfully completed......................................
2023-03-05 17:17:31,259:INFO:SubProcess create_model() end ==================================
2023-03-05 17:17:31,259:INFO:Creating metrics dataframe
2023-03-05 17:17:31,295:INFO:Initializing Light Gradient Boosting Machine
2023-03-05 17:17:31,295:INFO:Total runtime is 1.0317646344502767 minutes
2023-03-05 17:17:31,305:INFO:SubProcess create_model() called ==================================
2023-03-05 17:17:31,306:INFO:Initializing create_model()
2023-03-05 17:17:31,306:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000153716115B0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CEC5250>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:17:31,307:INFO:Checking exceptions
2023-03-05 17:17:31,308:INFO:Importing libraries
2023-03-05 17:17:31,308:INFO:Copying training dataset
2023-03-05 17:17:31,320:INFO:Defining folds
2023-03-05 17:17:31,320:INFO:Declaring metric variables
2023-03-05 17:17:31,330:INFO:Importing untrained model
2023-03-05 17:17:31,339:INFO:Light Gradient Boosting Machine Imported successfully
2023-03-05 17:17:31,356:INFO:Starting cross validation
2023-03-05 17:17:31,360:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:17:35,895:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-05 17:17:36,053:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:230: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-05 17:17:36,055:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:230: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-05 17:17:37,029:INFO:Calculating mean and std
2023-03-05 17:17:37,031:INFO:Creating metrics dataframe
2023-03-05 17:17:37,036:INFO:Uploading results into container
2023-03-05 17:17:37,037:INFO:Uploading model into container now
2023-03-05 17:17:37,037:INFO:_master_model_container: 19
2023-03-05 17:17:37,037:INFO:_display_container: 3
2023-03-05 17:17:37,039:INFO:LGBMRegressor(random_state=7152)
2023-03-05 17:17:37,039:INFO:create_model() successfully completed......................................
2023-03-05 17:17:37,224:INFO:SubProcess create_model() end ==================================
2023-03-05 17:17:37,225:INFO:Creating metrics dataframe
2023-03-05 17:17:37,267:INFO:Initializing Dummy Regressor
2023-03-05 17:17:37,267:INFO:Total runtime is 1.131289541721344 minutes
2023-03-05 17:17:37,275:INFO:SubProcess create_model() called ==================================
2023-03-05 17:17:37,276:INFO:Initializing create_model()
2023-03-05 17:17:37,276:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000153716115B0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CEC5250>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:17:37,277:INFO:Checking exceptions
2023-03-05 17:17:37,277:INFO:Importing libraries
2023-03-05 17:17:37,278:INFO:Copying training dataset
2023-03-05 17:17:37,291:INFO:Defining folds
2023-03-05 17:17:37,292:INFO:Declaring metric variables
2023-03-05 17:17:37,303:INFO:Importing untrained model
2023-03-05 17:17:37,311:INFO:Dummy Regressor Imported successfully
2023-03-05 17:17:37,326:INFO:Starting cross validation
2023-03-05 17:17:37,331:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:17:39,144:INFO:Calculating mean and std
2023-03-05 17:17:39,148:INFO:Creating metrics dataframe
2023-03-05 17:17:39,156:INFO:Uploading results into container
2023-03-05 17:17:39,158:INFO:Uploading model into container now
2023-03-05 17:17:39,159:INFO:_master_model_container: 20
2023-03-05 17:17:39,159:INFO:_display_container: 3
2023-03-05 17:17:39,159:INFO:DummyRegressor()
2023-03-05 17:17:39,160:INFO:create_model() successfully completed......................................
2023-03-05 17:17:39,310:INFO:SubProcess create_model() end ==================================
2023-03-05 17:17:39,311:INFO:Creating metrics dataframe
2023-03-05 17:17:39,378:INFO:Initializing create_model()
2023-03-05 17:17:39,378:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000153716115B0>, estimator=Ridge(random_state=7152), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:17:39,378:INFO:Checking exceptions
2023-03-05 17:17:39,383:INFO:Importing libraries
2023-03-05 17:17:39,383:INFO:Copying training dataset
2023-03-05 17:17:39,397:INFO:Defining folds
2023-03-05 17:17:39,397:INFO:Declaring metric variables
2023-03-05 17:17:39,398:INFO:Importing untrained model
2023-03-05 17:17:39,398:INFO:Declaring custom model
2023-03-05 17:17:39,400:INFO:Ridge Regression Imported successfully
2023-03-05 17:17:39,404:INFO:Cross validation set to False
2023-03-05 17:17:39,404:INFO:Fitting Model
2023-03-05 17:17:39,744:INFO:Ridge(random_state=7152)
2023-03-05 17:17:39,744:INFO:create_model() successfully completed......................................
2023-03-05 17:17:40,023:INFO:_master_model_container: 20
2023-03-05 17:17:40,023:INFO:_display_container: 3
2023-03-05 17:17:40,024:INFO:Ridge(random_state=7152)
2023-03-05 17:17:40,025:INFO:compare_models() successfully completed......................................
2023-03-05 17:17:40,479:INFO:PyCaret RegressionExperiment
2023-03-05 17:17:40,479:INFO:Logging name: reg-default-name
2023-03-05 17:17:40,479:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-05 17:17:40,479:INFO:version 3.0.0.rc9
2023-03-05 17:17:40,479:INFO:Initializing setup()
2023-03-05 17:17:40,480:INFO:self.USI: 8697
2023-03-05 17:17:40,480:INFO:self._variable_keys: {'USI', 'data', 'log_plots_param', '_ml_usecase', 'memory', 'transform_target_param', 'html_param', 'X_train', 'pipeline', 'fold_shuffle_param', 'n_jobs_param', 'fold_generator', 'exp_id', 'X_test', 'exp_name_log', 'logging_param', 'y_train', '_available_plots', 'X', 'gpu_param', 'idx', 'gpu_n_jobs_param', 'fold_groups_param', 'target_param', 'y_test', 'seed', 'y'}
2023-03-05 17:17:40,480:INFO:Checking environment
2023-03-05 17:17:40,480:INFO:python_version: 3.9.13
2023-03-05 17:17:40,480:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-03-05 17:17:40,481:INFO:machine: AMD64
2023-03-05 17:17:40,481:INFO:platform: Windows-10-10.0.19044-SP0
2023-03-05 17:17:40,481:INFO:Memory: svmem(total=17009516544, available=4848267264, percent=71.5, used=12161249280, free=4848267264)
2023-03-05 17:17:40,481:INFO:Physical Core: 4
2023-03-05 17:17:40,481:INFO:Logical Core: 8
2023-03-05 17:17:40,481:INFO:Checking libraries
2023-03-05 17:17:40,481:INFO:System:
2023-03-05 17:17:40,482:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-03-05 17:17:40,482:INFO:executable: C:\Users\Alvaro\anaconda3\python.exe
2023-03-05 17:17:40,482:INFO:   machine: Windows-10-10.0.19044-SP0
2023-03-05 17:17:40,482:INFO:PyCaret required dependencies:
2023-03-05 17:17:40,482:INFO:                 pip: 22.2.2
2023-03-05 17:17:40,482:INFO:          setuptools: 63.4.1
2023-03-05 17:17:40,482:INFO:             pycaret: 3.0.0rc9
2023-03-05 17:17:40,482:INFO:             IPython: 7.31.1
2023-03-05 17:17:40,483:INFO:          ipywidgets: 7.6.5
2023-03-05 17:17:40,483:INFO:                tqdm: 4.64.1
2023-03-05 17:17:40,483:INFO:               numpy: 1.21.5
2023-03-05 17:17:40,483:INFO:              pandas: 1.4.4
2023-03-05 17:17:40,483:INFO:              jinja2: 2.11.3
2023-03-05 17:17:40,483:INFO:               scipy: 1.9.1
2023-03-05 17:17:40,483:INFO:              joblib: 1.2.0
2023-03-05 17:17:40,483:INFO:             sklearn: 1.0.2
2023-03-05 17:17:40,484:INFO:                pyod: 1.0.7
2023-03-05 17:17:40,484:INFO:            imblearn: 0.10.1
2023-03-05 17:17:40,484:INFO:   category_encoders: 2.6.0
2023-03-05 17:17:40,484:INFO:            lightgbm: 3.3.5
2023-03-05 17:17:40,484:INFO:               numba: 0.55.1
2023-03-05 17:17:40,484:INFO:            requests: 2.28.1
2023-03-05 17:17:40,484:INFO:          matplotlib: 3.5.2
2023-03-05 17:17:40,484:INFO:          scikitplot: 0.3.7
2023-03-05 17:17:40,485:INFO:         yellowbrick: 1.5
2023-03-05 17:17:40,485:INFO:              plotly: 5.9.0
2023-03-05 17:17:40,485:INFO:             kaleido: 0.2.1
2023-03-05 17:17:40,486:INFO:         statsmodels: 0.13.2
2023-03-05 17:17:40,486:INFO:              sktime: 0.16.1
2023-03-05 17:17:40,486:INFO:               tbats: 1.1.2
2023-03-05 17:17:40,486:INFO:            pmdarima: 2.0.2
2023-03-05 17:17:40,486:INFO:              psutil: 5.9.0
2023-03-05 17:17:40,486:INFO:PyCaret optional dependencies:
2023-03-05 17:17:40,487:INFO:                shap: Not installed
2023-03-05 17:17:40,487:INFO:           interpret: Not installed
2023-03-05 17:17:40,487:INFO:                umap: Not installed
2023-03-05 17:17:40,487:INFO:    pandas_profiling: Not installed
2023-03-05 17:17:40,487:INFO:  explainerdashboard: Not installed
2023-03-05 17:17:40,487:INFO:             autoviz: Not installed
2023-03-05 17:17:40,487:INFO:           fairlearn: Not installed
2023-03-05 17:17:40,487:INFO:             xgboost: 1.7.4
2023-03-05 17:17:40,488:INFO:            catboost: Not installed
2023-03-05 17:17:40,488:INFO:              kmodes: Not installed
2023-03-05 17:17:40,488:INFO:             mlxtend: Not installed
2023-03-05 17:17:40,488:INFO:       statsforecast: Not installed
2023-03-05 17:17:40,488:INFO:        tune_sklearn: Not installed
2023-03-05 17:17:40,489:INFO:                 ray: Not installed
2023-03-05 17:17:40,489:INFO:            hyperopt: Not installed
2023-03-05 17:17:40,489:INFO:              optuna: Not installed
2023-03-05 17:17:40,489:INFO:               skopt: Not installed
2023-03-05 17:17:40,489:INFO:              mlflow: Not installed
2023-03-05 17:17:40,490:INFO:              gradio: Not installed
2023-03-05 17:17:40,490:INFO:             fastapi: Not installed
2023-03-05 17:17:40,490:INFO:             uvicorn: Not installed
2023-03-05 17:17:40,490:INFO:              m2cgen: Not installed
2023-03-05 17:17:40,490:INFO:           evidently: Not installed
2023-03-05 17:17:40,491:INFO:               fugue: Not installed
2023-03-05 17:17:40,491:INFO:           streamlit: Not installed
2023-03-05 17:17:40,491:INFO:             prophet: Not installed
2023-03-05 17:17:40,491:INFO:None
2023-03-05 17:17:40,491:INFO:Set up data.
2023-03-05 17:17:40,511:INFO:Set up train/test split.
2023-03-05 17:17:40,522:INFO:Set up index.
2023-03-05 17:17:40,523:INFO:Set up folding strategy.
2023-03-05 17:17:40,523:INFO:Assigning column types.
2023-03-05 17:17:40,531:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-05 17:17:40,532:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-05 17:17:40,544:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 17:17:40,557:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:17:40,725:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:17:40,841:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:17:40,842:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:17:40,847:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:17:40,849:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-05 17:17:40,860:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 17:17:40,872:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:17:41,020:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:17:41,128:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:17:41,129:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:17:41,136:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:17:41,137:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-03-05 17:17:41,149:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 17:17:41,161:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:17:41,304:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:17:41,421:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:17:41,423:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:17:41,429:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:17:41,443:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 17:17:41,455:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:17:41,603:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:17:41,723:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:17:41,725:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:17:41,731:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:17:41,732:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-03-05 17:17:41,755:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:17:41,899:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:17:42,059:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:17:42,061:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:17:42,073:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:17:42,097:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:17:42,241:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:17:42,355:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:17:42,356:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:17:42,362:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:17:42,363:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-03-05 17:17:42,531:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:17:42,651:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:17:42,652:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:17:42,659:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:17:42,828:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:17:42,935:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:17:42,937:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:17:42,943:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:17:42,944:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-05 17:17:43,117:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:17:43,235:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:17:43,241:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:17:43,416:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:17:43,535:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:17:43,548:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:17:43,549:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-03-05 17:17:43,871:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:17:43,894:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:17:44,267:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:17:44,279:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:17:44,283:INFO:Preparing preprocessing pipeline...
2023-03-05 17:17:44,285:INFO:Set up simple imputation.
2023-03-05 17:17:44,292:INFO:Set up encoding of categorical features.
2023-03-05 17:17:44,772:INFO:Finished creating preprocessing pipeline.
2023-03-05 17:17:44,795:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Alvaro\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['work_year', 'remote_ratio'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['experience_level',
                                             'employment_type', 'job_title',
                                             'employee_residence',
                                             'company_location',
                                             'company_size'],
                                    t...
                                    transformer=OneHotEncoder(cols=['experience_level',
                                                                    'employment_type',
                                                                    'company_size'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['job_title', 'employee_residence',
                                             'company_location'],
                                    transformer=LeaveOneOutEncoder(cols=['job_title',
                                                                         'employee_residence',
                                                                         'company_location'],
                                                                   handle_missing='return_nan',
                                                                   random_state=6697)))])
2023-03-05 17:17:44,796:INFO:Creating final display dataframe.
2023-03-05 17:17:45,824:INFO:Setup _display_container:                     Description             Value
0                    Session id              6697
1                        Target     salary_in_usd
2                   Target type        Regression
3           Original data shape          (500, 9)
4        Transformed data shape         (500, 17)
5   Transformed train set shape         (350, 17)
6    Transformed test set shape         (150, 17)
7              Numeric features                 2
8          Categorical features                 6
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator             KFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  reg-default-name
21                          USI              8697
2023-03-05 17:17:46,165:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:17:46,171:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:17:46,491:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:17:46,498:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:17:46,499:INFO:setup() successfully completed in 6.04s...............
2023-03-05 17:17:46,499:INFO:Initializing create_model()
2023-03-05 17:17:46,499:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:17:46,500:INFO:Checking exceptions
2023-03-05 17:17:46,578:INFO:Importing libraries
2023-03-05 17:17:46,581:INFO:Copying training dataset
2023-03-05 17:17:46,601:INFO:Defining folds
2023-03-05 17:17:46,602:INFO:Declaring metric variables
2023-03-05 17:17:46,617:INFO:Importing untrained model
2023-03-05 17:17:46,629:INFO:Decision Tree Regressor Imported successfully
2023-03-05 17:17:46,651:INFO:Starting cross validation
2023-03-05 17:17:46,654:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:17:48,706:INFO:Calculating mean and std
2023-03-05 17:17:48,709:INFO:Creating metrics dataframe
2023-03-05 17:17:48,721:INFO:Finalizing model
2023-03-05 17:17:49,070:INFO:Uploading results into container
2023-03-05 17:17:49,073:INFO:Uploading model into container now
2023-03-05 17:17:49,105:INFO:_master_model_container: 1
2023-03-05 17:17:49,105:INFO:_display_container: 2
2023-03-05 17:17:49,106:INFO:DecisionTreeRegressor(random_state=6697)
2023-03-05 17:17:49,108:INFO:create_model() successfully completed......................................
2023-03-05 17:17:49,264:INFO:Initializing tune_model()
2023-03-05 17:17:49,264:INFO:tune_model(estimator=DecisionTreeRegressor(random_state=6697), fold=None, round=4, n_iter=250, custom_grid=None, optimize=RMSE, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>)
2023-03-05 17:17:49,264:INFO:Checking exceptions
2023-03-05 17:17:49,335:INFO:Copying training dataset
2023-03-05 17:17:49,349:INFO:Checking base model
2023-03-05 17:17:49,349:INFO:Base model : Decision Tree Regressor
2023-03-05 17:17:49,364:INFO:Declaring metric variables
2023-03-05 17:17:49,377:INFO:Defining Hyperparameters
2023-03-05 17:17:49,532:INFO:Tuning with n_jobs=-1
2023-03-05 17:17:49,532:INFO:Initializing RandomizedSearchCV
2023-03-05 17:18:35,134:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:18:35,446:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:18:35,843:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:18:36,079:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:19:27,292:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:19:27,340:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:19:27,877:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:19:55,085:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:21:05,799:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:21:06,209:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:295: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-05 17:22:26,301:INFO:best_params: {'actual_estimator__min_samples_split': 9, 'actual_estimator__min_samples_leaf': 2, 'actual_estimator__min_impurity_decrease': 0.005, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 16, 'actual_estimator__criterion': 'friedman_mse'}
2023-03-05 17:22:26,304:INFO:Hyperparameter search completed
2023-03-05 17:22:26,304:INFO:SubProcess create_model() called ==================================
2023-03-05 17:22:26,305:INFO:Initializing create_model()
2023-03-05 17:22:26,305:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=DecisionTreeRegressor(random_state=6697), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537D2000A0>, model_only=True, return_train_score=False, kwargs={'min_samples_split': 9, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.005, 'max_features': 'sqrt', 'max_depth': 16, 'criterion': 'friedman_mse'})
2023-03-05 17:22:26,305:INFO:Checking exceptions
2023-03-05 17:22:26,306:INFO:Importing libraries
2023-03-05 17:22:26,306:INFO:Copying training dataset
2023-03-05 17:22:26,315:INFO:Defining folds
2023-03-05 17:22:26,316:INFO:Declaring metric variables
2023-03-05 17:22:26,323:INFO:Importing untrained model
2023-03-05 17:22:26,323:INFO:Declaring custom model
2023-03-05 17:22:26,329:INFO:Decision Tree Regressor Imported successfully
2023-03-05 17:22:26,343:INFO:Starting cross validation
2023-03-05 17:22:26,348:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:22:27,009:INFO:Calculating mean and std
2023-03-05 17:22:27,011:INFO:Creating metrics dataframe
2023-03-05 17:22:27,019:INFO:Finalizing model
2023-03-05 17:22:27,219:INFO:Uploading results into container
2023-03-05 17:22:27,221:INFO:Uploading model into container now
2023-03-05 17:22:27,222:INFO:_master_model_container: 2
2023-03-05 17:22:27,222:INFO:_display_container: 3
2023-03-05 17:22:27,223:INFO:DecisionTreeRegressor(criterion='friedman_mse', max_depth=16,
                      max_features='sqrt', min_impurity_decrease=0.005,
                      min_samples_leaf=2, min_samples_split=9,
                      random_state=6697)
2023-03-05 17:22:27,223:INFO:create_model() successfully completed......................................
2023-03-05 17:22:27,339:INFO:SubProcess create_model() end ==================================
2023-03-05 17:22:27,339:INFO:choose_better activated
2023-03-05 17:22:27,344:INFO:SubProcess create_model() called ==================================
2023-03-05 17:22:27,345:INFO:Initializing create_model()
2023-03-05 17:22:27,345:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=DecisionTreeRegressor(random_state=6697), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:22:27,345:INFO:Checking exceptions
2023-03-05 17:22:27,347:INFO:Importing libraries
2023-03-05 17:22:27,347:INFO:Copying training dataset
2023-03-05 17:22:27,352:INFO:Defining folds
2023-03-05 17:22:27,353:INFO:Declaring metric variables
2023-03-05 17:22:27,353:INFO:Importing untrained model
2023-03-05 17:22:27,353:INFO:Declaring custom model
2023-03-05 17:22:27,354:INFO:Decision Tree Regressor Imported successfully
2023-03-05 17:22:27,354:INFO:Starting cross validation
2023-03-05 17:22:27,356:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:22:28,192:INFO:Calculating mean and std
2023-03-05 17:22:28,192:INFO:Creating metrics dataframe
2023-03-05 17:22:28,195:INFO:Finalizing model
2023-03-05 17:22:28,365:INFO:Uploading results into container
2023-03-05 17:22:28,366:INFO:Uploading model into container now
2023-03-05 17:22:28,366:INFO:_master_model_container: 3
2023-03-05 17:22:28,366:INFO:_display_container: 4
2023-03-05 17:22:28,367:INFO:DecisionTreeRegressor(random_state=6697)
2023-03-05 17:22:28,367:INFO:create_model() successfully completed......................................
2023-03-05 17:22:28,462:INFO:SubProcess create_model() end ==================================
2023-03-05 17:22:28,463:INFO:DecisionTreeRegressor(random_state=6697) result for RMSE is 58449.01
2023-03-05 17:22:28,464:INFO:DecisionTreeRegressor(criterion='friedman_mse', max_depth=16,
                      max_features='sqrt', min_impurity_decrease=0.005,
                      min_samples_leaf=2, min_samples_split=9,
                      random_state=6697) result for RMSE is 53621.5469
2023-03-05 17:22:28,464:INFO:DecisionTreeRegressor(criterion='friedman_mse', max_depth=16,
                      max_features='sqrt', min_impurity_decrease=0.005,
                      min_samples_leaf=2, min_samples_split=9,
                      random_state=6697) is best model
2023-03-05 17:22:28,464:INFO:choose_better completed
2023-03-05 17:22:28,475:INFO:_master_model_container: 3
2023-03-05 17:22:28,475:INFO:_display_container: 3
2023-03-05 17:22:28,476:INFO:DecisionTreeRegressor(criterion='friedman_mse', max_depth=16,
                      max_features='sqrt', min_impurity_decrease=0.005,
                      min_samples_leaf=2, min_samples_split=9,
                      random_state=6697)
2023-03-05 17:22:28,476:INFO:tune_model() successfully completed......................................
2023-03-05 17:22:28,597:INFO:Initializing compare_models()
2023-03-05 17:22:28,598:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-03-05 17:22:28,598:INFO:Checking exceptions
2023-03-05 17:22:28,601:INFO:Preparing display monitor
2023-03-05 17:22:28,655:INFO:Initializing Linear Regression
2023-03-05 17:22:28,656:INFO:Total runtime is 1.6538302103678384e-05 minutes
2023-03-05 17:22:28,660:INFO:SubProcess create_model() called ==================================
2023-03-05 17:22:28,661:INFO:Initializing create_model()
2023-03-05 17:22:28,661:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537C8A2610>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:22:28,661:INFO:Checking exceptions
2023-03-05 17:22:28,661:INFO:Importing libraries
2023-03-05 17:22:28,661:INFO:Copying training dataset
2023-03-05 17:22:28,666:INFO:Defining folds
2023-03-05 17:22:28,667:INFO:Declaring metric variables
2023-03-05 17:22:28,671:INFO:Importing untrained model
2023-03-05 17:22:28,677:INFO:Linear Regression Imported successfully
2023-03-05 17:22:28,684:INFO:Starting cross validation
2023-03-05 17:22:28,685:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:22:29,526:INFO:Calculating mean and std
2023-03-05 17:22:29,527:INFO:Creating metrics dataframe
2023-03-05 17:22:29,531:INFO:Uploading results into container
2023-03-05 17:22:29,532:INFO:Uploading model into container now
2023-03-05 17:22:29,533:INFO:_master_model_container: 4
2023-03-05 17:22:29,533:INFO:_display_container: 4
2023-03-05 17:22:29,533:INFO:LinearRegression(n_jobs=-1)
2023-03-05 17:22:29,534:INFO:create_model() successfully completed......................................
2023-03-05 17:22:29,638:INFO:SubProcess create_model() end ==================================
2023-03-05 17:22:29,638:INFO:Creating metrics dataframe
2023-03-05 17:22:29,650:INFO:Initializing Lasso Regression
2023-03-05 17:22:29,651:INFO:Total runtime is 0.016600227355957033 minutes
2023-03-05 17:22:29,656:INFO:SubProcess create_model() called ==================================
2023-03-05 17:22:29,656:INFO:Initializing create_model()
2023-03-05 17:22:29,656:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537C8A2610>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:22:29,656:INFO:Checking exceptions
2023-03-05 17:22:29,656:INFO:Importing libraries
2023-03-05 17:22:29,657:INFO:Copying training dataset
2023-03-05 17:22:29,666:INFO:Defining folds
2023-03-05 17:22:29,666:INFO:Declaring metric variables
2023-03-05 17:22:29,670:INFO:Importing untrained model
2023-03-05 17:22:29,675:INFO:Lasso Regression Imported successfully
2023-03-05 17:22:29,685:INFO:Starting cross validation
2023-03-05 17:22:29,687:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:22:30,572:INFO:Calculating mean and std
2023-03-05 17:22:30,575:INFO:Creating metrics dataframe
2023-03-05 17:22:30,581:INFO:Uploading results into container
2023-03-05 17:22:30,582:INFO:Uploading model into container now
2023-03-05 17:22:30,582:INFO:_master_model_container: 5
2023-03-05 17:22:30,582:INFO:_display_container: 4
2023-03-05 17:22:30,583:INFO:Lasso(random_state=6697)
2023-03-05 17:22:30,583:INFO:create_model() successfully completed......................................
2023-03-05 17:22:30,686:INFO:SubProcess create_model() end ==================================
2023-03-05 17:22:30,687:INFO:Creating metrics dataframe
2023-03-05 17:22:30,699:INFO:Initializing Ridge Regression
2023-03-05 17:22:30,699:INFO:Total runtime is 0.03406676848729452 minutes
2023-03-05 17:22:30,706:INFO:SubProcess create_model() called ==================================
2023-03-05 17:22:30,706:INFO:Initializing create_model()
2023-03-05 17:22:30,706:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537C8A2610>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:22:30,706:INFO:Checking exceptions
2023-03-05 17:22:30,706:INFO:Importing libraries
2023-03-05 17:22:30,706:INFO:Copying training dataset
2023-03-05 17:22:30,715:INFO:Defining folds
2023-03-05 17:22:30,715:INFO:Declaring metric variables
2023-03-05 17:22:30,720:INFO:Importing untrained model
2023-03-05 17:22:30,725:INFO:Ridge Regression Imported successfully
2023-03-05 17:22:30,735:INFO:Starting cross validation
2023-03-05 17:22:30,738:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:22:31,569:INFO:Calculating mean and std
2023-03-05 17:22:31,572:INFO:Creating metrics dataframe
2023-03-05 17:22:31,577:INFO:Uploading results into container
2023-03-05 17:22:31,579:INFO:Uploading model into container now
2023-03-05 17:22:31,580:INFO:_master_model_container: 6
2023-03-05 17:22:31,580:INFO:_display_container: 4
2023-03-05 17:22:31,581:INFO:Ridge(random_state=6697)
2023-03-05 17:22:31,581:INFO:create_model() successfully completed......................................
2023-03-05 17:22:31,681:INFO:SubProcess create_model() end ==================================
2023-03-05 17:22:31,682:INFO:Creating metrics dataframe
2023-03-05 17:22:31,693:INFO:Initializing Elastic Net
2023-03-05 17:22:31,693:INFO:Total runtime is 0.050633533795674646 minutes
2023-03-05 17:22:31,698:INFO:SubProcess create_model() called ==================================
2023-03-05 17:22:31,698:INFO:Initializing create_model()
2023-03-05 17:22:31,699:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537C8A2610>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:22:31,699:INFO:Checking exceptions
2023-03-05 17:22:31,699:INFO:Importing libraries
2023-03-05 17:22:31,699:INFO:Copying training dataset
2023-03-05 17:22:31,707:INFO:Defining folds
2023-03-05 17:22:31,707:INFO:Declaring metric variables
2023-03-05 17:22:31,714:INFO:Importing untrained model
2023-03-05 17:22:31,719:INFO:Elastic Net Imported successfully
2023-03-05 17:22:31,728:INFO:Starting cross validation
2023-03-05 17:22:31,730:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:22:32,591:INFO:Calculating mean and std
2023-03-05 17:22:32,593:INFO:Creating metrics dataframe
2023-03-05 17:22:32,597:INFO:Uploading results into container
2023-03-05 17:22:32,598:INFO:Uploading model into container now
2023-03-05 17:22:32,598:INFO:_master_model_container: 7
2023-03-05 17:22:32,598:INFO:_display_container: 4
2023-03-05 17:22:32,599:INFO:ElasticNet(random_state=6697)
2023-03-05 17:22:32,599:INFO:create_model() successfully completed......................................
2023-03-05 17:22:32,702:INFO:SubProcess create_model() end ==================================
2023-03-05 17:22:32,702:INFO:Creating metrics dataframe
2023-03-05 17:22:32,719:INFO:Initializing Least Angle Regression
2023-03-05 17:22:32,719:INFO:Total runtime is 0.06773268779118856 minutes
2023-03-05 17:22:32,723:INFO:SubProcess create_model() called ==================================
2023-03-05 17:22:32,724:INFO:Initializing create_model()
2023-03-05 17:22:32,724:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537C8A2610>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:22:32,724:INFO:Checking exceptions
2023-03-05 17:22:32,724:INFO:Importing libraries
2023-03-05 17:22:32,724:INFO:Copying training dataset
2023-03-05 17:22:32,732:INFO:Defining folds
2023-03-05 17:22:32,732:INFO:Declaring metric variables
2023-03-05 17:22:32,737:INFO:Importing untrained model
2023-03-05 17:22:32,743:INFO:Least Angle Regression Imported successfully
2023-03-05 17:22:32,754:INFO:Starting cross validation
2023-03-05 17:22:32,756:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:22:32,972:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:22:32,988:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:22:33,008:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:22:33,017:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:22:33,074:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:22:33,116:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:22:33,128:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:22:33,143:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:22:33,458:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:22:33,576:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:22:33,652:INFO:Calculating mean and std
2023-03-05 17:22:33,655:INFO:Creating metrics dataframe
2023-03-05 17:22:33,661:INFO:Uploading results into container
2023-03-05 17:22:33,662:INFO:Uploading model into container now
2023-03-05 17:22:33,663:INFO:_master_model_container: 8
2023-03-05 17:22:33,663:INFO:_display_container: 4
2023-03-05 17:22:33,664:INFO:Lars(random_state=6697)
2023-03-05 17:22:33,665:INFO:create_model() successfully completed......................................
2023-03-05 17:22:33,764:INFO:SubProcess create_model() end ==================================
2023-03-05 17:22:33,764:INFO:Creating metrics dataframe
2023-03-05 17:22:33,776:INFO:Initializing Lasso Least Angle Regression
2023-03-05 17:22:33,776:INFO:Total runtime is 0.08535110553105672 minutes
2023-03-05 17:22:33,781:INFO:SubProcess create_model() called ==================================
2023-03-05 17:22:33,781:INFO:Initializing create_model()
2023-03-05 17:22:33,782:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537C8A2610>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:22:33,782:INFO:Checking exceptions
2023-03-05 17:22:33,782:INFO:Importing libraries
2023-03-05 17:22:33,782:INFO:Copying training dataset
2023-03-05 17:22:33,789:INFO:Defining folds
2023-03-05 17:22:33,790:INFO:Declaring metric variables
2023-03-05 17:22:33,795:INFO:Importing untrained model
2023-03-05 17:22:33,802:INFO:Lasso Least Angle Regression Imported successfully
2023-03-05 17:22:33,811:INFO:Starting cross validation
2023-03-05 17:22:33,813:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:22:34,040:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:22:34,042:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:22:34,065:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:22:34,077:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:22:34,124:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:22:34,171:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:22:34,187:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:22:34,193:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:22:34,484:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:22:34,607:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:22:34,687:INFO:Calculating mean and std
2023-03-05 17:22:34,689:INFO:Creating metrics dataframe
2023-03-05 17:22:34,693:INFO:Uploading results into container
2023-03-05 17:22:34,694:INFO:Uploading model into container now
2023-03-05 17:22:34,695:INFO:_master_model_container: 9
2023-03-05 17:22:34,695:INFO:_display_container: 4
2023-03-05 17:22:34,695:INFO:LassoLars(random_state=6697)
2023-03-05 17:22:34,696:INFO:create_model() successfully completed......................................
2023-03-05 17:22:34,801:INFO:SubProcess create_model() end ==================================
2023-03-05 17:22:34,802:INFO:Creating metrics dataframe
2023-03-05 17:22:34,814:INFO:Initializing Orthogonal Matching Pursuit
2023-03-05 17:22:34,814:INFO:Total runtime is 0.10265486240386963 minutes
2023-03-05 17:22:34,819:INFO:SubProcess create_model() called ==================================
2023-03-05 17:22:34,820:INFO:Initializing create_model()
2023-03-05 17:22:34,820:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537C8A2610>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:22:34,820:INFO:Checking exceptions
2023-03-05 17:22:34,820:INFO:Importing libraries
2023-03-05 17:22:34,820:INFO:Copying training dataset
2023-03-05 17:22:34,827:INFO:Defining folds
2023-03-05 17:22:34,828:INFO:Declaring metric variables
2023-03-05 17:22:34,834:INFO:Importing untrained model
2023-03-05 17:22:34,839:INFO:Orthogonal Matching Pursuit Imported successfully
2023-03-05 17:22:34,848:INFO:Starting cross validation
2023-03-05 17:22:34,850:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:22:35,053:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:22:35,078:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:22:35,104:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:22:35,116:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:22:35,178:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:22:35,221:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:22:35,233:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:22:35,235:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:22:35,623:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:22:35,630:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:22:35,712:INFO:Calculating mean and std
2023-03-05 17:22:35,714:INFO:Creating metrics dataframe
2023-03-05 17:22:35,720:INFO:Uploading results into container
2023-03-05 17:22:35,721:INFO:Uploading model into container now
2023-03-05 17:22:35,722:INFO:_master_model_container: 10
2023-03-05 17:22:35,722:INFO:_display_container: 4
2023-03-05 17:22:35,722:INFO:OrthogonalMatchingPursuit()
2023-03-05 17:22:35,722:INFO:create_model() successfully completed......................................
2023-03-05 17:22:35,824:INFO:SubProcess create_model() end ==================================
2023-03-05 17:22:35,824:INFO:Creating metrics dataframe
2023-03-05 17:22:35,842:INFO:Initializing Bayesian Ridge
2023-03-05 17:22:35,842:INFO:Total runtime is 0.1197832187016805 minutes
2023-03-05 17:22:35,846:INFO:SubProcess create_model() called ==================================
2023-03-05 17:22:35,846:INFO:Initializing create_model()
2023-03-05 17:22:35,846:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537C8A2610>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:22:35,846:INFO:Checking exceptions
2023-03-05 17:22:35,846:INFO:Importing libraries
2023-03-05 17:22:35,846:INFO:Copying training dataset
2023-03-05 17:22:35,853:INFO:Defining folds
2023-03-05 17:22:35,853:INFO:Declaring metric variables
2023-03-05 17:22:35,858:INFO:Importing untrained model
2023-03-05 17:22:35,863:INFO:Bayesian Ridge Imported successfully
2023-03-05 17:22:35,874:INFO:Starting cross validation
2023-03-05 17:22:35,876:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:22:36,740:INFO:Calculating mean and std
2023-03-05 17:22:36,743:INFO:Creating metrics dataframe
2023-03-05 17:22:36,747:INFO:Uploading results into container
2023-03-05 17:22:36,748:INFO:Uploading model into container now
2023-03-05 17:22:36,750:INFO:_master_model_container: 11
2023-03-05 17:22:36,751:INFO:_display_container: 4
2023-03-05 17:22:36,752:INFO:BayesianRidge()
2023-03-05 17:22:36,752:INFO:create_model() successfully completed......................................
2023-03-05 17:22:36,853:INFO:SubProcess create_model() end ==================================
2023-03-05 17:22:36,853:INFO:Creating metrics dataframe
2023-03-05 17:22:36,866:INFO:Initializing Passive Aggressive Regressor
2023-03-05 17:22:36,866:INFO:Total runtime is 0.1368477463722229 minutes
2023-03-05 17:22:36,872:INFO:SubProcess create_model() called ==================================
2023-03-05 17:22:36,872:INFO:Initializing create_model()
2023-03-05 17:22:36,872:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537C8A2610>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:22:36,872:INFO:Checking exceptions
2023-03-05 17:22:36,873:INFO:Importing libraries
2023-03-05 17:22:36,873:INFO:Copying training dataset
2023-03-05 17:22:36,881:INFO:Defining folds
2023-03-05 17:22:36,881:INFO:Declaring metric variables
2023-03-05 17:22:36,888:INFO:Importing untrained model
2023-03-05 17:22:36,893:INFO:Passive Aggressive Regressor Imported successfully
2023-03-05 17:22:36,901:INFO:Starting cross validation
2023-03-05 17:22:36,903:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:22:37,757:INFO:Calculating mean and std
2023-03-05 17:22:37,758:INFO:Creating metrics dataframe
2023-03-05 17:22:37,763:INFO:Uploading results into container
2023-03-05 17:22:37,763:INFO:Uploading model into container now
2023-03-05 17:22:37,764:INFO:_master_model_container: 12
2023-03-05 17:22:37,764:INFO:_display_container: 4
2023-03-05 17:22:37,764:INFO:PassiveAggressiveRegressor(random_state=6697)
2023-03-05 17:22:37,764:INFO:create_model() successfully completed......................................
2023-03-05 17:22:37,865:INFO:SubProcess create_model() end ==================================
2023-03-05 17:22:37,865:INFO:Creating metrics dataframe
2023-03-05 17:22:37,881:INFO:Initializing Huber Regressor
2023-03-05 17:22:37,881:INFO:Total runtime is 0.15376332998275757 minutes
2023-03-05 17:22:37,888:INFO:SubProcess create_model() called ==================================
2023-03-05 17:22:37,888:INFO:Initializing create_model()
2023-03-05 17:22:37,888:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537C8A2610>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:22:37,888:INFO:Checking exceptions
2023-03-05 17:22:37,889:INFO:Importing libraries
2023-03-05 17:22:37,889:INFO:Copying training dataset
2023-03-05 17:22:37,895:INFO:Defining folds
2023-03-05 17:22:37,895:INFO:Declaring metric variables
2023-03-05 17:22:37,900:INFO:Importing untrained model
2023-03-05 17:22:37,906:INFO:Huber Regressor Imported successfully
2023-03-05 17:22:37,916:INFO:Starting cross validation
2023-03-05 17:22:37,920:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:22:38,910:INFO:Calculating mean and std
2023-03-05 17:22:38,913:INFO:Creating metrics dataframe
2023-03-05 17:22:38,917:INFO:Uploading results into container
2023-03-05 17:22:38,920:INFO:Uploading model into container now
2023-03-05 17:22:38,921:INFO:_master_model_container: 13
2023-03-05 17:22:38,921:INFO:_display_container: 4
2023-03-05 17:22:38,922:INFO:HuberRegressor()
2023-03-05 17:22:38,922:INFO:create_model() successfully completed......................................
2023-03-05 17:22:39,024:INFO:SubProcess create_model() end ==================================
2023-03-05 17:22:39,024:INFO:Creating metrics dataframe
2023-03-05 17:22:39,039:INFO:Initializing K Neighbors Regressor
2023-03-05 17:22:39,039:INFO:Total runtime is 0.17306562662124633 minutes
2023-03-05 17:22:39,044:INFO:SubProcess create_model() called ==================================
2023-03-05 17:22:39,045:INFO:Initializing create_model()
2023-03-05 17:22:39,045:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537C8A2610>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:22:39,045:INFO:Checking exceptions
2023-03-05 17:22:39,045:INFO:Importing libraries
2023-03-05 17:22:39,045:INFO:Copying training dataset
2023-03-05 17:22:39,054:INFO:Defining folds
2023-03-05 17:22:39,054:INFO:Declaring metric variables
2023-03-05 17:22:39,059:INFO:Importing untrained model
2023-03-05 17:22:39,063:INFO:K Neighbors Regressor Imported successfully
2023-03-05 17:22:39,072:INFO:Starting cross validation
2023-03-05 17:22:39,075:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:22:39,974:INFO:Calculating mean and std
2023-03-05 17:22:39,976:INFO:Creating metrics dataframe
2023-03-05 17:22:39,981:INFO:Uploading results into container
2023-03-05 17:22:39,982:INFO:Uploading model into container now
2023-03-05 17:22:39,982:INFO:_master_model_container: 14
2023-03-05 17:22:39,983:INFO:_display_container: 4
2023-03-05 17:22:39,983:INFO:KNeighborsRegressor(n_jobs=-1)
2023-03-05 17:22:39,983:INFO:create_model() successfully completed......................................
2023-03-05 17:22:40,089:INFO:SubProcess create_model() end ==================================
2023-03-05 17:22:40,089:INFO:Creating metrics dataframe
2023-03-05 17:22:40,104:INFO:Initializing Decision Tree Regressor
2023-03-05 17:22:40,104:INFO:Total runtime is 0.1908238410949707 minutes
2023-03-05 17:22:40,108:INFO:SubProcess create_model() called ==================================
2023-03-05 17:22:40,108:INFO:Initializing create_model()
2023-03-05 17:22:40,108:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537C8A2610>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:22:40,108:INFO:Checking exceptions
2023-03-05 17:22:40,108:INFO:Importing libraries
2023-03-05 17:22:40,109:INFO:Copying training dataset
2023-03-05 17:22:40,115:INFO:Defining folds
2023-03-05 17:22:40,116:INFO:Declaring metric variables
2023-03-05 17:22:40,122:INFO:Importing untrained model
2023-03-05 17:22:40,127:INFO:Decision Tree Regressor Imported successfully
2023-03-05 17:22:40,139:INFO:Starting cross validation
2023-03-05 17:22:40,140:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:22:40,680:INFO:Calculating mean and std
2023-03-05 17:22:40,682:INFO:Creating metrics dataframe
2023-03-05 17:22:40,688:INFO:Uploading results into container
2023-03-05 17:22:40,688:INFO:Uploading model into container now
2023-03-05 17:22:40,689:INFO:_master_model_container: 15
2023-03-05 17:22:40,689:INFO:_display_container: 4
2023-03-05 17:22:40,689:INFO:DecisionTreeRegressor(random_state=6697)
2023-03-05 17:22:40,689:INFO:create_model() successfully completed......................................
2023-03-05 17:22:40,788:INFO:SubProcess create_model() end ==================================
2023-03-05 17:22:40,789:INFO:Creating metrics dataframe
2023-03-05 17:22:40,806:INFO:Initializing Random Forest Regressor
2023-03-05 17:22:40,806:INFO:Total runtime is 0.20251061916351318 minutes
2023-03-05 17:22:40,810:INFO:SubProcess create_model() called ==================================
2023-03-05 17:22:40,810:INFO:Initializing create_model()
2023-03-05 17:22:40,810:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537C8A2610>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:22:40,810:INFO:Checking exceptions
2023-03-05 17:22:40,811:INFO:Importing libraries
2023-03-05 17:22:40,811:INFO:Copying training dataset
2023-03-05 17:22:40,817:INFO:Defining folds
2023-03-05 17:22:40,817:INFO:Declaring metric variables
2023-03-05 17:22:40,822:INFO:Importing untrained model
2023-03-05 17:22:40,829:INFO:Random Forest Regressor Imported successfully
2023-03-05 17:22:40,840:INFO:Starting cross validation
2023-03-05 17:22:40,842:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:22:43,586:INFO:Calculating mean and std
2023-03-05 17:22:43,588:INFO:Creating metrics dataframe
2023-03-05 17:22:43,594:INFO:Uploading results into container
2023-03-05 17:22:43,595:INFO:Uploading model into container now
2023-03-05 17:22:43,595:INFO:_master_model_container: 16
2023-03-05 17:22:43,596:INFO:_display_container: 4
2023-03-05 17:22:43,596:INFO:RandomForestRegressor(n_jobs=-1, random_state=6697)
2023-03-05 17:22:43,596:INFO:create_model() successfully completed......................................
2023-03-05 17:22:43,693:INFO:SubProcess create_model() end ==================================
2023-03-05 17:22:43,694:INFO:Creating metrics dataframe
2023-03-05 17:22:43,708:INFO:Initializing Extra Trees Regressor
2023-03-05 17:22:43,708:INFO:Total runtime is 0.25087924003601075 minutes
2023-03-05 17:22:43,712:INFO:SubProcess create_model() called ==================================
2023-03-05 17:22:43,713:INFO:Initializing create_model()
2023-03-05 17:22:43,713:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537C8A2610>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:22:43,713:INFO:Checking exceptions
2023-03-05 17:22:43,713:INFO:Importing libraries
2023-03-05 17:22:43,713:INFO:Copying training dataset
2023-03-05 17:22:43,720:INFO:Defining folds
2023-03-05 17:22:43,720:INFO:Declaring metric variables
2023-03-05 17:22:43,727:INFO:Importing untrained model
2023-03-05 17:22:43,732:INFO:Extra Trees Regressor Imported successfully
2023-03-05 17:22:43,740:INFO:Starting cross validation
2023-03-05 17:22:43,742:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:22:46,352:INFO:Calculating mean and std
2023-03-05 17:22:46,354:INFO:Creating metrics dataframe
2023-03-05 17:22:46,358:INFO:Uploading results into container
2023-03-05 17:22:46,358:INFO:Uploading model into container now
2023-03-05 17:22:46,359:INFO:_master_model_container: 17
2023-03-05 17:22:46,359:INFO:_display_container: 4
2023-03-05 17:22:46,359:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=6697)
2023-03-05 17:22:46,360:INFO:create_model() successfully completed......................................
2023-03-05 17:22:46,457:INFO:SubProcess create_model() end ==================================
2023-03-05 17:22:46,458:INFO:Creating metrics dataframe
2023-03-05 17:22:46,474:INFO:Initializing AdaBoost Regressor
2023-03-05 17:22:46,475:INFO:Total runtime is 0.29699554840723674 minutes
2023-03-05 17:22:46,481:INFO:SubProcess create_model() called ==================================
2023-03-05 17:22:46,482:INFO:Initializing create_model()
2023-03-05 17:22:46,482:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537C8A2610>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:22:46,482:INFO:Checking exceptions
2023-03-05 17:22:46,482:INFO:Importing libraries
2023-03-05 17:22:46,482:INFO:Copying training dataset
2023-03-05 17:22:46,488:INFO:Defining folds
2023-03-05 17:22:46,488:INFO:Declaring metric variables
2023-03-05 17:22:46,493:INFO:Importing untrained model
2023-03-05 17:22:46,499:INFO:AdaBoost Regressor Imported successfully
2023-03-05 17:22:46,508:INFO:Starting cross validation
2023-03-05 17:22:46,513:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:22:48,205:INFO:Calculating mean and std
2023-03-05 17:22:48,206:INFO:Creating metrics dataframe
2023-03-05 17:22:48,211:INFO:Uploading results into container
2023-03-05 17:22:48,212:INFO:Uploading model into container now
2023-03-05 17:22:48,213:INFO:_master_model_container: 18
2023-03-05 17:22:48,213:INFO:_display_container: 4
2023-03-05 17:22:48,213:INFO:AdaBoostRegressor(random_state=6697)
2023-03-05 17:22:48,213:INFO:create_model() successfully completed......................................
2023-03-05 17:22:48,313:INFO:SubProcess create_model() end ==================================
2023-03-05 17:22:48,314:INFO:Creating metrics dataframe
2023-03-05 17:22:48,328:INFO:Initializing Gradient Boosting Regressor
2023-03-05 17:22:48,328:INFO:Total runtime is 0.32788814703623453 minutes
2023-03-05 17:22:48,332:INFO:SubProcess create_model() called ==================================
2023-03-05 17:22:48,333:INFO:Initializing create_model()
2023-03-05 17:22:48,334:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537C8A2610>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:22:48,334:INFO:Checking exceptions
2023-03-05 17:22:48,334:INFO:Importing libraries
2023-03-05 17:22:48,334:INFO:Copying training dataset
2023-03-05 17:22:48,342:INFO:Defining folds
2023-03-05 17:22:48,342:INFO:Declaring metric variables
2023-03-05 17:22:48,350:INFO:Importing untrained model
2023-03-05 17:22:48,354:INFO:Gradient Boosting Regressor Imported successfully
2023-03-05 17:22:48,361:INFO:Starting cross validation
2023-03-05 17:22:48,363:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:22:50,115:INFO:Calculating mean and std
2023-03-05 17:22:50,117:INFO:Creating metrics dataframe
2023-03-05 17:22:50,122:INFO:Uploading results into container
2023-03-05 17:22:50,122:INFO:Uploading model into container now
2023-03-05 17:22:50,123:INFO:_master_model_container: 19
2023-03-05 17:22:50,123:INFO:_display_container: 4
2023-03-05 17:22:50,124:INFO:GradientBoostingRegressor(random_state=6697)
2023-03-05 17:22:50,124:INFO:create_model() successfully completed......................................
2023-03-05 17:22:50,224:INFO:SubProcess create_model() end ==================================
2023-03-05 17:22:50,224:INFO:Creating metrics dataframe
2023-03-05 17:22:50,241:INFO:Initializing Extreme Gradient Boosting
2023-03-05 17:22:50,242:INFO:Total runtime is 0.3597606102625529 minutes
2023-03-05 17:22:50,246:INFO:SubProcess create_model() called ==================================
2023-03-05 17:22:50,246:INFO:Initializing create_model()
2023-03-05 17:22:50,247:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537C8A2610>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:22:50,247:INFO:Checking exceptions
2023-03-05 17:22:50,247:INFO:Importing libraries
2023-03-05 17:22:50,247:INFO:Copying training dataset
2023-03-05 17:22:50,253:INFO:Defining folds
2023-03-05 17:22:50,254:INFO:Declaring metric variables
2023-03-05 17:22:50,259:INFO:Importing untrained model
2023-03-05 17:22:50,266:INFO:Extreme Gradient Boosting Imported successfully
2023-03-05 17:22:50,275:INFO:Starting cross validation
2023-03-05 17:22:50,277:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:22:51,666:INFO:Calculating mean and std
2023-03-05 17:22:51,669:INFO:Creating metrics dataframe
2023-03-05 17:22:51,675:INFO:Uploading results into container
2023-03-05 17:22:51,676:INFO:Uploading model into container now
2023-03-05 17:22:51,677:INFO:_master_model_container: 20
2023-03-05 17:22:51,678:INFO:_display_container: 4
2023-03-05 17:22:51,679:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=6697, ...)
2023-03-05 17:22:51,680:INFO:create_model() successfully completed......................................
2023-03-05 17:22:51,787:INFO:SubProcess create_model() end ==================================
2023-03-05 17:22:51,787:INFO:Creating metrics dataframe
2023-03-05 17:22:51,803:INFO:Initializing Light Gradient Boosting Machine
2023-03-05 17:22:51,804:INFO:Total runtime is 0.3858163515726725 minutes
2023-03-05 17:22:51,809:INFO:SubProcess create_model() called ==================================
2023-03-05 17:22:51,810:INFO:Initializing create_model()
2023-03-05 17:22:51,810:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537C8A2610>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:22:51,810:INFO:Checking exceptions
2023-03-05 17:22:51,810:INFO:Importing libraries
2023-03-05 17:22:51,810:INFO:Copying training dataset
2023-03-05 17:22:51,820:INFO:Defining folds
2023-03-05 17:22:51,820:INFO:Declaring metric variables
2023-03-05 17:22:51,825:INFO:Importing untrained model
2023-03-05 17:22:51,830:INFO:Light Gradient Boosting Machine Imported successfully
2023-03-05 17:22:51,840:INFO:Starting cross validation
2023-03-05 17:22:51,844:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:22:52,829:INFO:Calculating mean and std
2023-03-05 17:22:52,831:INFO:Creating metrics dataframe
2023-03-05 17:22:52,839:INFO:Uploading results into container
2023-03-05 17:22:52,840:INFO:Uploading model into container now
2023-03-05 17:22:52,840:INFO:_master_model_container: 21
2023-03-05 17:22:52,840:INFO:_display_container: 4
2023-03-05 17:22:52,841:INFO:LGBMRegressor(random_state=6697)
2023-03-05 17:22:52,841:INFO:create_model() successfully completed......................................
2023-03-05 17:22:52,950:INFO:SubProcess create_model() end ==================================
2023-03-05 17:22:52,950:INFO:Creating metrics dataframe
2023-03-05 17:22:52,970:INFO:Initializing Dummy Regressor
2023-03-05 17:22:52,970:INFO:Total runtime is 0.40524530013402305 minutes
2023-03-05 17:22:52,975:INFO:SubProcess create_model() called ==================================
2023-03-05 17:22:52,976:INFO:Initializing create_model()
2023-03-05 17:22:52,976:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537C8A2610>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:22:52,976:INFO:Checking exceptions
2023-03-05 17:22:52,976:INFO:Importing libraries
2023-03-05 17:22:52,976:INFO:Copying training dataset
2023-03-05 17:22:52,987:INFO:Defining folds
2023-03-05 17:22:52,987:INFO:Declaring metric variables
2023-03-05 17:22:52,992:INFO:Importing untrained model
2023-03-05 17:22:52,997:INFO:Dummy Regressor Imported successfully
2023-03-05 17:22:53,008:INFO:Starting cross validation
2023-03-05 17:22:53,010:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:22:54,075:INFO:Calculating mean and std
2023-03-05 17:22:54,077:INFO:Creating metrics dataframe
2023-03-05 17:22:54,082:INFO:Uploading results into container
2023-03-05 17:22:54,082:INFO:Uploading model into container now
2023-03-05 17:22:54,083:INFO:_master_model_container: 22
2023-03-05 17:22:54,083:INFO:_display_container: 4
2023-03-05 17:22:54,084:INFO:DummyRegressor()
2023-03-05 17:22:54,084:INFO:create_model() successfully completed......................................
2023-03-05 17:22:54,190:INFO:SubProcess create_model() end ==================================
2023-03-05 17:22:54,190:INFO:Creating metrics dataframe
2023-03-05 17:22:54,220:INFO:Initializing create_model()
2023-03-05 17:22:54,220:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=Ridge(random_state=6697), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:22:54,221:INFO:Checking exceptions
2023-03-05 17:22:54,224:INFO:Importing libraries
2023-03-05 17:22:54,224:INFO:Copying training dataset
2023-03-05 17:22:54,230:INFO:Defining folds
2023-03-05 17:22:54,230:INFO:Declaring metric variables
2023-03-05 17:22:54,230:INFO:Importing untrained model
2023-03-05 17:22:54,230:INFO:Declaring custom model
2023-03-05 17:22:54,231:INFO:Ridge Regression Imported successfully
2023-03-05 17:22:54,232:INFO:Cross validation set to False
2023-03-05 17:22:54,233:INFO:Fitting Model
2023-03-05 17:22:54,389:INFO:Ridge(random_state=6697)
2023-03-05 17:22:54,389:INFO:create_model() successfully completed......................................
2023-03-05 17:22:54,584:INFO:_master_model_container: 22
2023-03-05 17:22:54,584:INFO:_display_container: 4
2023-03-05 17:22:54,585:INFO:Ridge(random_state=6697)
2023-03-05 17:22:54,585:INFO:compare_models() successfully completed......................................
2023-03-05 17:22:54,602:INFO:Initializing compare_models()
2023-03-05 17:22:54,602:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-03-05 17:22:54,603:INFO:Checking exceptions
2023-03-05 17:22:54,608:INFO:Preparing display monitor
2023-03-05 17:22:54,686:INFO:Initializing Linear Regression
2023-03-05 17:22:54,687:INFO:Total runtime is 3.3239523569742836e-05 minutes
2023-03-05 17:22:54,698:INFO:SubProcess create_model() called ==================================
2023-03-05 17:22:54,699:INFO:Initializing create_model()
2023-03-05 17:22:54,700:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CC03EB0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:22:54,702:INFO:Checking exceptions
2023-03-05 17:22:54,702:INFO:Importing libraries
2023-03-05 17:22:54,703:INFO:Copying training dataset
2023-03-05 17:22:54,718:INFO:Defining folds
2023-03-05 17:22:54,718:INFO:Declaring metric variables
2023-03-05 17:22:54,725:INFO:Importing untrained model
2023-03-05 17:22:54,733:INFO:Linear Regression Imported successfully
2023-03-05 17:22:54,744:INFO:Starting cross validation
2023-03-05 17:22:54,746:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:22:55,438:INFO:Calculating mean and std
2023-03-05 17:22:55,439:INFO:Creating metrics dataframe
2023-03-05 17:22:55,444:INFO:Uploading results into container
2023-03-05 17:22:55,444:INFO:Uploading model into container now
2023-03-05 17:22:55,445:INFO:_master_model_container: 23
2023-03-05 17:22:55,445:INFO:_display_container: 5
2023-03-05 17:22:55,445:INFO:LinearRegression(n_jobs=-1)
2023-03-05 17:22:55,445:INFO:create_model() successfully completed......................................
2023-03-05 17:22:55,572:INFO:SubProcess create_model() end ==================================
2023-03-05 17:22:55,572:INFO:Creating metrics dataframe
2023-03-05 17:22:55,582:INFO:Initializing Lasso Regression
2023-03-05 17:22:55,582:INFO:Total runtime is 0.014944736162821451 minutes
2023-03-05 17:22:55,587:INFO:SubProcess create_model() called ==================================
2023-03-05 17:22:55,588:INFO:Initializing create_model()
2023-03-05 17:22:55,588:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CC03EB0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:22:55,588:INFO:Checking exceptions
2023-03-05 17:22:55,588:INFO:Importing libraries
2023-03-05 17:22:55,588:INFO:Copying training dataset
2023-03-05 17:22:55,596:INFO:Defining folds
2023-03-05 17:22:55,597:INFO:Declaring metric variables
2023-03-05 17:22:55,604:INFO:Importing untrained model
2023-03-05 17:22:55,609:INFO:Lasso Regression Imported successfully
2023-03-05 17:22:55,620:INFO:Starting cross validation
2023-03-05 17:22:55,624:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:22:56,253:INFO:Calculating mean and std
2023-03-05 17:22:56,255:INFO:Creating metrics dataframe
2023-03-05 17:22:56,263:INFO:Uploading results into container
2023-03-05 17:22:56,265:INFO:Uploading model into container now
2023-03-05 17:22:56,265:INFO:_master_model_container: 24
2023-03-05 17:22:56,266:INFO:_display_container: 5
2023-03-05 17:22:56,267:INFO:Lasso(random_state=6697)
2023-03-05 17:22:56,267:INFO:create_model() successfully completed......................................
2023-03-05 17:22:56,388:INFO:SubProcess create_model() end ==================================
2023-03-05 17:22:56,388:INFO:Creating metrics dataframe
2023-03-05 17:22:56,403:INFO:Initializing Ridge Regression
2023-03-05 17:22:56,404:INFO:Total runtime is 0.028646540641784665 minutes
2023-03-05 17:22:56,409:INFO:SubProcess create_model() called ==================================
2023-03-05 17:22:56,409:INFO:Initializing create_model()
2023-03-05 17:22:56,409:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CC03EB0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:22:56,409:INFO:Checking exceptions
2023-03-05 17:22:56,410:INFO:Importing libraries
2023-03-05 17:22:56,410:INFO:Copying training dataset
2023-03-05 17:22:56,417:INFO:Defining folds
2023-03-05 17:22:56,418:INFO:Declaring metric variables
2023-03-05 17:22:56,427:INFO:Importing untrained model
2023-03-05 17:22:56,439:INFO:Ridge Regression Imported successfully
2023-03-05 17:22:56,452:INFO:Starting cross validation
2023-03-05 17:22:56,456:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:22:57,049:INFO:Calculating mean and std
2023-03-05 17:22:57,052:INFO:Creating metrics dataframe
2023-03-05 17:22:57,057:INFO:Uploading results into container
2023-03-05 17:22:57,059:INFO:Uploading model into container now
2023-03-05 17:22:57,060:INFO:_master_model_container: 25
2023-03-05 17:22:57,061:INFO:_display_container: 5
2023-03-05 17:22:57,061:INFO:Ridge(random_state=6697)
2023-03-05 17:22:57,062:INFO:create_model() successfully completed......................................
2023-03-05 17:22:57,180:INFO:SubProcess create_model() end ==================================
2023-03-05 17:22:57,180:INFO:Creating metrics dataframe
2023-03-05 17:22:57,195:INFO:Initializing Elastic Net
2023-03-05 17:22:57,196:INFO:Total runtime is 0.04183699289957682 minutes
2023-03-05 17:22:57,200:INFO:SubProcess create_model() called ==================================
2023-03-05 17:22:57,200:INFO:Initializing create_model()
2023-03-05 17:22:57,200:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CC03EB0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:22:57,200:INFO:Checking exceptions
2023-03-05 17:22:57,200:INFO:Importing libraries
2023-03-05 17:22:57,200:INFO:Copying training dataset
2023-03-05 17:22:57,210:INFO:Defining folds
2023-03-05 17:22:57,210:INFO:Declaring metric variables
2023-03-05 17:22:57,215:INFO:Importing untrained model
2023-03-05 17:22:57,224:INFO:Elastic Net Imported successfully
2023-03-05 17:22:57,238:INFO:Starting cross validation
2023-03-05 17:22:57,241:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:22:57,839:INFO:Calculating mean and std
2023-03-05 17:22:57,842:INFO:Creating metrics dataframe
2023-03-05 17:22:57,847:INFO:Uploading results into container
2023-03-05 17:22:57,849:INFO:Uploading model into container now
2023-03-05 17:22:57,849:INFO:_master_model_container: 26
2023-03-05 17:22:57,850:INFO:_display_container: 5
2023-03-05 17:22:57,851:INFO:ElasticNet(random_state=6697)
2023-03-05 17:22:57,851:INFO:create_model() successfully completed......................................
2023-03-05 17:22:57,956:INFO:SubProcess create_model() end ==================================
2023-03-05 17:22:57,956:INFO:Creating metrics dataframe
2023-03-05 17:22:57,968:INFO:Initializing Least Angle Regression
2023-03-05 17:22:57,969:INFO:Total runtime is 0.05472930272420247 minutes
2023-03-05 17:22:57,974:INFO:SubProcess create_model() called ==================================
2023-03-05 17:22:57,975:INFO:Initializing create_model()
2023-03-05 17:22:57,975:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CC03EB0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:22:57,975:INFO:Checking exceptions
2023-03-05 17:22:57,976:INFO:Importing libraries
2023-03-05 17:22:57,976:INFO:Copying training dataset
2023-03-05 17:22:57,986:INFO:Defining folds
2023-03-05 17:22:57,986:INFO:Declaring metric variables
2023-03-05 17:22:57,994:INFO:Importing untrained model
2023-03-05 17:22:58,000:INFO:Least Angle Regression Imported successfully
2023-03-05 17:22:58,015:INFO:Starting cross validation
2023-03-05 17:22:58,018:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:22:58,255:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:22:58,279:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:22:58,300:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:22:58,315:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:22:58,378:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:22:58,399:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:22:58,414:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:22:58,428:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:22:58,526:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:22:58,552:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:22:58,636:INFO:Calculating mean and std
2023-03-05 17:22:58,638:INFO:Creating metrics dataframe
2023-03-05 17:22:58,644:INFO:Uploading results into container
2023-03-05 17:22:58,645:INFO:Uploading model into container now
2023-03-05 17:22:58,646:INFO:_master_model_container: 27
2023-03-05 17:22:58,646:INFO:_display_container: 5
2023-03-05 17:22:58,646:INFO:Lars(random_state=6697)
2023-03-05 17:22:58,647:INFO:create_model() successfully completed......................................
2023-03-05 17:22:58,759:INFO:SubProcess create_model() end ==================================
2023-03-05 17:22:58,759:INFO:Creating metrics dataframe
2023-03-05 17:22:58,774:INFO:Initializing Lasso Least Angle Regression
2023-03-05 17:22:58,775:INFO:Total runtime is 0.06816515922546386 minutes
2023-03-05 17:22:58,779:INFO:SubProcess create_model() called ==================================
2023-03-05 17:22:58,779:INFO:Initializing create_model()
2023-03-05 17:22:58,780:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CC03EB0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:22:58,780:INFO:Checking exceptions
2023-03-05 17:22:58,780:INFO:Importing libraries
2023-03-05 17:22:58,780:INFO:Copying training dataset
2023-03-05 17:22:58,787:INFO:Defining folds
2023-03-05 17:22:58,787:INFO:Declaring metric variables
2023-03-05 17:22:58,792:INFO:Importing untrained model
2023-03-05 17:22:58,798:INFO:Lasso Least Angle Regression Imported successfully
2023-03-05 17:22:58,811:INFO:Starting cross validation
2023-03-05 17:22:58,814:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:22:59,060:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:22:59,097:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:22:59,126:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:22:59,160:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:22:59,231:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:22:59,235:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:22:59,251:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:22:59,383:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:22:59,414:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:22:59,501:INFO:Calculating mean and std
2023-03-05 17:22:59,503:INFO:Creating metrics dataframe
2023-03-05 17:22:59,508:INFO:Uploading results into container
2023-03-05 17:22:59,509:INFO:Uploading model into container now
2023-03-05 17:22:59,509:INFO:_master_model_container: 28
2023-03-05 17:22:59,510:INFO:_display_container: 5
2023-03-05 17:22:59,510:INFO:LassoLars(random_state=6697)
2023-03-05 17:22:59,511:INFO:create_model() successfully completed......................................
2023-03-05 17:22:59,627:INFO:SubProcess create_model() end ==================================
2023-03-05 17:22:59,627:INFO:Creating metrics dataframe
2023-03-05 17:22:59,640:INFO:Initializing Orthogonal Matching Pursuit
2023-03-05 17:22:59,640:INFO:Total runtime is 0.08258197704950967 minutes
2023-03-05 17:22:59,646:INFO:SubProcess create_model() called ==================================
2023-03-05 17:22:59,646:INFO:Initializing create_model()
2023-03-05 17:22:59,647:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CC03EB0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:22:59,647:INFO:Checking exceptions
2023-03-05 17:22:59,647:INFO:Importing libraries
2023-03-05 17:22:59,647:INFO:Copying training dataset
2023-03-05 17:22:59,658:INFO:Defining folds
2023-03-05 17:22:59,658:INFO:Declaring metric variables
2023-03-05 17:22:59,664:INFO:Importing untrained model
2023-03-05 17:22:59,669:INFO:Orthogonal Matching Pursuit Imported successfully
2023-03-05 17:22:59,683:INFO:Starting cross validation
2023-03-05 17:22:59,686:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:22:59,918:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:22:59,950:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:22:59,952:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:22:59,977:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:23:00,020:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:23:00,043:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:23:00,065:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:23:00,086:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:23:00,179:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:23:00,197:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:23:00,273:INFO:Calculating mean and std
2023-03-05 17:23:00,276:INFO:Creating metrics dataframe
2023-03-05 17:23:00,281:INFO:Uploading results into container
2023-03-05 17:23:00,282:INFO:Uploading model into container now
2023-03-05 17:23:00,282:INFO:_master_model_container: 29
2023-03-05 17:23:00,282:INFO:_display_container: 5
2023-03-05 17:23:00,283:INFO:OrthogonalMatchingPursuit()
2023-03-05 17:23:00,283:INFO:create_model() successfully completed......................................
2023-03-05 17:23:00,390:INFO:SubProcess create_model() end ==================================
2023-03-05 17:23:00,390:INFO:Creating metrics dataframe
2023-03-05 17:23:00,406:INFO:Initializing Bayesian Ridge
2023-03-05 17:23:00,406:INFO:Total runtime is 0.09533456563949584 minutes
2023-03-05 17:23:00,413:INFO:SubProcess create_model() called ==================================
2023-03-05 17:23:00,413:INFO:Initializing create_model()
2023-03-05 17:23:00,413:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CC03EB0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:23:00,414:INFO:Checking exceptions
2023-03-05 17:23:00,414:INFO:Importing libraries
2023-03-05 17:23:00,414:INFO:Copying training dataset
2023-03-05 17:23:00,421:INFO:Defining folds
2023-03-05 17:23:00,421:INFO:Declaring metric variables
2023-03-05 17:23:00,427:INFO:Importing untrained model
2023-03-05 17:23:00,434:INFO:Bayesian Ridge Imported successfully
2023-03-05 17:23:00,448:INFO:Starting cross validation
2023-03-05 17:23:00,450:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:23:01,124:INFO:Calculating mean and std
2023-03-05 17:23:01,126:INFO:Creating metrics dataframe
2023-03-05 17:23:01,130:INFO:Uploading results into container
2023-03-05 17:23:01,130:INFO:Uploading model into container now
2023-03-05 17:23:01,131:INFO:_master_model_container: 30
2023-03-05 17:23:01,131:INFO:_display_container: 5
2023-03-05 17:23:01,131:INFO:BayesianRidge()
2023-03-05 17:23:01,131:INFO:create_model() successfully completed......................................
2023-03-05 17:23:01,235:INFO:SubProcess create_model() end ==================================
2023-03-05 17:23:01,235:INFO:Creating metrics dataframe
2023-03-05 17:23:01,255:INFO:Initializing Passive Aggressive Regressor
2023-03-05 17:23:01,256:INFO:Total runtime is 0.10950177510579426 minutes
2023-03-05 17:23:01,264:INFO:SubProcess create_model() called ==================================
2023-03-05 17:23:01,265:INFO:Initializing create_model()
2023-03-05 17:23:01,266:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CC03EB0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:23:01,266:INFO:Checking exceptions
2023-03-05 17:23:01,266:INFO:Importing libraries
2023-03-05 17:23:01,266:INFO:Copying training dataset
2023-03-05 17:23:01,276:INFO:Defining folds
2023-03-05 17:23:01,277:INFO:Declaring metric variables
2023-03-05 17:23:01,282:INFO:Importing untrained model
2023-03-05 17:23:01,287:INFO:Passive Aggressive Regressor Imported successfully
2023-03-05 17:23:01,301:INFO:Starting cross validation
2023-03-05 17:23:01,303:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:23:01,981:INFO:Calculating mean and std
2023-03-05 17:23:01,984:INFO:Creating metrics dataframe
2023-03-05 17:23:01,989:INFO:Uploading results into container
2023-03-05 17:23:01,990:INFO:Uploading model into container now
2023-03-05 17:23:01,991:INFO:_master_model_container: 31
2023-03-05 17:23:01,992:INFO:_display_container: 5
2023-03-05 17:23:01,993:INFO:PassiveAggressiveRegressor(random_state=6697)
2023-03-05 17:23:01,994:INFO:create_model() successfully completed......................................
2023-03-05 17:23:02,104:INFO:SubProcess create_model() end ==================================
2023-03-05 17:23:02,104:INFO:Creating metrics dataframe
2023-03-05 17:23:02,120:INFO:Initializing Huber Regressor
2023-03-05 17:23:02,120:INFO:Total runtime is 0.12391473054885863 minutes
2023-03-05 17:23:02,125:INFO:SubProcess create_model() called ==================================
2023-03-05 17:23:02,125:INFO:Initializing create_model()
2023-03-05 17:23:02,125:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CC03EB0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:23:02,126:INFO:Checking exceptions
2023-03-05 17:23:02,126:INFO:Importing libraries
2023-03-05 17:23:02,126:INFO:Copying training dataset
2023-03-05 17:23:02,133:INFO:Defining folds
2023-03-05 17:23:02,134:INFO:Declaring metric variables
2023-03-05 17:23:02,138:INFO:Importing untrained model
2023-03-05 17:23:02,147:INFO:Huber Regressor Imported successfully
2023-03-05 17:23:02,159:INFO:Starting cross validation
2023-03-05 17:23:02,162:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:23:02,878:INFO:Calculating mean and std
2023-03-05 17:23:02,880:INFO:Creating metrics dataframe
2023-03-05 17:23:02,885:INFO:Uploading results into container
2023-03-05 17:23:02,886:INFO:Uploading model into container now
2023-03-05 17:23:02,887:INFO:_master_model_container: 32
2023-03-05 17:23:02,887:INFO:_display_container: 5
2023-03-05 17:23:02,888:INFO:HuberRegressor()
2023-03-05 17:23:02,888:INFO:create_model() successfully completed......................................
2023-03-05 17:23:03,000:INFO:SubProcess create_model() end ==================================
2023-03-05 17:23:03,000:INFO:Creating metrics dataframe
2023-03-05 17:23:03,014:INFO:Initializing K Neighbors Regressor
2023-03-05 17:23:03,014:INFO:Total runtime is 0.13881562550862628 minutes
2023-03-05 17:23:03,019:INFO:SubProcess create_model() called ==================================
2023-03-05 17:23:03,020:INFO:Initializing create_model()
2023-03-05 17:23:03,020:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CC03EB0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:23:03,020:INFO:Checking exceptions
2023-03-05 17:23:03,020:INFO:Importing libraries
2023-03-05 17:23:03,021:INFO:Copying training dataset
2023-03-05 17:23:03,031:INFO:Defining folds
2023-03-05 17:23:03,031:INFO:Declaring metric variables
2023-03-05 17:23:03,036:INFO:Importing untrained model
2023-03-05 17:23:03,041:INFO:K Neighbors Regressor Imported successfully
2023-03-05 17:23:03,054:INFO:Starting cross validation
2023-03-05 17:23:03,056:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:23:03,672:INFO:Calculating mean and std
2023-03-05 17:23:03,674:INFO:Creating metrics dataframe
2023-03-05 17:23:03,685:INFO:Uploading results into container
2023-03-05 17:23:03,687:INFO:Uploading model into container now
2023-03-05 17:23:03,688:INFO:_master_model_container: 33
2023-03-05 17:23:03,688:INFO:_display_container: 5
2023-03-05 17:23:03,689:INFO:KNeighborsRegressor(n_jobs=-1)
2023-03-05 17:23:03,689:INFO:create_model() successfully completed......................................
2023-03-05 17:23:03,821:INFO:SubProcess create_model() end ==================================
2023-03-05 17:23:03,822:INFO:Creating metrics dataframe
2023-03-05 17:23:03,839:INFO:Initializing Decision Tree Regressor
2023-03-05 17:23:03,840:INFO:Total runtime is 0.15257468620936074 minutes
2023-03-05 17:23:03,846:INFO:SubProcess create_model() called ==================================
2023-03-05 17:23:03,846:INFO:Initializing create_model()
2023-03-05 17:23:03,846:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CC03EB0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:23:03,846:INFO:Checking exceptions
2023-03-05 17:23:03,847:INFO:Importing libraries
2023-03-05 17:23:03,847:INFO:Copying training dataset
2023-03-05 17:23:03,864:INFO:Defining folds
2023-03-05 17:23:03,865:INFO:Declaring metric variables
2023-03-05 17:23:03,871:INFO:Importing untrained model
2023-03-05 17:23:03,877:INFO:Decision Tree Regressor Imported successfully
2023-03-05 17:23:03,897:INFO:Starting cross validation
2023-03-05 17:23:03,901:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:23:04,534:INFO:Calculating mean and std
2023-03-05 17:23:04,536:INFO:Creating metrics dataframe
2023-03-05 17:23:04,540:INFO:Uploading results into container
2023-03-05 17:23:04,540:INFO:Uploading model into container now
2023-03-05 17:23:04,541:INFO:_master_model_container: 34
2023-03-05 17:23:04,541:INFO:_display_container: 5
2023-03-05 17:23:04,541:INFO:DecisionTreeRegressor(random_state=6697)
2023-03-05 17:23:04,541:INFO:create_model() successfully completed......................................
2023-03-05 17:23:04,650:INFO:SubProcess create_model() end ==================================
2023-03-05 17:23:04,651:INFO:Creating metrics dataframe
2023-03-05 17:23:04,671:INFO:Initializing Random Forest Regressor
2023-03-05 17:23:04,671:INFO:Total runtime is 0.16643009185791013 minutes
2023-03-05 17:23:04,675:INFO:SubProcess create_model() called ==================================
2023-03-05 17:23:04,676:INFO:Initializing create_model()
2023-03-05 17:23:04,677:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CC03EB0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:23:04,677:INFO:Checking exceptions
2023-03-05 17:23:04,678:INFO:Importing libraries
2023-03-05 17:23:04,678:INFO:Copying training dataset
2023-03-05 17:23:04,688:INFO:Defining folds
2023-03-05 17:23:04,688:INFO:Declaring metric variables
2023-03-05 17:23:04,695:INFO:Importing untrained model
2023-03-05 17:23:04,700:INFO:Random Forest Regressor Imported successfully
2023-03-05 17:23:04,710:INFO:Starting cross validation
2023-03-05 17:23:04,717:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:23:05,774:INFO:Calculating mean and std
2023-03-05 17:23:05,777:INFO:Creating metrics dataframe
2023-03-05 17:23:05,783:INFO:Uploading results into container
2023-03-05 17:23:05,783:INFO:Uploading model into container now
2023-03-05 17:23:05,784:INFO:_master_model_container: 35
2023-03-05 17:23:05,784:INFO:_display_container: 5
2023-03-05 17:23:05,785:INFO:RandomForestRegressor(n_jobs=-1, random_state=6697)
2023-03-05 17:23:05,785:INFO:create_model() successfully completed......................................
2023-03-05 17:23:05,889:INFO:SubProcess create_model() end ==================================
2023-03-05 17:23:05,890:INFO:Creating metrics dataframe
2023-03-05 17:23:05,906:INFO:Initializing Extra Trees Regressor
2023-03-05 17:23:05,906:INFO:Total runtime is 0.18700049718221026 minutes
2023-03-05 17:23:05,910:INFO:SubProcess create_model() called ==================================
2023-03-05 17:23:05,910:INFO:Initializing create_model()
2023-03-05 17:23:05,911:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CC03EB0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:23:05,911:INFO:Checking exceptions
2023-03-05 17:23:05,911:INFO:Importing libraries
2023-03-05 17:23:05,911:INFO:Copying training dataset
2023-03-05 17:23:05,920:INFO:Defining folds
2023-03-05 17:23:05,920:INFO:Declaring metric variables
2023-03-05 17:23:05,925:INFO:Importing untrained model
2023-03-05 17:23:05,932:INFO:Extra Trees Regressor Imported successfully
2023-03-05 17:23:05,947:INFO:Starting cross validation
2023-03-05 17:23:05,950:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:23:06,973:INFO:Calculating mean and std
2023-03-05 17:23:06,975:INFO:Creating metrics dataframe
2023-03-05 17:23:06,980:INFO:Uploading results into container
2023-03-05 17:23:06,981:INFO:Uploading model into container now
2023-03-05 17:23:06,982:INFO:_master_model_container: 36
2023-03-05 17:23:06,982:INFO:_display_container: 5
2023-03-05 17:23:06,983:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=6697)
2023-03-05 17:23:06,984:INFO:create_model() successfully completed......................................
2023-03-05 17:23:07,093:INFO:SubProcess create_model() end ==================================
2023-03-05 17:23:07,093:INFO:Creating metrics dataframe
2023-03-05 17:23:07,109:INFO:Initializing AdaBoost Regressor
2023-03-05 17:23:07,109:INFO:Total runtime is 0.20705636342366535 minutes
2023-03-05 17:23:07,116:INFO:SubProcess create_model() called ==================================
2023-03-05 17:23:07,117:INFO:Initializing create_model()
2023-03-05 17:23:07,117:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CC03EB0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:23:07,117:INFO:Checking exceptions
2023-03-05 17:23:07,117:INFO:Importing libraries
2023-03-05 17:23:07,117:INFO:Copying training dataset
2023-03-05 17:23:07,123:INFO:Defining folds
2023-03-05 17:23:07,123:INFO:Declaring metric variables
2023-03-05 17:23:07,129:INFO:Importing untrained model
2023-03-05 17:23:07,134:INFO:AdaBoost Regressor Imported successfully
2023-03-05 17:23:07,144:INFO:Starting cross validation
2023-03-05 17:23:07,147:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:23:08,055:INFO:Calculating mean and std
2023-03-05 17:23:08,057:INFO:Creating metrics dataframe
2023-03-05 17:23:08,062:INFO:Uploading results into container
2023-03-05 17:23:08,063:INFO:Uploading model into container now
2023-03-05 17:23:08,064:INFO:_master_model_container: 37
2023-03-05 17:23:08,064:INFO:_display_container: 5
2023-03-05 17:23:08,064:INFO:AdaBoostRegressor(random_state=6697)
2023-03-05 17:23:08,064:INFO:create_model() successfully completed......................................
2023-03-05 17:23:08,186:INFO:SubProcess create_model() end ==================================
2023-03-05 17:23:08,187:INFO:Creating metrics dataframe
2023-03-05 17:23:08,207:INFO:Initializing Gradient Boosting Regressor
2023-03-05 17:23:08,207:INFO:Total runtime is 0.22535859743754066 minutes
2023-03-05 17:23:08,212:INFO:SubProcess create_model() called ==================================
2023-03-05 17:23:08,213:INFO:Initializing create_model()
2023-03-05 17:23:08,213:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CC03EB0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:23:08,213:INFO:Checking exceptions
2023-03-05 17:23:08,213:INFO:Importing libraries
2023-03-05 17:23:08,213:INFO:Copying training dataset
2023-03-05 17:23:08,220:INFO:Defining folds
2023-03-05 17:23:08,220:INFO:Declaring metric variables
2023-03-05 17:23:08,226:INFO:Importing untrained model
2023-03-05 17:23:08,233:INFO:Gradient Boosting Regressor Imported successfully
2023-03-05 17:23:08,247:INFO:Starting cross validation
2023-03-05 17:23:08,253:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:23:09,660:INFO:Calculating mean and std
2023-03-05 17:23:09,662:INFO:Creating metrics dataframe
2023-03-05 17:23:09,670:INFO:Uploading results into container
2023-03-05 17:23:09,671:INFO:Uploading model into container now
2023-03-05 17:23:09,671:INFO:_master_model_container: 38
2023-03-05 17:23:09,672:INFO:_display_container: 5
2023-03-05 17:23:09,672:INFO:GradientBoostingRegressor(random_state=6697)
2023-03-05 17:23:09,673:INFO:create_model() successfully completed......................................
2023-03-05 17:23:09,782:INFO:SubProcess create_model() end ==================================
2023-03-05 17:23:09,782:INFO:Creating metrics dataframe
2023-03-05 17:23:09,799:INFO:Initializing Extreme Gradient Boosting
2023-03-05 17:23:09,799:INFO:Total runtime is 0.25189105669657386 minutes
2023-03-05 17:23:09,804:INFO:SubProcess create_model() called ==================================
2023-03-05 17:23:09,804:INFO:Initializing create_model()
2023-03-05 17:23:09,804:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CC03EB0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:23:09,805:INFO:Checking exceptions
2023-03-05 17:23:09,805:INFO:Importing libraries
2023-03-05 17:23:09,805:INFO:Copying training dataset
2023-03-05 17:23:09,812:INFO:Defining folds
2023-03-05 17:23:09,812:INFO:Declaring metric variables
2023-03-05 17:23:09,818:INFO:Importing untrained model
2023-03-05 17:23:09,826:INFO:Extreme Gradient Boosting Imported successfully
2023-03-05 17:23:09,839:INFO:Starting cross validation
2023-03-05 17:23:09,842:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:23:10,544:INFO:Calculating mean and std
2023-03-05 17:23:10,546:INFO:Creating metrics dataframe
2023-03-05 17:23:10,551:INFO:Uploading results into container
2023-03-05 17:23:10,553:INFO:Uploading model into container now
2023-03-05 17:23:10,553:INFO:_master_model_container: 39
2023-03-05 17:23:10,553:INFO:_display_container: 5
2023-03-05 17:23:10,554:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=6697, ...)
2023-03-05 17:23:10,555:INFO:create_model() successfully completed......................................
2023-03-05 17:23:10,661:INFO:SubProcess create_model() end ==================================
2023-03-05 17:23:10,661:INFO:Creating metrics dataframe
2023-03-05 17:23:10,679:INFO:Initializing Light Gradient Boosting Machine
2023-03-05 17:23:10,679:INFO:Total runtime is 0.26656314134597775 minutes
2023-03-05 17:23:10,683:INFO:SubProcess create_model() called ==================================
2023-03-05 17:23:10,684:INFO:Initializing create_model()
2023-03-05 17:23:10,684:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CC03EB0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:23:10,684:INFO:Checking exceptions
2023-03-05 17:23:10,684:INFO:Importing libraries
2023-03-05 17:23:10,684:INFO:Copying training dataset
2023-03-05 17:23:10,693:INFO:Defining folds
2023-03-05 17:23:10,693:INFO:Declaring metric variables
2023-03-05 17:23:10,698:INFO:Importing untrained model
2023-03-05 17:23:10,706:INFO:Light Gradient Boosting Machine Imported successfully
2023-03-05 17:23:10,716:INFO:Starting cross validation
2023-03-05 17:23:10,718:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:23:11,414:INFO:Calculating mean and std
2023-03-05 17:23:11,416:INFO:Creating metrics dataframe
2023-03-05 17:23:11,422:INFO:Uploading results into container
2023-03-05 17:23:11,423:INFO:Uploading model into container now
2023-03-05 17:23:11,423:INFO:_master_model_container: 40
2023-03-05 17:23:11,423:INFO:_display_container: 5
2023-03-05 17:23:11,424:INFO:LGBMRegressor(random_state=6697)
2023-03-05 17:23:11,424:INFO:create_model() successfully completed......................................
2023-03-05 17:23:11,538:INFO:SubProcess create_model() end ==================================
2023-03-05 17:23:11,538:INFO:Creating metrics dataframe
2023-03-05 17:23:11,554:INFO:Initializing Dummy Regressor
2023-03-05 17:23:11,554:INFO:Total runtime is 0.2811439196268717 minutes
2023-03-05 17:23:11,559:INFO:SubProcess create_model() called ==================================
2023-03-05 17:23:11,559:INFO:Initializing create_model()
2023-03-05 17:23:11,559:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001537CC03EB0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:23:11,560:INFO:Checking exceptions
2023-03-05 17:23:11,560:INFO:Importing libraries
2023-03-05 17:23:11,560:INFO:Copying training dataset
2023-03-05 17:23:11,566:INFO:Defining folds
2023-03-05 17:23:11,567:INFO:Declaring metric variables
2023-03-05 17:23:11,574:INFO:Importing untrained model
2023-03-05 17:23:11,581:INFO:Dummy Regressor Imported successfully
2023-03-05 17:23:11,595:INFO:Starting cross validation
2023-03-05 17:23:11,597:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:23:12,150:INFO:Calculating mean and std
2023-03-05 17:23:12,152:INFO:Creating metrics dataframe
2023-03-05 17:23:12,157:INFO:Uploading results into container
2023-03-05 17:23:12,158:INFO:Uploading model into container now
2023-03-05 17:23:12,159:INFO:_master_model_container: 41
2023-03-05 17:23:12,159:INFO:_display_container: 5
2023-03-05 17:23:12,159:INFO:DummyRegressor()
2023-03-05 17:23:12,159:INFO:create_model() successfully completed......................................
2023-03-05 17:23:12,270:INFO:SubProcess create_model() end ==================================
2023-03-05 17:23:12,270:INFO:Creating metrics dataframe
2023-03-05 17:23:12,301:INFO:Initializing create_model()
2023-03-05 17:23:12,301:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=Ridge(random_state=6697), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:23:12,302:INFO:Checking exceptions
2023-03-05 17:23:12,304:INFO:Importing libraries
2023-03-05 17:23:12,304:INFO:Copying training dataset
2023-03-05 17:23:12,309:INFO:Defining folds
2023-03-05 17:23:12,309:INFO:Declaring metric variables
2023-03-05 17:23:12,309:INFO:Importing untrained model
2023-03-05 17:23:12,309:INFO:Declaring custom model
2023-03-05 17:23:12,310:INFO:Ridge Regression Imported successfully
2023-03-05 17:23:12,312:INFO:Cross validation set to False
2023-03-05 17:23:12,312:INFO:Fitting Model
2023-03-05 17:23:12,483:INFO:Ridge(random_state=6697)
2023-03-05 17:23:12,483:INFO:create_model() successfully completed......................................
2023-03-05 17:23:12,669:INFO:_master_model_container: 41
2023-03-05 17:23:12,670:INFO:_display_container: 5
2023-03-05 17:23:12,670:INFO:Ridge(random_state=6697)
2023-03-05 17:23:12,671:INFO:compare_models() successfully completed......................................
2023-03-05 17:23:12,694:INFO:Initializing evaluate_model()
2023-03-05 17:23:12,695:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=Ridge(random_state=6697), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-03-05 17:23:12,739:INFO:Initializing plot_model()
2023-03-05 17:23:12,739:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=Ridge(random_state=6697), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, system=True)
2023-03-05 17:23:12,740:INFO:Checking exceptions
2023-03-05 17:23:12,745:INFO:Preloading libraries
2023-03-05 17:23:12,746:INFO:Copying training dataset
2023-03-05 17:23:12,746:INFO:Plot type: pipeline
2023-03-05 17:23:12,996:INFO:Visual Rendered Successfully
2023-03-05 17:23:13,155:INFO:plot_model() successfully completed......................................
2023-03-05 17:23:13,177:INFO:Initializing predict_model()
2023-03-05 17:23:13,177:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=Ridge(random_state=6697), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x0000015300303F70>)
2023-03-05 17:23:13,178:INFO:Checking exceptions
2023-03-05 17:23:13,178:INFO:Preloading libraries
2023-03-05 17:23:13,181:INFO:Set up data.
2023-03-05 17:23:13,196:INFO:Set up index.
2023-03-05 17:23:37,238:INFO:Initializing plot_model()
2023-03-05 17:23:37,238:INFO:plot_model(plot=error, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=Ridge(random_state=6697), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, system=True)
2023-03-05 17:23:37,239:INFO:Checking exceptions
2023-03-05 17:23:37,240:INFO:Preloading libraries
2023-03-05 17:23:37,241:INFO:Copying training dataset
2023-03-05 17:23:37,241:INFO:Plot type: error
2023-03-05 17:23:37,423:INFO:Fitting Model
2023-03-05 17:23:37,424:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\base.py:450: UserWarning: X does not have valid feature names, but Ridge was fitted with feature names
  warnings.warn(

2023-03-05 17:23:37,424:INFO:Scoring test/hold-out set
2023-03-05 17:23:38,097:INFO:Visual Rendered Successfully
2023-03-05 17:23:38,199:INFO:plot_model() successfully completed......................................
2023-03-05 17:23:50,907:INFO:Initializing plot_model()
2023-03-05 17:23:50,909:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=Ridge(random_state=6697), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, system=True)
2023-03-05 17:23:50,909:INFO:Checking exceptions
2023-03-05 17:23:50,912:INFO:Preloading libraries
2023-03-05 17:23:50,912:INFO:Copying training dataset
2023-03-05 17:23:50,912:INFO:Plot type: pipeline
2023-03-05 17:23:51,003:INFO:Visual Rendered Successfully
2023-03-05 17:23:51,103:INFO:plot_model() successfully completed......................................
2023-03-05 17:23:51,991:INFO:Initializing plot_model()
2023-03-05 17:23:51,991:INFO:plot_model(plot=cooks, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=Ridge(random_state=6697), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, system=True)
2023-03-05 17:23:51,992:INFO:Checking exceptions
2023-03-05 17:23:51,995:INFO:Preloading libraries
2023-03-05 17:23:51,996:INFO:Copying training dataset
2023-03-05 17:23:51,996:INFO:Plot type: cooks
2023-03-05 17:23:52,077:INFO:Fitting Model
2023-03-05 17:23:52,337:INFO:Visual Rendered Successfully
2023-03-05 17:23:52,455:INFO:plot_model() successfully completed......................................
2023-03-05 17:23:53,383:INFO:Initializing plot_model()
2023-03-05 17:23:53,383:INFO:plot_model(plot=vc, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=Ridge(random_state=6697), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, system=True)
2023-03-05 17:23:53,383:INFO:Checking exceptions
2023-03-05 17:23:53,386:INFO:Preloading libraries
2023-03-05 17:23:53,386:INFO:Copying training dataset
2023-03-05 17:23:53,386:INFO:Plot type: vc
2023-03-05 17:23:53,387:INFO:Determining param_name
2023-03-05 17:23:53,387:INFO:param_name: alpha
2023-03-05 17:23:53,507:INFO:Fitting Model
2023-03-05 17:23:53,635:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=6.44423e-27): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-05 17:23:53,963:INFO:Visual Rendered Successfully
2023-03-05 17:23:54,065:INFO:plot_model() successfully completed......................................
2023-03-05 17:24:01,535:INFO:Initializing plot_model()
2023-03-05 17:24:01,535:INFO:plot_model(plot=parameter, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=Ridge(random_state=6697), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, system=True)
2023-03-05 17:24:01,535:INFO:Checking exceptions
2023-03-05 17:24:01,539:INFO:Preloading libraries
2023-03-05 17:24:01,539:INFO:Copying training dataset
2023-03-05 17:24:01,539:INFO:Plot type: parameter
2023-03-05 17:24:01,543:INFO:Visual Rendered Successfully
2023-03-05 17:24:01,659:INFO:plot_model() successfully completed......................................
2023-03-05 17:24:09,321:INFO:Initializing plot_model()
2023-03-05 17:24:09,322:INFO:plot_model(plot=rfe, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=Ridge(random_state=6697), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, system=True)
2023-03-05 17:24:09,322:INFO:Checking exceptions
2023-03-05 17:24:09,324:INFO:Preloading libraries
2023-03-05 17:24:09,324:INFO:Copying training dataset
2023-03-05 17:24:09,325:INFO:Plot type: rfe
2023-03-05 17:24:09,406:INFO:Fitting Model
2023-03-05 17:24:10,785:INFO:Visual Rendered Successfully
2023-03-05 17:24:10,903:INFO:plot_model() successfully completed......................................
2023-03-05 17:24:10,912:INFO:Initializing plot_model()
2023-03-05 17:24:10,912:INFO:plot_model(plot=feature, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=Ridge(random_state=6697), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, system=True)
2023-03-05 17:24:10,912:INFO:Checking exceptions
2023-03-05 17:24:10,915:INFO:Preloading libraries
2023-03-05 17:24:10,916:INFO:Copying training dataset
2023-03-05 17:24:10,916:INFO:Plot type: feature
2023-03-05 17:24:11,093:INFO:Visual Rendered Successfully
2023-03-05 17:24:11,208:INFO:plot_model() successfully completed......................................
2023-03-05 17:24:22,498:INFO:Initializing plot_model()
2023-03-05 17:24:22,499:INFO:plot_model(plot=residuals, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=Ridge(random_state=6697), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, system=True)
2023-03-05 17:24:22,499:INFO:Checking exceptions
2023-03-05 17:24:22,502:INFO:Preloading libraries
2023-03-05 17:24:22,502:INFO:Copying training dataset
2023-03-05 17:24:22,503:INFO:Plot type: residuals
2023-03-05 17:24:22,628:INFO:Fitting Model
2023-03-05 17:24:22,628:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\base.py:450: UserWarning: X does not have valid feature names, but Ridge was fitted with feature names
  warnings.warn(

2023-03-05 17:24:22,666:INFO:Scoring test/hold-out set
2023-03-05 17:24:23,077:INFO:Visual Rendered Successfully
2023-03-05 17:24:23,195:INFO:plot_model() successfully completed......................................
2023-03-05 17:24:26,744:INFO:Initializing plot_model()
2023-03-05 17:24:26,744:INFO:plot_model(plot=learning, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=Ridge(random_state=6697), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, system=True)
2023-03-05 17:24:26,744:INFO:Checking exceptions
2023-03-05 17:24:26,747:INFO:Preloading libraries
2023-03-05 17:24:26,747:INFO:Copying training dataset
2023-03-05 17:24:26,747:INFO:Plot type: learning
2023-03-05 17:24:26,830:INFO:Fitting Model
2023-03-05 17:24:27,292:INFO:Visual Rendered Successfully
2023-03-05 17:24:27,400:INFO:plot_model() successfully completed......................................
2023-03-05 17:24:30,648:INFO:Initializing plot_model()
2023-03-05 17:24:30,648:INFO:plot_model(plot=feature_all, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=Ridge(random_state=6697), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, system=True)
2023-03-05 17:24:30,649:INFO:Checking exceptions
2023-03-05 17:24:30,652:INFO:Preloading libraries
2023-03-05 17:24:30,652:INFO:Copying training dataset
2023-03-05 17:24:30,652:INFO:Plot type: feature_all
2023-03-05 17:24:30,870:INFO:Visual Rendered Successfully
2023-03-05 17:24:30,963:INFO:plot_model() successfully completed......................................
2023-03-05 17:24:34,810:INFO:Initializing plot_model()
2023-03-05 17:24:34,810:INFO:plot_model(plot=tree, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=Ridge(random_state=6697), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, system=True)
2023-03-05 17:24:34,810:INFO:Checking exceptions
2023-03-05 17:24:35,847:INFO:Initializing plot_model()
2023-03-05 17:24:35,848:INFO:plot_model(plot=manifold, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=Ridge(random_state=6697), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, system=True)
2023-03-05 17:24:35,848:INFO:Checking exceptions
2023-03-05 17:24:35,850:INFO:Preloading libraries
2023-03-05 17:24:35,850:INFO:Copying training dataset
2023-03-05 17:24:35,851:INFO:Plot type: manifold
2023-03-05 17:24:35,989:INFO:Fitting & Transforming Model
2023-03-05 17:24:35,990:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\manifold\_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.
  warnings.warn(

2023-03-05 17:24:36,048:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\manifold\_t_sne.py:982: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.
  warnings.warn(

2023-03-05 17:24:37,811:INFO:Visual Rendered Successfully
2023-03-05 17:24:37,908:INFO:plot_model() successfully completed......................................
2023-03-05 17:24:37,919:INFO:Initializing plot_model()
2023-03-05 17:24:37,920:INFO:plot_model(plot=error, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=Ridge(random_state=6697), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, system=True)
2023-03-05 17:24:37,920:INFO:Checking exceptions
2023-03-05 17:24:37,925:INFO:Preloading libraries
2023-03-05 17:24:37,925:INFO:Copying training dataset
2023-03-05 17:24:37,925:INFO:Plot type: error
2023-03-05 17:24:38,013:INFO:Fitting Model
2023-03-05 17:24:38,014:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\base.py:450: UserWarning: X does not have valid feature names, but Ridge was fitted with feature names
  warnings.warn(

2023-03-05 17:24:38,014:INFO:Scoring test/hold-out set
2023-03-05 17:24:38,294:INFO:Visual Rendered Successfully
2023-03-05 17:24:38,394:INFO:plot_model() successfully completed......................................
2023-03-05 17:24:38,409:INFO:Initializing plot_model()
2023-03-05 17:24:38,410:INFO:plot_model(plot=residuals, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=Ridge(random_state=6697), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, system=True)
2023-03-05 17:24:38,410:INFO:Checking exceptions
2023-03-05 17:24:38,413:INFO:Preloading libraries
2023-03-05 17:24:38,414:INFO:Copying training dataset
2023-03-05 17:24:38,414:INFO:Plot type: residuals
2023-03-05 17:24:38,517:INFO:Fitting Model
2023-03-05 17:24:38,517:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\base.py:450: UserWarning: X does not have valid feature names, but Ridge was fitted with feature names
  warnings.warn(

2023-03-05 17:24:38,545:INFO:Scoring test/hold-out set
2023-03-05 17:24:38,909:INFO:Visual Rendered Successfully
2023-03-05 17:24:39,017:INFO:plot_model() successfully completed......................................
2023-03-05 17:24:39,034:INFO:Initializing plot_model()
2023-03-05 17:24:39,034:INFO:plot_model(plot=error, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=Ridge(random_state=6697), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, system=True)
2023-03-05 17:24:39,034:INFO:Checking exceptions
2023-03-05 17:24:39,037:INFO:Preloading libraries
2023-03-05 17:24:39,040:INFO:Copying training dataset
2023-03-05 17:24:39,041:INFO:Plot type: error
2023-03-05 17:24:39,121:INFO:Fitting Model
2023-03-05 17:24:39,122:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\base.py:450: UserWarning: X does not have valid feature names, but Ridge was fitted with feature names
  warnings.warn(

2023-03-05 17:24:39,123:INFO:Scoring test/hold-out set
2023-03-05 17:24:39,352:INFO:Visual Rendered Successfully
2023-03-05 17:24:39,452:INFO:plot_model() successfully completed......................................
2023-03-05 17:25:22,888:INFO:Initializing evaluate_model()
2023-03-05 17:25:22,889:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=DecisionTreeRegressor(criterion='friedman_mse', max_depth=16,
                      max_features='sqrt', min_impurity_decrease=0.005,
                      min_samples_leaf=2, min_samples_split=9,
                      random_state=6697), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-03-05 17:25:22,914:INFO:Initializing plot_model()
2023-03-05 17:25:22,914:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=DecisionTreeRegressor(criterion='friedman_mse', max_depth=16,
                      max_features='sqrt', min_impurity_decrease=0.005,
                      min_samples_leaf=2, min_samples_split=9,
                      random_state=6697), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, system=True)
2023-03-05 17:25:22,914:INFO:Checking exceptions
2023-03-05 17:25:22,921:INFO:Preloading libraries
2023-03-05 17:25:22,921:INFO:Copying training dataset
2023-03-05 17:25:22,922:INFO:Plot type: pipeline
2023-03-05 17:25:23,027:INFO:Visual Rendered Successfully
2023-03-05 17:25:23,147:INFO:plot_model() successfully completed......................................
2023-03-05 17:25:25,448:INFO:Initializing plot_model()
2023-03-05 17:25:25,448:INFO:plot_model(plot=error, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=DecisionTreeRegressor(criterion='friedman_mse', max_depth=16,
                      max_features='sqrt', min_impurity_decrease=0.005,
                      min_samples_leaf=2, min_samples_split=9,
                      random_state=6697), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, system=True)
2023-03-05 17:25:25,448:INFO:Checking exceptions
2023-03-05 17:25:25,450:INFO:Preloading libraries
2023-03-05 17:25:25,451:INFO:Copying training dataset
2023-03-05 17:25:25,451:INFO:Plot type: error
2023-03-05 17:25:25,538:INFO:Fitting Model
2023-03-05 17:25:25,538:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\base.py:450: UserWarning: X does not have valid feature names, but DecisionTreeRegressor was fitted with feature names
  warnings.warn(

2023-03-05 17:25:25,538:INFO:Scoring test/hold-out set
2023-03-05 17:25:25,776:INFO:Visual Rendered Successfully
2023-03-05 17:25:25,900:INFO:plot_model() successfully completed......................................
2023-03-05 17:25:34,801:INFO:Initializing predict_model()
2023-03-05 17:25:34,801:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001537CDD1A30>, estimator=DecisionTreeRegressor(criterion='friedman_mse', max_depth=16,
                      max_features='sqrt', min_impurity_decrease=0.005,
                      min_samples_leaf=2, min_samples_split=9,
                      random_state=6697), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x000001537D22CF70>)
2023-03-05 17:25:34,802:INFO:Checking exceptions
2023-03-05 17:25:34,802:INFO:Preloading libraries
2023-03-05 17:25:34,805:INFO:Set up data.
2023-03-05 17:25:34,815:INFO:Set up index.
2023-03-05 17:34:20,707:INFO:PyCaret RegressionExperiment
2023-03-05 17:34:20,707:INFO:Logging name: reg-default-name
2023-03-05 17:34:20,708:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-05 17:34:20,708:INFO:version 3.0.0.rc9
2023-03-05 17:34:20,708:INFO:Initializing setup()
2023-03-05 17:34:20,708:INFO:self.USI: d590
2023-03-05 17:34:20,708:INFO:self._variable_keys: {'fold_shuffle_param', '_available_plots', '_ml_usecase', 'exp_id', 'exp_name_log', 'X_train', 'html_param', 'fold_generator', 'n_jobs_param', 'logging_param', 'X_test', 'gpu_param', 'seed', 'y_train', 'data', 'pipeline', 'memory', 'log_plots_param', 'X', 'fold_groups_param', 'transform_target_param', 'USI', 'idx', 'y_test', 'target_param', 'y', 'gpu_n_jobs_param'}
2023-03-05 17:34:20,708:INFO:Checking environment
2023-03-05 17:34:20,708:INFO:python_version: 3.9.13
2023-03-05 17:34:20,708:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-03-05 17:34:20,709:INFO:machine: AMD64
2023-03-05 17:34:20,709:INFO:platform: Windows-10-10.0.19044-SP0
2023-03-05 17:34:20,709:INFO:Memory: svmem(total=17009516544, available=6553157632, percent=61.5, used=10456358912, free=6553157632)
2023-03-05 17:34:20,709:INFO:Physical Core: 4
2023-03-05 17:34:20,709:INFO:Logical Core: 8
2023-03-05 17:34:20,709:INFO:Checking libraries
2023-03-05 17:34:20,710:INFO:System:
2023-03-05 17:34:20,710:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-03-05 17:34:20,710:INFO:executable: C:\Users\Alvaro\anaconda3\python.exe
2023-03-05 17:34:20,710:INFO:   machine: Windows-10-10.0.19044-SP0
2023-03-05 17:34:20,710:INFO:PyCaret required dependencies:
2023-03-05 17:34:20,710:INFO:                 pip: 22.2.2
2023-03-05 17:34:20,710:INFO:          setuptools: 63.4.1
2023-03-05 17:34:20,710:INFO:             pycaret: 3.0.0rc9
2023-03-05 17:34:20,710:INFO:             IPython: 7.31.1
2023-03-05 17:34:20,710:INFO:          ipywidgets: 7.6.5
2023-03-05 17:34:20,711:INFO:                tqdm: 4.64.1
2023-03-05 17:34:20,711:INFO:               numpy: 1.21.5
2023-03-05 17:34:20,711:INFO:              pandas: 1.4.4
2023-03-05 17:34:20,711:INFO:              jinja2: 2.11.3
2023-03-05 17:34:20,711:INFO:               scipy: 1.9.1
2023-03-05 17:34:20,711:INFO:              joblib: 1.2.0
2023-03-05 17:34:20,711:INFO:             sklearn: 1.0.2
2023-03-05 17:34:20,711:INFO:                pyod: 1.0.7
2023-03-05 17:34:20,711:INFO:            imblearn: 0.10.1
2023-03-05 17:34:20,711:INFO:   category_encoders: 2.6.0
2023-03-05 17:34:20,711:INFO:            lightgbm: 3.3.5
2023-03-05 17:34:20,711:INFO:               numba: 0.55.1
2023-03-05 17:34:20,711:INFO:            requests: 2.28.1
2023-03-05 17:34:20,711:INFO:          matplotlib: 3.5.2
2023-03-05 17:34:20,711:INFO:          scikitplot: 0.3.7
2023-03-05 17:34:20,711:INFO:         yellowbrick: 1.5
2023-03-05 17:34:20,711:INFO:              plotly: 5.9.0
2023-03-05 17:34:20,711:INFO:             kaleido: 0.2.1
2023-03-05 17:34:20,711:INFO:         statsmodels: 0.13.2
2023-03-05 17:34:20,711:INFO:              sktime: 0.16.1
2023-03-05 17:34:20,711:INFO:               tbats: 1.1.2
2023-03-05 17:34:20,711:INFO:            pmdarima: 2.0.2
2023-03-05 17:34:20,712:INFO:              psutil: 5.9.0
2023-03-05 17:34:20,712:INFO:PyCaret optional dependencies:
2023-03-05 17:34:20,712:INFO:                shap: Not installed
2023-03-05 17:34:20,712:INFO:           interpret: Not installed
2023-03-05 17:34:20,712:INFO:                umap: Not installed
2023-03-05 17:34:20,712:INFO:    pandas_profiling: Not installed
2023-03-05 17:34:20,712:INFO:  explainerdashboard: Not installed
2023-03-05 17:34:20,712:INFO:             autoviz: Not installed
2023-03-05 17:34:20,712:INFO:           fairlearn: Not installed
2023-03-05 17:34:20,712:INFO:             xgboost: 1.7.4
2023-03-05 17:34:20,712:INFO:            catboost: Not installed
2023-03-05 17:34:20,712:INFO:              kmodes: Not installed
2023-03-05 17:34:20,712:INFO:             mlxtend: Not installed
2023-03-05 17:34:20,712:INFO:       statsforecast: Not installed
2023-03-05 17:34:20,712:INFO:        tune_sklearn: Not installed
2023-03-05 17:34:20,712:INFO:                 ray: Not installed
2023-03-05 17:34:20,712:INFO:            hyperopt: Not installed
2023-03-05 17:34:20,712:INFO:              optuna: Not installed
2023-03-05 17:34:20,712:INFO:               skopt: Not installed
2023-03-05 17:34:20,712:INFO:              mlflow: Not installed
2023-03-05 17:34:20,712:INFO:              gradio: Not installed
2023-03-05 17:34:20,712:INFO:             fastapi: Not installed
2023-03-05 17:34:20,712:INFO:             uvicorn: Not installed
2023-03-05 17:34:20,713:INFO:              m2cgen: Not installed
2023-03-05 17:34:20,713:INFO:           evidently: Not installed
2023-03-05 17:34:20,713:INFO:               fugue: Not installed
2023-03-05 17:34:20,713:INFO:           streamlit: Not installed
2023-03-05 17:34:20,713:INFO:             prophet: Not installed
2023-03-05 17:34:20,713:INFO:None
2023-03-05 17:34:20,713:INFO:Set up data.
2023-03-05 17:34:20,728:INFO:Set up train/test split.
2023-03-05 17:34:20,734:INFO:Set up index.
2023-03-05 17:34:20,735:INFO:Set up folding strategy.
2023-03-05 17:34:20,735:INFO:Assigning column types.
2023-03-05 17:34:20,741:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-05 17:34:20,743:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-05 17:34:20,748:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 17:34:20,753:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:34:20,808:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:34:20,855:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:34:20,859:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:34:20,861:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:34:20,862:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-05 17:34:20,866:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 17:34:20,870:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:34:20,921:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:34:20,961:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:34:20,962:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:34:20,965:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:34:20,965:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-03-05 17:34:20,971:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 17:34:20,977:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:34:21,036:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:34:21,074:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:34:21,075:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:34:21,077:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:34:21,082:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 17:34:21,086:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:34:21,142:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:34:21,183:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:34:21,184:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:34:21,186:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:34:21,186:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-03-05 17:34:21,194:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:34:21,244:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:34:21,291:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:34:21,292:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:34:21,294:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:34:21,304:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:34:21,356:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:34:21,395:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:34:21,395:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:34:21,398:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:34:21,398:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-03-05 17:34:21,461:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:34:21,503:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:34:21,503:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:34:21,506:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:34:21,565:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:34:21,614:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:34:21,615:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:34:21,617:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:34:21,618:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-05 17:34:21,677:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:34:21,730:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:34:21,733:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:34:21,793:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:34:21,832:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:34:21,836:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:34:21,837:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-03-05 17:34:21,954:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:34:21,957:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:34:22,072:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:34:22,075:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:34:22,077:INFO:Preparing preprocessing pipeline...
2023-03-05 17:34:22,077:INFO:Set up simple imputation.
2023-03-05 17:34:22,080:INFO:Set up encoding of categorical features.
2023-03-05 17:34:22,183:INFO:Finished creating preprocessing pipeline.
2023-03-05 17:34:22,192:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Alvaro\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['work_year', 'remote_ratio'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['experience_level',
                                             'employment_type', 'job_title',
                                             'employee_residence',
                                             'company_location',
                                             'company_size'],
                                    t...
                                    transformer=OneHotEncoder(cols=['experience_level',
                                                                    'employment_type',
                                                                    'company_size'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['job_title', 'employee_residence',
                                             'company_location'],
                                    transformer=LeaveOneOutEncoder(cols=['job_title',
                                                                         'employee_residence',
                                                                         'company_location'],
                                                                   handle_missing='return_nan',
                                                                   random_state=4313)))])
2023-03-05 17:34:22,192:INFO:Creating final display dataframe.
2023-03-05 17:34:22,612:INFO:Setup _display_container:                     Description             Value
0                    Session id              4313
1                        Target     salary_in_usd
2                   Target type        Regression
3           Original data shape          (500, 9)
4        Transformed data shape         (500, 17)
5   Transformed train set shape         (350, 17)
6    Transformed test set shape         (150, 17)
7              Numeric features                 2
8          Categorical features                 6
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator             KFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  reg-default-name
21                          USI              d590
2023-03-05 17:34:22,730:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:34:22,732:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:34:22,830:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:34:22,832:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:34:22,834:INFO:setup() successfully completed in 2.14s...............
2023-03-05 17:34:22,843:INFO:Initializing compare_models()
2023-03-05 17:34:22,843:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-03-05 17:34:22,843:INFO:Checking exceptions
2023-03-05 17:34:22,846:INFO:Preparing display monitor
2023-03-05 17:34:22,888:INFO:Initializing Linear Regression
2023-03-05 17:34:22,888:INFO:Total runtime is 0.0 minutes
2023-03-05 17:34:22,895:INFO:SubProcess create_model() called ==================================
2023-03-05 17:34:22,896:INFO:Initializing create_model()
2023-03-05 17:34:22,896:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A500A95E0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:34:22,896:INFO:Checking exceptions
2023-03-05 17:34:22,896:INFO:Importing libraries
2023-03-05 17:34:22,896:INFO:Copying training dataset
2023-03-05 17:34:22,901:INFO:Defining folds
2023-03-05 17:34:22,902:INFO:Declaring metric variables
2023-03-05 17:34:22,908:INFO:Importing untrained model
2023-03-05 17:34:22,912:INFO:Linear Regression Imported successfully
2023-03-05 17:34:22,920:INFO:Starting cross validation
2023-03-05 17:34:22,922:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:34:31,535:INFO:Calculating mean and std
2023-03-05 17:34:31,536:INFO:Creating metrics dataframe
2023-03-05 17:34:31,542:INFO:Uploading results into container
2023-03-05 17:34:31,543:INFO:Uploading model into container now
2023-03-05 17:34:31,544:INFO:_master_model_container: 1
2023-03-05 17:34:31,544:INFO:_display_container: 2
2023-03-05 17:34:31,544:INFO:LinearRegression(n_jobs=-1)
2023-03-05 17:34:31,544:INFO:create_model() successfully completed......................................
2023-03-05 17:34:31,689:INFO:SubProcess create_model() end ==================================
2023-03-05 17:34:31,689:INFO:Creating metrics dataframe
2023-03-05 17:34:31,698:INFO:Initializing Lasso Regression
2023-03-05 17:34:31,698:INFO:Total runtime is 0.14682626326878864 minutes
2023-03-05 17:34:31,702:INFO:SubProcess create_model() called ==================================
2023-03-05 17:34:31,702:INFO:Initializing create_model()
2023-03-05 17:34:31,702:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A500A95E0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:34:31,702:INFO:Checking exceptions
2023-03-05 17:34:31,703:INFO:Importing libraries
2023-03-05 17:34:31,703:INFO:Copying training dataset
2023-03-05 17:34:31,710:INFO:Defining folds
2023-03-05 17:34:31,710:INFO:Declaring metric variables
2023-03-05 17:34:31,715:INFO:Importing untrained model
2023-03-05 17:34:31,721:INFO:Lasso Regression Imported successfully
2023-03-05 17:34:31,729:INFO:Starting cross validation
2023-03-05 17:34:31,732:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:34:32,069:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.275e+09, tolerance: 1.570e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-05 17:34:32,088:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.249e+09, tolerance: 1.533e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-05 17:34:32,104:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.372e+09, tolerance: 1.529e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-05 17:34:32,549:INFO:Calculating mean and std
2023-03-05 17:34:32,550:INFO:Creating metrics dataframe
2023-03-05 17:34:32,553:INFO:Uploading results into container
2023-03-05 17:34:32,554:INFO:Uploading model into container now
2023-03-05 17:34:32,554:INFO:_master_model_container: 2
2023-03-05 17:34:32,554:INFO:_display_container: 2
2023-03-05 17:34:32,555:INFO:Lasso(random_state=4313)
2023-03-05 17:34:32,555:INFO:create_model() successfully completed......................................
2023-03-05 17:34:32,630:INFO:SubProcess create_model() end ==================================
2023-03-05 17:34:32,630:INFO:Creating metrics dataframe
2023-03-05 17:34:32,638:INFO:Initializing Ridge Regression
2023-03-05 17:34:32,638:INFO:Total runtime is 0.16250101327896116 minutes
2023-03-05 17:34:32,643:INFO:SubProcess create_model() called ==================================
2023-03-05 17:34:32,644:INFO:Initializing create_model()
2023-03-05 17:34:32,644:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A500A95E0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:34:32,644:INFO:Checking exceptions
2023-03-05 17:34:32,644:INFO:Importing libraries
2023-03-05 17:34:32,644:INFO:Copying training dataset
2023-03-05 17:34:32,652:INFO:Defining folds
2023-03-05 17:34:32,652:INFO:Declaring metric variables
2023-03-05 17:34:32,656:INFO:Importing untrained model
2023-03-05 17:34:32,662:INFO:Ridge Regression Imported successfully
2023-03-05 17:34:32,675:INFO:Starting cross validation
2023-03-05 17:34:32,677:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:34:33,490:INFO:Calculating mean and std
2023-03-05 17:34:33,492:INFO:Creating metrics dataframe
2023-03-05 17:34:33,495:INFO:Uploading results into container
2023-03-05 17:34:33,495:INFO:Uploading model into container now
2023-03-05 17:34:33,495:INFO:_master_model_container: 3
2023-03-05 17:34:33,495:INFO:_display_container: 2
2023-03-05 17:34:33,496:INFO:Ridge(random_state=4313)
2023-03-05 17:34:33,496:INFO:create_model() successfully completed......................................
2023-03-05 17:34:33,589:INFO:SubProcess create_model() end ==================================
2023-03-05 17:34:33,589:INFO:Creating metrics dataframe
2023-03-05 17:34:33,597:INFO:Initializing Elastic Net
2023-03-05 17:34:33,597:INFO:Total runtime is 0.17847844362258908 minutes
2023-03-05 17:34:33,607:INFO:SubProcess create_model() called ==================================
2023-03-05 17:34:33,607:INFO:Initializing create_model()
2023-03-05 17:34:33,608:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A500A95E0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:34:33,608:INFO:Checking exceptions
2023-03-05 17:34:33,608:INFO:Importing libraries
2023-03-05 17:34:33,608:INFO:Copying training dataset
2023-03-05 17:34:33,614:INFO:Defining folds
2023-03-05 17:34:33,614:INFO:Declaring metric variables
2023-03-05 17:34:33,624:INFO:Importing untrained model
2023-03-05 17:34:33,628:INFO:Elastic Net Imported successfully
2023-03-05 17:34:33,642:INFO:Starting cross validation
2023-03-05 17:34:33,646:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:34:34,708:INFO:Calculating mean and std
2023-03-05 17:34:34,709:INFO:Creating metrics dataframe
2023-03-05 17:34:34,712:INFO:Uploading results into container
2023-03-05 17:34:34,713:INFO:Uploading model into container now
2023-03-05 17:34:34,713:INFO:_master_model_container: 4
2023-03-05 17:34:34,713:INFO:_display_container: 2
2023-03-05 17:34:34,713:INFO:ElasticNet(random_state=4313)
2023-03-05 17:34:34,714:INFO:create_model() successfully completed......................................
2023-03-05 17:34:34,795:INFO:SubProcess create_model() end ==================================
2023-03-05 17:34:34,795:INFO:Creating metrics dataframe
2023-03-05 17:34:34,803:INFO:Initializing Least Angle Regression
2023-03-05 17:34:34,803:INFO:Total runtime is 0.1985828081766764 minutes
2023-03-05 17:34:34,806:INFO:SubProcess create_model() called ==================================
2023-03-05 17:34:34,807:INFO:Initializing create_model()
2023-03-05 17:34:34,807:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A500A95E0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:34:34,807:INFO:Checking exceptions
2023-03-05 17:34:34,807:INFO:Importing libraries
2023-03-05 17:34:34,807:INFO:Copying training dataset
2023-03-05 17:34:34,812:INFO:Defining folds
2023-03-05 17:34:34,812:INFO:Declaring metric variables
2023-03-05 17:34:34,815:INFO:Importing untrained model
2023-03-05 17:34:34,821:INFO:Least Angle Regression Imported successfully
2023-03-05 17:34:34,832:INFO:Starting cross validation
2023-03-05 17:34:34,837:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:34:35,080:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:34:35,092:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:34:35,120:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:34:35,142:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:34:35,146:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:34:35,160:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:34:35,174:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:34:35,216:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:34:35,563:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:34:35,578:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:34:35,648:INFO:Calculating mean and std
2023-03-05 17:34:35,650:INFO:Creating metrics dataframe
2023-03-05 17:34:35,653:INFO:Uploading results into container
2023-03-05 17:34:35,653:INFO:Uploading model into container now
2023-03-05 17:34:35,654:INFO:_master_model_container: 5
2023-03-05 17:34:35,654:INFO:_display_container: 2
2023-03-05 17:34:35,654:INFO:Lars(random_state=4313)
2023-03-05 17:34:35,654:INFO:create_model() successfully completed......................................
2023-03-05 17:34:35,741:INFO:SubProcess create_model() end ==================================
2023-03-05 17:34:35,741:INFO:Creating metrics dataframe
2023-03-05 17:34:35,750:INFO:Initializing Lasso Least Angle Regression
2023-03-05 17:34:35,750:INFO:Total runtime is 0.21436332066853836 minutes
2023-03-05 17:34:35,754:INFO:SubProcess create_model() called ==================================
2023-03-05 17:34:35,754:INFO:Initializing create_model()
2023-03-05 17:34:35,754:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A500A95E0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:34:35,754:INFO:Checking exceptions
2023-03-05 17:34:35,755:INFO:Importing libraries
2023-03-05 17:34:35,755:INFO:Copying training dataset
2023-03-05 17:34:35,762:INFO:Defining folds
2023-03-05 17:34:35,762:INFO:Declaring metric variables
2023-03-05 17:34:35,766:INFO:Importing untrained model
2023-03-05 17:34:35,774:INFO:Lasso Least Angle Regression Imported successfully
2023-03-05 17:34:35,781:INFO:Starting cross validation
2023-03-05 17:34:35,783:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:34:36,095:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:34:36,104:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:34:36,136:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:34:36,154:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:34:36,164:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:34:36,201:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:34:36,204:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:34:36,276:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:34:36,592:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:34:36,597:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:34:36,662:INFO:Calculating mean and std
2023-03-05 17:34:36,663:INFO:Creating metrics dataframe
2023-03-05 17:34:36,666:INFO:Uploading results into container
2023-03-05 17:34:36,667:INFO:Uploading model into container now
2023-03-05 17:34:36,667:INFO:_master_model_container: 6
2023-03-05 17:34:36,667:INFO:_display_container: 2
2023-03-05 17:34:36,667:INFO:LassoLars(random_state=4313)
2023-03-05 17:34:36,668:INFO:create_model() successfully completed......................................
2023-03-05 17:34:36,751:INFO:SubProcess create_model() end ==================================
2023-03-05 17:34:36,751:INFO:Creating metrics dataframe
2023-03-05 17:34:36,760:INFO:Initializing Orthogonal Matching Pursuit
2023-03-05 17:34:36,760:INFO:Total runtime is 0.23119697968165076 minutes
2023-03-05 17:34:36,763:INFO:SubProcess create_model() called ==================================
2023-03-05 17:34:36,763:INFO:Initializing create_model()
2023-03-05 17:34:36,763:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A500A95E0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:34:36,763:INFO:Checking exceptions
2023-03-05 17:34:36,764:INFO:Importing libraries
2023-03-05 17:34:36,764:INFO:Copying training dataset
2023-03-05 17:34:36,769:INFO:Defining folds
2023-03-05 17:34:36,770:INFO:Declaring metric variables
2023-03-05 17:34:36,775:INFO:Importing untrained model
2023-03-05 17:34:36,779:INFO:Orthogonal Matching Pursuit Imported successfully
2023-03-05 17:34:36,789:INFO:Starting cross validation
2023-03-05 17:34:36,793:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:34:37,034:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:34:37,057:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:34:37,058:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:34:37,097:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:34:37,121:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:34:37,128:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:34:37,129:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:34:37,188:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:34:37,492:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:34:37,503:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:34:37,565:INFO:Calculating mean and std
2023-03-05 17:34:37,566:INFO:Creating metrics dataframe
2023-03-05 17:34:37,568:INFO:Uploading results into container
2023-03-05 17:34:37,569:INFO:Uploading model into container now
2023-03-05 17:34:37,569:INFO:_master_model_container: 7
2023-03-05 17:34:37,570:INFO:_display_container: 2
2023-03-05 17:34:37,570:INFO:OrthogonalMatchingPursuit()
2023-03-05 17:34:37,571:INFO:create_model() successfully completed......................................
2023-03-05 17:34:37,653:INFO:SubProcess create_model() end ==================================
2023-03-05 17:34:37,653:INFO:Creating metrics dataframe
2023-03-05 17:34:37,661:INFO:Initializing Bayesian Ridge
2023-03-05 17:34:37,661:INFO:Total runtime is 0.2462189714113871 minutes
2023-03-05 17:34:37,664:INFO:SubProcess create_model() called ==================================
2023-03-05 17:34:37,665:INFO:Initializing create_model()
2023-03-05 17:34:37,665:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A500A95E0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:34:37,665:INFO:Checking exceptions
2023-03-05 17:34:37,665:INFO:Importing libraries
2023-03-05 17:34:37,665:INFO:Copying training dataset
2023-03-05 17:34:37,670:INFO:Defining folds
2023-03-05 17:34:37,671:INFO:Declaring metric variables
2023-03-05 17:34:37,674:INFO:Importing untrained model
2023-03-05 17:34:37,679:INFO:Bayesian Ridge Imported successfully
2023-03-05 17:34:37,687:INFO:Starting cross validation
2023-03-05 17:34:37,691:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:34:38,455:INFO:Calculating mean and std
2023-03-05 17:34:38,456:INFO:Creating metrics dataframe
2023-03-05 17:34:38,460:INFO:Uploading results into container
2023-03-05 17:34:38,461:INFO:Uploading model into container now
2023-03-05 17:34:38,461:INFO:_master_model_container: 8
2023-03-05 17:34:38,462:INFO:_display_container: 2
2023-03-05 17:34:38,462:INFO:BayesianRidge()
2023-03-05 17:34:38,462:INFO:create_model() successfully completed......................................
2023-03-05 17:34:38,547:INFO:SubProcess create_model() end ==================================
2023-03-05 17:34:38,547:INFO:Creating metrics dataframe
2023-03-05 17:34:38,556:INFO:Initializing Passive Aggressive Regressor
2023-03-05 17:34:38,556:INFO:Total runtime is 0.2611266771952311 minutes
2023-03-05 17:34:38,560:INFO:SubProcess create_model() called ==================================
2023-03-05 17:34:38,560:INFO:Initializing create_model()
2023-03-05 17:34:38,561:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A500A95E0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:34:38,561:INFO:Checking exceptions
2023-03-05 17:34:38,561:INFO:Importing libraries
2023-03-05 17:34:38,561:INFO:Copying training dataset
2023-03-05 17:34:38,566:INFO:Defining folds
2023-03-05 17:34:38,566:INFO:Declaring metric variables
2023-03-05 17:34:38,570:INFO:Importing untrained model
2023-03-05 17:34:38,576:INFO:Passive Aggressive Regressor Imported successfully
2023-03-05 17:34:38,583:INFO:Starting cross validation
2023-03-05 17:34:38,585:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:34:39,487:INFO:Calculating mean and std
2023-03-05 17:34:39,489:INFO:Creating metrics dataframe
2023-03-05 17:34:39,494:INFO:Uploading results into container
2023-03-05 17:34:39,495:INFO:Uploading model into container now
2023-03-05 17:34:39,495:INFO:_master_model_container: 9
2023-03-05 17:34:39,496:INFO:_display_container: 2
2023-03-05 17:34:39,496:INFO:PassiveAggressiveRegressor(random_state=4313)
2023-03-05 17:34:39,496:INFO:create_model() successfully completed......................................
2023-03-05 17:34:39,597:INFO:SubProcess create_model() end ==================================
2023-03-05 17:34:39,597:INFO:Creating metrics dataframe
2023-03-05 17:34:39,612:INFO:Initializing Huber Regressor
2023-03-05 17:34:39,612:INFO:Total runtime is 0.2787303288777669 minutes
2023-03-05 17:34:39,616:INFO:SubProcess create_model() called ==================================
2023-03-05 17:34:39,616:INFO:Initializing create_model()
2023-03-05 17:34:39,616:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A500A95E0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:34:39,616:INFO:Checking exceptions
2023-03-05 17:34:39,616:INFO:Importing libraries
2023-03-05 17:34:39,617:INFO:Copying training dataset
2023-03-05 17:34:39,623:INFO:Defining folds
2023-03-05 17:34:39,623:INFO:Declaring metric variables
2023-03-05 17:34:39,627:INFO:Importing untrained model
2023-03-05 17:34:39,631:INFO:Huber Regressor Imported successfully
2023-03-05 17:34:39,643:INFO:Starting cross validation
2023-03-05 17:34:39,645:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:34:40,145:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-05 17:34:40,835:INFO:Calculating mean and std
2023-03-05 17:34:40,837:INFO:Creating metrics dataframe
2023-03-05 17:34:40,841:INFO:Uploading results into container
2023-03-05 17:34:40,841:INFO:Uploading model into container now
2023-03-05 17:34:40,842:INFO:_master_model_container: 10
2023-03-05 17:34:40,842:INFO:_display_container: 2
2023-03-05 17:34:40,842:INFO:HuberRegressor()
2023-03-05 17:34:40,842:INFO:create_model() successfully completed......................................
2023-03-05 17:34:40,930:INFO:SubProcess create_model() end ==================================
2023-03-05 17:34:40,930:INFO:Creating metrics dataframe
2023-03-05 17:34:40,944:INFO:Initializing K Neighbors Regressor
2023-03-05 17:34:40,944:INFO:Total runtime is 0.3009345213572184 minutes
2023-03-05 17:34:40,949:INFO:SubProcess create_model() called ==================================
2023-03-05 17:34:40,949:INFO:Initializing create_model()
2023-03-05 17:34:40,949:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A500A95E0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:34:40,949:INFO:Checking exceptions
2023-03-05 17:34:40,949:INFO:Importing libraries
2023-03-05 17:34:40,950:INFO:Copying training dataset
2023-03-05 17:34:40,957:INFO:Defining folds
2023-03-05 17:34:40,958:INFO:Declaring metric variables
2023-03-05 17:34:40,965:INFO:Importing untrained model
2023-03-05 17:34:40,970:INFO:K Neighbors Regressor Imported successfully
2023-03-05 17:34:40,985:INFO:Starting cross validation
2023-03-05 17:34:40,988:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:34:42,082:INFO:Calculating mean and std
2023-03-05 17:34:42,084:INFO:Creating metrics dataframe
2023-03-05 17:34:42,088:INFO:Uploading results into container
2023-03-05 17:34:42,089:INFO:Uploading model into container now
2023-03-05 17:34:42,089:INFO:_master_model_container: 11
2023-03-05 17:34:42,090:INFO:_display_container: 2
2023-03-05 17:34:42,090:INFO:KNeighborsRegressor(n_jobs=-1)
2023-03-05 17:34:42,090:INFO:create_model() successfully completed......................................
2023-03-05 17:34:42,183:INFO:SubProcess create_model() end ==================================
2023-03-05 17:34:42,184:INFO:Creating metrics dataframe
2023-03-05 17:34:42,196:INFO:Initializing Decision Tree Regressor
2023-03-05 17:34:42,196:INFO:Total runtime is 0.32178632020950315 minutes
2023-03-05 17:34:42,200:INFO:SubProcess create_model() called ==================================
2023-03-05 17:34:42,200:INFO:Initializing create_model()
2023-03-05 17:34:42,201:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A500A95E0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:34:42,201:INFO:Checking exceptions
2023-03-05 17:34:42,201:INFO:Importing libraries
2023-03-05 17:34:42,201:INFO:Copying training dataset
2023-03-05 17:34:42,206:INFO:Defining folds
2023-03-05 17:34:42,206:INFO:Declaring metric variables
2023-03-05 17:34:42,211:INFO:Importing untrained model
2023-03-05 17:34:42,217:INFO:Decision Tree Regressor Imported successfully
2023-03-05 17:34:42,226:INFO:Starting cross validation
2023-03-05 17:34:42,230:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:34:43,209:INFO:Calculating mean and std
2023-03-05 17:34:43,211:INFO:Creating metrics dataframe
2023-03-05 17:34:43,215:INFO:Uploading results into container
2023-03-05 17:34:43,216:INFO:Uploading model into container now
2023-03-05 17:34:43,217:INFO:_master_model_container: 12
2023-03-05 17:34:43,217:INFO:_display_container: 2
2023-03-05 17:34:43,218:INFO:DecisionTreeRegressor(random_state=4313)
2023-03-05 17:34:43,218:INFO:create_model() successfully completed......................................
2023-03-05 17:34:43,306:INFO:SubProcess create_model() end ==================================
2023-03-05 17:34:43,306:INFO:Creating metrics dataframe
2023-03-05 17:34:43,319:INFO:Initializing Random Forest Regressor
2023-03-05 17:34:43,320:INFO:Total runtime is 0.3405315319697062 minutes
2023-03-05 17:34:43,324:INFO:SubProcess create_model() called ==================================
2023-03-05 17:34:43,324:INFO:Initializing create_model()
2023-03-05 17:34:43,325:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A500A95E0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:34:43,325:INFO:Checking exceptions
2023-03-05 17:34:43,325:INFO:Importing libraries
2023-03-05 17:34:43,325:INFO:Copying training dataset
2023-03-05 17:34:43,332:INFO:Defining folds
2023-03-05 17:34:43,332:INFO:Declaring metric variables
2023-03-05 17:34:43,337:INFO:Importing untrained model
2023-03-05 17:34:43,344:INFO:Random Forest Regressor Imported successfully
2023-03-05 17:34:43,359:INFO:Starting cross validation
2023-03-05 17:34:43,362:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:34:46,552:INFO:Calculating mean and std
2023-03-05 17:34:46,554:INFO:Creating metrics dataframe
2023-03-05 17:34:46,558:INFO:Uploading results into container
2023-03-05 17:34:46,559:INFO:Uploading model into container now
2023-03-05 17:34:46,560:INFO:_master_model_container: 13
2023-03-05 17:34:46,560:INFO:_display_container: 2
2023-03-05 17:34:46,560:INFO:RandomForestRegressor(n_jobs=-1, random_state=4313)
2023-03-05 17:34:46,561:INFO:create_model() successfully completed......................................
2023-03-05 17:34:46,655:INFO:SubProcess create_model() end ==================================
2023-03-05 17:34:46,655:INFO:Creating metrics dataframe
2023-03-05 17:34:46,670:INFO:Initializing Extra Trees Regressor
2023-03-05 17:34:46,670:INFO:Total runtime is 0.3963528593381246 minutes
2023-03-05 17:34:46,673:INFO:SubProcess create_model() called ==================================
2023-03-05 17:34:46,673:INFO:Initializing create_model()
2023-03-05 17:34:46,673:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A500A95E0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:34:46,673:INFO:Checking exceptions
2023-03-05 17:34:46,675:INFO:Importing libraries
2023-03-05 17:34:46,675:INFO:Copying training dataset
2023-03-05 17:34:46,681:INFO:Defining folds
2023-03-05 17:34:46,681:INFO:Declaring metric variables
2023-03-05 17:34:46,686:INFO:Importing untrained model
2023-03-05 17:34:46,690:INFO:Extra Trees Regressor Imported successfully
2023-03-05 17:34:46,699:INFO:Starting cross validation
2023-03-05 17:34:46,701:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:34:50,477:INFO:Calculating mean and std
2023-03-05 17:34:50,480:INFO:Creating metrics dataframe
2023-03-05 17:34:50,485:INFO:Uploading results into container
2023-03-05 17:34:50,486:INFO:Uploading model into container now
2023-03-05 17:34:50,486:INFO:_master_model_container: 14
2023-03-05 17:34:50,486:INFO:_display_container: 2
2023-03-05 17:34:50,487:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=4313)
2023-03-05 17:34:50,487:INFO:create_model() successfully completed......................................
2023-03-05 17:34:50,607:INFO:SubProcess create_model() end ==================================
2023-03-05 17:34:50,608:INFO:Creating metrics dataframe
2023-03-05 17:34:50,625:INFO:Initializing AdaBoost Regressor
2023-03-05 17:34:50,626:INFO:Total runtime is 0.46230214834213257 minutes
2023-03-05 17:34:50,634:INFO:SubProcess create_model() called ==================================
2023-03-05 17:34:50,635:INFO:Initializing create_model()
2023-03-05 17:34:50,635:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A500A95E0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:34:50,635:INFO:Checking exceptions
2023-03-05 17:34:50,636:INFO:Importing libraries
2023-03-05 17:34:50,636:INFO:Copying training dataset
2023-03-05 17:34:50,647:INFO:Defining folds
2023-03-05 17:34:50,648:INFO:Declaring metric variables
2023-03-05 17:34:50,656:INFO:Importing untrained model
2023-03-05 17:34:50,662:INFO:AdaBoost Regressor Imported successfully
2023-03-05 17:34:50,672:INFO:Starting cross validation
2023-03-05 17:34:50,675:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:34:52,832:INFO:Calculating mean and std
2023-03-05 17:34:52,834:INFO:Creating metrics dataframe
2023-03-05 17:34:52,838:INFO:Uploading results into container
2023-03-05 17:34:52,839:INFO:Uploading model into container now
2023-03-05 17:34:52,839:INFO:_master_model_container: 15
2023-03-05 17:34:52,839:INFO:_display_container: 2
2023-03-05 17:34:52,840:INFO:AdaBoostRegressor(random_state=4313)
2023-03-05 17:34:52,840:INFO:create_model() successfully completed......................................
2023-03-05 17:34:52,928:INFO:SubProcess create_model() end ==================================
2023-03-05 17:34:52,928:INFO:Creating metrics dataframe
2023-03-05 17:34:52,943:INFO:Initializing Gradient Boosting Regressor
2023-03-05 17:34:52,943:INFO:Total runtime is 0.5009156227111816 minutes
2023-03-05 17:34:52,948:INFO:SubProcess create_model() called ==================================
2023-03-05 17:34:52,949:INFO:Initializing create_model()
2023-03-05 17:34:52,949:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A500A95E0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:34:52,949:INFO:Checking exceptions
2023-03-05 17:34:52,949:INFO:Importing libraries
2023-03-05 17:34:52,949:INFO:Copying training dataset
2023-03-05 17:34:52,955:INFO:Defining folds
2023-03-05 17:34:52,956:INFO:Declaring metric variables
2023-03-05 17:34:52,961:INFO:Importing untrained model
2023-03-05 17:34:52,966:INFO:Gradient Boosting Regressor Imported successfully
2023-03-05 17:34:52,975:INFO:Starting cross validation
2023-03-05 17:34:52,978:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:34:55,066:INFO:Calculating mean and std
2023-03-05 17:34:55,068:INFO:Creating metrics dataframe
2023-03-05 17:34:55,074:INFO:Uploading results into container
2023-03-05 17:34:55,075:INFO:Uploading model into container now
2023-03-05 17:34:55,075:INFO:_master_model_container: 16
2023-03-05 17:34:55,075:INFO:_display_container: 2
2023-03-05 17:34:55,076:INFO:GradientBoostingRegressor(random_state=4313)
2023-03-05 17:34:55,076:INFO:create_model() successfully completed......................................
2023-03-05 17:34:55,186:INFO:SubProcess create_model() end ==================================
2023-03-05 17:34:55,187:INFO:Creating metrics dataframe
2023-03-05 17:34:55,202:INFO:Initializing Extreme Gradient Boosting
2023-03-05 17:34:55,203:INFO:Total runtime is 0.5385748624801635 minutes
2023-03-05 17:34:55,208:INFO:SubProcess create_model() called ==================================
2023-03-05 17:34:55,209:INFO:Initializing create_model()
2023-03-05 17:34:55,209:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A500A95E0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:34:55,210:INFO:Checking exceptions
2023-03-05 17:34:55,210:INFO:Importing libraries
2023-03-05 17:34:55,210:INFO:Copying training dataset
2023-03-05 17:34:55,217:INFO:Defining folds
2023-03-05 17:34:55,217:INFO:Declaring metric variables
2023-03-05 17:34:55,221:INFO:Importing untrained model
2023-03-05 17:34:55,227:INFO:Extreme Gradient Boosting Imported successfully
2023-03-05 17:34:55,237:INFO:Starting cross validation
2023-03-05 17:34:55,239:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:34:57,412:INFO:Calculating mean and std
2023-03-05 17:34:57,413:INFO:Creating metrics dataframe
2023-03-05 17:34:57,417:INFO:Uploading results into container
2023-03-05 17:34:57,418:INFO:Uploading model into container now
2023-03-05 17:34:57,418:INFO:_master_model_container: 17
2023-03-05 17:34:57,418:INFO:_display_container: 2
2023-03-05 17:34:57,419:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=4313, ...)
2023-03-05 17:34:57,419:INFO:create_model() successfully completed......................................
2023-03-05 17:34:57,512:INFO:SubProcess create_model() end ==================================
2023-03-05 17:34:57,512:INFO:Creating metrics dataframe
2023-03-05 17:34:57,527:INFO:Initializing Light Gradient Boosting Machine
2023-03-05 17:34:57,527:INFO:Total runtime is 0.5773029685020447 minutes
2023-03-05 17:34:57,532:INFO:SubProcess create_model() called ==================================
2023-03-05 17:34:57,532:INFO:Initializing create_model()
2023-03-05 17:34:57,533:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A500A95E0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:34:57,533:INFO:Checking exceptions
2023-03-05 17:34:57,533:INFO:Importing libraries
2023-03-05 17:34:57,533:INFO:Copying training dataset
2023-03-05 17:34:57,538:INFO:Defining folds
2023-03-05 17:34:57,538:INFO:Declaring metric variables
2023-03-05 17:34:57,543:INFO:Importing untrained model
2023-03-05 17:34:57,549:INFO:Light Gradient Boosting Machine Imported successfully
2023-03-05 17:34:57,557:INFO:Starting cross validation
2023-03-05 17:34:57,559:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:35:01,106:INFO:Calculating mean and std
2023-03-05 17:35:01,108:INFO:Creating metrics dataframe
2023-03-05 17:35:01,112:INFO:Uploading results into container
2023-03-05 17:35:01,113:INFO:Uploading model into container now
2023-03-05 17:35:01,113:INFO:_master_model_container: 18
2023-03-05 17:35:01,113:INFO:_display_container: 2
2023-03-05 17:35:01,114:INFO:LGBMRegressor(random_state=4313)
2023-03-05 17:35:01,114:INFO:create_model() successfully completed......................................
2023-03-05 17:35:01,207:INFO:SubProcess create_model() end ==================================
2023-03-05 17:35:01,207:INFO:Creating metrics dataframe
2023-03-05 17:35:01,224:INFO:Initializing Dummy Regressor
2023-03-05 17:35:01,224:INFO:Total runtime is 0.6389301896095275 minutes
2023-03-05 17:35:01,228:INFO:SubProcess create_model() called ==================================
2023-03-05 17:35:01,228:INFO:Initializing create_model()
2023-03-05 17:35:01,229:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A500A95E0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:35:01,229:INFO:Checking exceptions
2023-03-05 17:35:01,229:INFO:Importing libraries
2023-03-05 17:35:01,229:INFO:Copying training dataset
2023-03-05 17:35:01,235:INFO:Defining folds
2023-03-05 17:35:01,235:INFO:Declaring metric variables
2023-03-05 17:35:01,240:INFO:Importing untrained model
2023-03-05 17:35:01,244:INFO:Dummy Regressor Imported successfully
2023-03-05 17:35:01,254:INFO:Starting cross validation
2023-03-05 17:35:01,256:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:35:02,312:INFO:Calculating mean and std
2023-03-05 17:35:02,314:INFO:Creating metrics dataframe
2023-03-05 17:35:02,318:INFO:Uploading results into container
2023-03-05 17:35:02,318:INFO:Uploading model into container now
2023-03-05 17:35:02,319:INFO:_master_model_container: 19
2023-03-05 17:35:02,319:INFO:_display_container: 2
2023-03-05 17:35:02,319:INFO:DummyRegressor()
2023-03-05 17:35:02,319:INFO:create_model() successfully completed......................................
2023-03-05 17:35:02,407:INFO:SubProcess create_model() end ==================================
2023-03-05 17:35:02,408:INFO:Creating metrics dataframe
2023-03-05 17:35:02,436:INFO:Initializing create_model()
2023-03-05 17:35:02,436:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=GradientBoostingRegressor(random_state=4313), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:35:02,437:INFO:Checking exceptions
2023-03-05 17:35:02,439:INFO:Importing libraries
2023-03-05 17:35:02,440:INFO:Copying training dataset
2023-03-05 17:35:02,445:INFO:Defining folds
2023-03-05 17:35:02,446:INFO:Declaring metric variables
2023-03-05 17:35:02,446:INFO:Importing untrained model
2023-03-05 17:35:02,446:INFO:Declaring custom model
2023-03-05 17:35:02,447:INFO:Gradient Boosting Regressor Imported successfully
2023-03-05 17:35:02,448:INFO:Cross validation set to False
2023-03-05 17:35:02,449:INFO:Fitting Model
2023-03-05 17:35:02,639:INFO:GradientBoostingRegressor(random_state=4313)
2023-03-05 17:35:02,639:INFO:create_model() successfully completed......................................
2023-03-05 17:35:02,750:INFO:_master_model_container: 19
2023-03-05 17:35:02,751:INFO:_display_container: 2
2023-03-05 17:35:02,751:INFO:GradientBoostingRegressor(random_state=4313)
2023-03-05 17:35:02,752:INFO:compare_models() successfully completed......................................
2023-03-05 17:35:02,772:INFO:Initializing evaluate_model()
2023-03-05 17:35:02,772:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=GradientBoostingRegressor(random_state=4313), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-03-05 17:35:02,792:INFO:Initializing plot_model()
2023-03-05 17:35:02,792:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=4313), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, system=True)
2023-03-05 17:35:02,792:INFO:Checking exceptions
2023-03-05 17:35:02,797:INFO:Preloading libraries
2023-03-05 17:35:02,809:INFO:Copying training dataset
2023-03-05 17:35:02,809:INFO:Plot type: pipeline
2023-03-05 17:35:02,898:INFO:Visual Rendered Successfully
2023-03-05 17:35:02,980:INFO:plot_model() successfully completed......................................
2023-03-05 17:35:02,993:INFO:Initializing predict_model()
2023-03-05 17:35:02,993:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=GradientBoostingRegressor(random_state=4313), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x0000023A56D26C10>)
2023-03-05 17:35:02,993:INFO:Checking exceptions
2023-03-05 17:35:02,994:INFO:Preloading libraries
2023-03-05 17:35:02,996:INFO:Set up data.
2023-03-05 17:35:03,001:INFO:Set up index.
2023-03-05 17:42:34,926:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-05 17:42:34,926:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-05 17:42:34,926:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-05 17:42:34,926:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-05 17:42:35,468:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-03-05 17:42:46,174:INFO:PyCaret RegressionExperiment
2023-03-05 17:42:46,174:INFO:Logging name: reg-default-name
2023-03-05 17:42:46,174:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-05 17:42:46,174:INFO:version 3.0.0.rc9
2023-03-05 17:42:46,175:INFO:Initializing setup()
2023-03-05 17:42:46,175:INFO:self.USI: e7e7
2023-03-05 17:42:46,175:INFO:self._variable_keys: {'html_param', 'y_test', '_ml_usecase', 'fold_shuffle_param', 'X_test', 'idx', 'USI', '_available_plots', 'X_train', 'fold_groups_param', 'gpu_param', 'logging_param', 'transform_target_param', 'pipeline', 'n_jobs_param', 'fold_generator', 'target_param', 'log_plots_param', 'data', 'memory', 'exp_id', 'X', 'gpu_n_jobs_param', 'y', 'y_train', 'seed', 'exp_name_log'}
2023-03-05 17:42:46,175:INFO:Checking environment
2023-03-05 17:42:46,175:INFO:python_version: 3.9.13
2023-03-05 17:42:46,175:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-03-05 17:42:46,175:INFO:machine: AMD64
2023-03-05 17:42:46,175:INFO:platform: Windows-10-10.0.19044-SP0
2023-03-05 17:42:46,175:INFO:Memory: svmem(total=17009516544, available=6269730816, percent=63.1, used=10739785728, free=6269730816)
2023-03-05 17:42:46,175:INFO:Physical Core: 4
2023-03-05 17:42:46,175:INFO:Logical Core: 8
2023-03-05 17:42:46,175:INFO:Checking libraries
2023-03-05 17:42:46,175:INFO:System:
2023-03-05 17:42:46,175:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-03-05 17:42:46,175:INFO:executable: C:\Users\Alvaro\anaconda3\python.exe
2023-03-05 17:42:46,175:INFO:   machine: Windows-10-10.0.19044-SP0
2023-03-05 17:42:46,175:INFO:PyCaret required dependencies:
2023-03-05 17:42:46,175:INFO:                 pip: 22.2.2
2023-03-05 17:42:46,175:INFO:          setuptools: 63.4.1
2023-03-05 17:42:46,176:INFO:             pycaret: 3.0.0rc9
2023-03-05 17:42:46,176:INFO:             IPython: 7.31.1
2023-03-05 17:42:46,176:INFO:          ipywidgets: 7.6.5
2023-03-05 17:42:46,176:INFO:                tqdm: 4.64.1
2023-03-05 17:42:46,176:INFO:               numpy: 1.21.5
2023-03-05 17:42:46,176:INFO:              pandas: 1.4.4
2023-03-05 17:42:46,176:INFO:              jinja2: 2.11.3
2023-03-05 17:42:46,176:INFO:               scipy: 1.9.1
2023-03-05 17:42:46,176:INFO:              joblib: 1.2.0
2023-03-05 17:42:46,176:INFO:             sklearn: 1.0.2
2023-03-05 17:42:46,176:INFO:                pyod: 1.0.7
2023-03-05 17:42:46,176:INFO:            imblearn: 0.10.1
2023-03-05 17:42:46,176:INFO:   category_encoders: 2.6.0
2023-03-05 17:42:46,176:INFO:            lightgbm: 3.3.5
2023-03-05 17:42:46,176:INFO:               numba: 0.55.1
2023-03-05 17:42:46,176:INFO:            requests: 2.28.1
2023-03-05 17:42:46,176:INFO:          matplotlib: 3.5.2
2023-03-05 17:42:46,176:INFO:          scikitplot: 0.3.7
2023-03-05 17:42:46,176:INFO:         yellowbrick: 1.5
2023-03-05 17:42:46,176:INFO:              plotly: 5.9.0
2023-03-05 17:42:46,177:INFO:             kaleido: 0.2.1
2023-03-05 17:42:46,177:INFO:         statsmodels: 0.13.2
2023-03-05 17:42:46,177:INFO:              sktime: 0.16.1
2023-03-05 17:42:46,177:INFO:               tbats: 1.1.2
2023-03-05 17:42:46,177:INFO:            pmdarima: 2.0.2
2023-03-05 17:42:46,177:INFO:              psutil: 5.9.0
2023-03-05 17:42:46,177:INFO:PyCaret optional dependencies:
2023-03-05 17:42:46,200:INFO:                shap: Not installed
2023-03-05 17:42:46,200:INFO:           interpret: Not installed
2023-03-05 17:42:46,200:INFO:                umap: Not installed
2023-03-05 17:42:46,200:INFO:    pandas_profiling: Not installed
2023-03-05 17:42:46,200:INFO:  explainerdashboard: Not installed
2023-03-05 17:42:46,200:INFO:             autoviz: Not installed
2023-03-05 17:42:46,200:INFO:           fairlearn: Not installed
2023-03-05 17:42:46,200:INFO:             xgboost: 1.7.4
2023-03-05 17:42:46,200:INFO:            catboost: Not installed
2023-03-05 17:42:46,200:INFO:              kmodes: Not installed
2023-03-05 17:42:46,200:INFO:             mlxtend: Not installed
2023-03-05 17:42:46,200:INFO:       statsforecast: Not installed
2023-03-05 17:42:46,200:INFO:        tune_sklearn: Not installed
2023-03-05 17:42:46,200:INFO:                 ray: Not installed
2023-03-05 17:42:46,200:INFO:            hyperopt: Not installed
2023-03-05 17:42:46,200:INFO:              optuna: Not installed
2023-03-05 17:42:46,200:INFO:               skopt: Not installed
2023-03-05 17:42:46,200:INFO:              mlflow: Not installed
2023-03-05 17:42:46,200:INFO:              gradio: Not installed
2023-03-05 17:42:46,201:INFO:             fastapi: Not installed
2023-03-05 17:42:46,201:INFO:             uvicorn: Not installed
2023-03-05 17:42:46,201:INFO:              m2cgen: Not installed
2023-03-05 17:42:46,201:INFO:           evidently: Not installed
2023-03-05 17:42:46,201:INFO:               fugue: Not installed
2023-03-05 17:42:46,201:INFO:           streamlit: Not installed
2023-03-05 17:42:46,201:INFO:             prophet: Not installed
2023-03-05 17:42:46,201:INFO:None
2023-03-05 17:42:46,201:INFO:Set up data.
2023-03-05 17:42:46,206:INFO:Set up train/test split.
2023-03-05 17:42:46,213:INFO:Set up index.
2023-03-05 17:42:46,213:INFO:Set up folding strategy.
2023-03-05 17:42:46,213:INFO:Assigning column types.
2023-03-05 17:42:46,216:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-05 17:42:46,216:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-05 17:42:46,220:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 17:42:46,225:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:42:46,277:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:42:46,317:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:42:46,318:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:42:46,441:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:42:46,441:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-05 17:42:46,445:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 17:42:46,449:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:42:46,503:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:42:46,543:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:42:46,543:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:42:46,546:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:42:46,546:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-03-05 17:42:46,550:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 17:42:46,554:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:42:46,606:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:42:46,646:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:42:46,646:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:42:46,649:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:42:46,655:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 17:42:46,660:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:42:46,713:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:42:46,751:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:42:46,752:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:42:46,754:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:42:46,755:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-03-05 17:42:46,763:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:42:46,813:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:42:46,852:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:42:46,853:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:42:46,855:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:42:46,863:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:42:46,914:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:42:46,954:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:42:46,955:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:42:46,957:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:42:46,958:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-03-05 17:42:47,034:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:42:47,074:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:42:47,074:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:42:47,076:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:42:47,135:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:42:47,177:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:42:47,178:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:42:47,180:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:42:47,181:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-05 17:42:47,244:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:42:47,285:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:42:47,288:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:42:47,371:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:42:47,422:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:42:47,425:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:42:47,425:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-03-05 17:42:47,532:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:42:47,534:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:42:47,652:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:42:47,654:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:42:47,656:INFO:Preparing preprocessing pipeline...
2023-03-05 17:42:47,657:INFO:Set up simple imputation.
2023-03-05 17:42:47,659:INFO:Set up encoding of categorical features.
2023-03-05 17:42:47,774:INFO:Finished creating preprocessing pipeline.
2023-03-05 17:42:47,784:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Alvaro\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['work_year', 'remote_ratio'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['experience_level',
                                             'employment_type', 'job_title',
                                             'employee_residence',
                                             'company_location',
                                             'company_size'],
                                    t...
                                    transformer=OneHotEncoder(cols=['experience_level',
                                                                    'employment_type',
                                                                    'company_size'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['job_title', 'employee_residence',
                                             'company_location'],
                                    transformer=LeaveOneOutEncoder(cols=['job_title',
                                                                         'employee_residence',
                                                                         'company_location'],
                                                                   handle_missing='return_nan',
                                                                   random_state=1496)))])
2023-03-05 17:42:47,784:INFO:Creating final display dataframe.
2023-03-05 17:42:48,226:INFO:Setup _display_container:                     Description             Value
0                    Session id              1496
1                        Target     salary_in_usd
2                   Target type        Regression
3           Original data shape          (500, 9)
4        Transformed data shape         (500, 17)
5   Transformed train set shape         (350, 17)
6    Transformed test set shape         (150, 17)
7              Numeric features                 2
8          Categorical features                 6
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator             KFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  reg-default-name
21                          USI              e7e7
2023-03-05 17:42:48,349:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:42:48,352:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:42:48,449:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:42:48,452:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:42:48,452:INFO:setup() successfully completed in 2.28s...............
2023-03-05 17:42:48,452:INFO:Initializing create_model()
2023-03-05 17:42:48,452:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021E41F07BB0>, estimator=dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:42:48,452:INFO:Checking exceptions
2023-03-05 17:42:48,485:INFO:Importing libraries
2023-03-05 17:42:48,486:INFO:Copying training dataset
2023-03-05 17:42:48,491:INFO:Defining folds
2023-03-05 17:42:48,491:INFO:Declaring metric variables
2023-03-05 17:42:48,502:INFO:Importing untrained model
2023-03-05 17:42:48,508:INFO:Decision Tree Regressor Imported successfully
2023-03-05 17:42:48,519:INFO:Starting cross validation
2023-03-05 17:42:48,526:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:42:56,794:INFO:Calculating mean and std
2023-03-05 17:42:56,796:INFO:Creating metrics dataframe
2023-03-05 17:42:56,801:INFO:Finalizing model
2023-03-05 17:42:56,918:INFO:Uploading results into container
2023-03-05 17:42:56,919:INFO:Uploading model into container now
2023-03-05 17:42:56,930:INFO:_master_model_container: 1
2023-03-05 17:42:56,930:INFO:_display_container: 2
2023-03-05 17:42:56,930:INFO:DecisionTreeRegressor(random_state=1496)
2023-03-05 17:42:56,931:INFO:create_model() successfully completed......................................
2023-03-05 17:42:57,039:INFO:Initializing tune_model()
2023-03-05 17:42:57,039:INFO:tune_model(estimator=DecisionTreeRegressor(random_state=1496), fold=None, round=4, n_iter=10, custom_grid=None, optimize=RMSE, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021E41F07BB0>)
2023-03-05 17:42:57,039:INFO:Checking exceptions
2023-03-05 17:42:57,079:INFO:Copying training dataset
2023-03-05 17:42:57,083:INFO:Checking base model
2023-03-05 17:42:57,083:INFO:Base model : Decision Tree Regressor
2023-03-05 17:42:57,094:INFO:Declaring metric variables
2023-03-05 17:42:57,099:INFO:Defining Hyperparameters
2023-03-05 17:42:57,215:INFO:Tuning with n_jobs=-1
2023-03-05 17:42:57,216:INFO:Initializing RandomizedSearchCV
2023-03-05 17:43:05,777:INFO:best_params: {'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0.05, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 6, 'actual_estimator__criterion': 'friedman_mse'}
2023-03-05 17:43:05,778:INFO:Hyperparameter search completed
2023-03-05 17:43:05,779:INFO:SubProcess create_model() called ==================================
2023-03-05 17:43:05,779:INFO:Initializing create_model()
2023-03-05 17:43:05,780:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021E41F07BB0>, estimator=DecisionTreeRegressor(random_state=1496), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021E36C2BE80>, model_only=True, return_train_score=False, kwargs={'min_samples_split': 10, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.05, 'max_features': 1.0, 'max_depth': 6, 'criterion': 'friedman_mse'})
2023-03-05 17:43:05,780:INFO:Checking exceptions
2023-03-05 17:43:05,780:INFO:Importing libraries
2023-03-05 17:43:05,780:INFO:Copying training dataset
2023-03-05 17:43:05,787:INFO:Defining folds
2023-03-05 17:43:05,787:INFO:Declaring metric variables
2023-03-05 17:43:05,793:INFO:Importing untrained model
2023-03-05 17:43:05,793:INFO:Declaring custom model
2023-03-05 17:43:05,802:INFO:Decision Tree Regressor Imported successfully
2023-03-05 17:43:05,813:INFO:Starting cross validation
2023-03-05 17:43:05,816:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:43:06,467:INFO:Calculating mean and std
2023-03-05 17:43:06,469:INFO:Creating metrics dataframe
2023-03-05 17:43:06,476:INFO:Finalizing model
2023-03-05 17:43:06,645:INFO:Uploading results into container
2023-03-05 17:43:06,647:INFO:Uploading model into container now
2023-03-05 17:43:06,647:INFO:_master_model_container: 2
2023-03-05 17:43:06,647:INFO:_display_container: 3
2023-03-05 17:43:06,648:INFO:DecisionTreeRegressor(criterion='friedman_mse', max_depth=6, max_features=1.0,
                      min_impurity_decrease=0.05, min_samples_leaf=6,
                      min_samples_split=10, random_state=1496)
2023-03-05 17:43:06,648:INFO:create_model() successfully completed......................................
2023-03-05 17:43:06,758:INFO:SubProcess create_model() end ==================================
2023-03-05 17:43:06,759:INFO:choose_better activated
2023-03-05 17:43:06,763:INFO:SubProcess create_model() called ==================================
2023-03-05 17:43:06,763:INFO:Initializing create_model()
2023-03-05 17:43:06,763:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021E41F07BB0>, estimator=DecisionTreeRegressor(random_state=1496), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:43:06,763:INFO:Checking exceptions
2023-03-05 17:43:06,765:INFO:Importing libraries
2023-03-05 17:43:06,765:INFO:Copying training dataset
2023-03-05 17:43:06,770:INFO:Defining folds
2023-03-05 17:43:06,770:INFO:Declaring metric variables
2023-03-05 17:43:06,770:INFO:Importing untrained model
2023-03-05 17:43:06,770:INFO:Declaring custom model
2023-03-05 17:43:06,771:INFO:Decision Tree Regressor Imported successfully
2023-03-05 17:43:06,772:INFO:Starting cross validation
2023-03-05 17:43:06,773:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:43:07,470:INFO:Calculating mean and std
2023-03-05 17:43:07,470:INFO:Creating metrics dataframe
2023-03-05 17:43:07,473:INFO:Finalizing model
2023-03-05 17:43:07,615:INFO:Uploading results into container
2023-03-05 17:43:07,615:INFO:Uploading model into container now
2023-03-05 17:43:07,616:INFO:_master_model_container: 3
2023-03-05 17:43:07,616:INFO:_display_container: 4
2023-03-05 17:43:07,616:INFO:DecisionTreeRegressor(random_state=1496)
2023-03-05 17:43:07,616:INFO:create_model() successfully completed......................................
2023-03-05 17:43:07,712:INFO:SubProcess create_model() end ==================================
2023-03-05 17:43:07,713:INFO:DecisionTreeRegressor(random_state=1496) result for RMSE is 55091.1199
2023-03-05 17:43:07,713:INFO:DecisionTreeRegressor(criterion='friedman_mse', max_depth=6, max_features=1.0,
                      min_impurity_decrease=0.05, min_samples_leaf=6,
                      min_samples_split=10, random_state=1496) result for RMSE is 53264.6638
2023-03-05 17:43:07,714:INFO:DecisionTreeRegressor(criterion='friedman_mse', max_depth=6, max_features=1.0,
                      min_impurity_decrease=0.05, min_samples_leaf=6,
                      min_samples_split=10, random_state=1496) is best model
2023-03-05 17:43:07,714:INFO:choose_better completed
2023-03-05 17:43:07,723:INFO:_master_model_container: 3
2023-03-05 17:43:07,723:INFO:_display_container: 3
2023-03-05 17:43:07,724:INFO:DecisionTreeRegressor(criterion='friedman_mse', max_depth=6, max_features=1.0,
                      min_impurity_decrease=0.05, min_samples_leaf=6,
                      min_samples_split=10, random_state=1496)
2023-03-05 17:43:07,724:INFO:tune_model() successfully completed......................................
2023-03-05 17:43:50,753:INFO:Initializing compare_models()
2023-03-05 17:43:50,754:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021E41F07BB0>, include=None, fold=None, round=4, cross_validation=True, sort=RMSE, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000021E41F07BB0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'RMSE', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-03-05 17:43:50,754:INFO:Checking exceptions
2023-03-05 17:43:50,756:INFO:Preparing display monitor
2023-03-05 17:43:50,797:INFO:Initializing Linear Regression
2023-03-05 17:43:50,798:INFO:Total runtime is 1.6601880391438804e-05 minutes
2023-03-05 17:43:50,803:INFO:SubProcess create_model() called ==================================
2023-03-05 17:43:50,804:INFO:Initializing create_model()
2023-03-05 17:43:50,804:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021E41F07BB0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021E4248B3D0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:43:50,804:INFO:Checking exceptions
2023-03-05 17:43:50,804:INFO:Importing libraries
2023-03-05 17:43:50,804:INFO:Copying training dataset
2023-03-05 17:43:50,818:INFO:Defining folds
2023-03-05 17:43:50,818:INFO:Declaring metric variables
2023-03-05 17:43:50,823:INFO:Importing untrained model
2023-03-05 17:43:50,828:INFO:Linear Regression Imported successfully
2023-03-05 17:43:50,836:INFO:Starting cross validation
2023-03-05 17:43:50,838:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:43:51,621:INFO:Calculating mean and std
2023-03-05 17:43:51,621:INFO:Creating metrics dataframe
2023-03-05 17:43:51,624:INFO:Uploading results into container
2023-03-05 17:43:51,624:INFO:Uploading model into container now
2023-03-05 17:43:51,624:INFO:_master_model_container: 4
2023-03-05 17:43:51,625:INFO:_display_container: 4
2023-03-05 17:43:51,625:INFO:LinearRegression(n_jobs=-1)
2023-03-05 17:43:51,625:INFO:create_model() successfully completed......................................
2023-03-05 17:43:51,715:INFO:SubProcess create_model() end ==================================
2023-03-05 17:43:51,715:INFO:Creating metrics dataframe
2023-03-05 17:43:51,722:INFO:Initializing Lasso Regression
2023-03-05 17:43:51,722:INFO:Total runtime is 0.015416455268859862 minutes
2023-03-05 17:43:51,725:INFO:SubProcess create_model() called ==================================
2023-03-05 17:43:51,725:INFO:Initializing create_model()
2023-03-05 17:43:51,725:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021E41F07BB0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021E4248B3D0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:43:51,725:INFO:Checking exceptions
2023-03-05 17:43:51,725:INFO:Importing libraries
2023-03-05 17:43:51,726:INFO:Copying training dataset
2023-03-05 17:43:51,730:INFO:Defining folds
2023-03-05 17:43:51,730:INFO:Declaring metric variables
2023-03-05 17:43:51,733:INFO:Importing untrained model
2023-03-05 17:43:51,739:INFO:Lasso Regression Imported successfully
2023-03-05 17:43:51,749:INFO:Starting cross validation
2023-03-05 17:43:51,752:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:43:52,548:INFO:Calculating mean and std
2023-03-05 17:43:52,549:INFO:Creating metrics dataframe
2023-03-05 17:43:52,552:INFO:Uploading results into container
2023-03-05 17:43:52,553:INFO:Uploading model into container now
2023-03-05 17:43:52,553:INFO:_master_model_container: 5
2023-03-05 17:43:52,554:INFO:_display_container: 4
2023-03-05 17:43:52,554:INFO:Lasso(random_state=1496)
2023-03-05 17:43:52,554:INFO:create_model() successfully completed......................................
2023-03-05 17:43:52,656:INFO:SubProcess create_model() end ==================================
2023-03-05 17:43:52,657:INFO:Creating metrics dataframe
2023-03-05 17:43:52,664:INFO:Initializing Ridge Regression
2023-03-05 17:43:52,664:INFO:Total runtime is 0.031114395459493002 minutes
2023-03-05 17:43:52,668:INFO:SubProcess create_model() called ==================================
2023-03-05 17:43:52,669:INFO:Initializing create_model()
2023-03-05 17:43:52,669:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021E41F07BB0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021E4248B3D0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:43:52,669:INFO:Checking exceptions
2023-03-05 17:43:52,669:INFO:Importing libraries
2023-03-05 17:43:52,669:INFO:Copying training dataset
2023-03-05 17:43:52,676:INFO:Defining folds
2023-03-05 17:43:52,676:INFO:Declaring metric variables
2023-03-05 17:43:52,681:INFO:Importing untrained model
2023-03-05 17:43:52,686:INFO:Ridge Regression Imported successfully
2023-03-05 17:43:52,696:INFO:Starting cross validation
2023-03-05 17:43:52,699:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:43:53,479:INFO:Calculating mean and std
2023-03-05 17:43:53,480:INFO:Creating metrics dataframe
2023-03-05 17:43:53,483:INFO:Uploading results into container
2023-03-05 17:43:53,484:INFO:Uploading model into container now
2023-03-05 17:43:53,484:INFO:_master_model_container: 6
2023-03-05 17:43:53,484:INFO:_display_container: 4
2023-03-05 17:43:53,485:INFO:Ridge(random_state=1496)
2023-03-05 17:43:53,485:INFO:create_model() successfully completed......................................
2023-03-05 17:43:53,585:INFO:SubProcess create_model() end ==================================
2023-03-05 17:43:53,585:INFO:Creating metrics dataframe
2023-03-05 17:43:53,593:INFO:Initializing Elastic Net
2023-03-05 17:43:53,593:INFO:Total runtime is 0.04660230875015259 minutes
2023-03-05 17:43:53,596:INFO:SubProcess create_model() called ==================================
2023-03-05 17:43:53,596:INFO:Initializing create_model()
2023-03-05 17:43:53,596:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021E41F07BB0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021E4248B3D0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:43:53,596:INFO:Checking exceptions
2023-03-05 17:43:53,597:INFO:Importing libraries
2023-03-05 17:43:53,597:INFO:Copying training dataset
2023-03-05 17:43:53,602:INFO:Defining folds
2023-03-05 17:43:53,602:INFO:Declaring metric variables
2023-03-05 17:43:53,607:INFO:Importing untrained model
2023-03-05 17:43:53,613:INFO:Elastic Net Imported successfully
2023-03-05 17:43:53,620:INFO:Starting cross validation
2023-03-05 17:43:53,622:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:43:54,401:INFO:Calculating mean and std
2023-03-05 17:43:54,402:INFO:Creating metrics dataframe
2023-03-05 17:43:54,405:INFO:Uploading results into container
2023-03-05 17:43:54,406:INFO:Uploading model into container now
2023-03-05 17:43:54,406:INFO:_master_model_container: 7
2023-03-05 17:43:54,406:INFO:_display_container: 4
2023-03-05 17:43:54,407:INFO:ElasticNet(random_state=1496)
2023-03-05 17:43:54,407:INFO:create_model() successfully completed......................................
2023-03-05 17:43:54,510:INFO:SubProcess create_model() end ==================================
2023-03-05 17:43:54,510:INFO:Creating metrics dataframe
2023-03-05 17:43:54,519:INFO:Initializing Least Angle Regression
2023-03-05 17:43:54,519:INFO:Total runtime is 0.062031447887420654 minutes
2023-03-05 17:43:54,524:INFO:SubProcess create_model() called ==================================
2023-03-05 17:43:54,524:INFO:Initializing create_model()
2023-03-05 17:43:54,524:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021E41F07BB0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021E4248B3D0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:43:54,524:INFO:Checking exceptions
2023-03-05 17:43:54,524:INFO:Importing libraries
2023-03-05 17:43:54,525:INFO:Copying training dataset
2023-03-05 17:43:54,531:INFO:Defining folds
2023-03-05 17:43:54,531:INFO:Declaring metric variables
2023-03-05 17:43:54,535:INFO:Importing untrained model
2023-03-05 17:43:54,540:INFO:Least Angle Regression Imported successfully
2023-03-05 17:43:54,550:INFO:Starting cross validation
2023-03-05 17:43:54,552:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:43:54,805:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:43:54,827:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:43:54,840:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:43:54,892:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:43:54,905:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:43:54,911:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:43:54,924:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:43:54,938:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:43:55,271:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:43:55,272:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:43:55,338:INFO:Calculating mean and std
2023-03-05 17:43:55,339:INFO:Creating metrics dataframe
2023-03-05 17:43:55,342:INFO:Uploading results into container
2023-03-05 17:43:55,342:INFO:Uploading model into container now
2023-03-05 17:43:55,343:INFO:_master_model_container: 8
2023-03-05 17:43:55,343:INFO:_display_container: 4
2023-03-05 17:43:55,343:INFO:Lars(random_state=1496)
2023-03-05 17:43:55,343:INFO:create_model() successfully completed......................................
2023-03-05 17:43:55,443:INFO:SubProcess create_model() end ==================================
2023-03-05 17:43:55,443:INFO:Creating metrics dataframe
2023-03-05 17:43:55,451:INFO:Initializing Lasso Least Angle Regression
2023-03-05 17:43:55,451:INFO:Total runtime is 0.07756561438242594 minutes
2023-03-05 17:43:55,455:INFO:SubProcess create_model() called ==================================
2023-03-05 17:43:55,456:INFO:Initializing create_model()
2023-03-05 17:43:55,456:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021E41F07BB0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021E4248B3D0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:43:55,456:INFO:Checking exceptions
2023-03-05 17:43:55,456:INFO:Importing libraries
2023-03-05 17:43:55,456:INFO:Copying training dataset
2023-03-05 17:43:55,464:INFO:Defining folds
2023-03-05 17:43:55,464:INFO:Declaring metric variables
2023-03-05 17:43:55,468:INFO:Importing untrained model
2023-03-05 17:43:55,475:INFO:Lasso Least Angle Regression Imported successfully
2023-03-05 17:43:55,486:INFO:Starting cross validation
2023-03-05 17:43:55,487:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:43:55,739:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:43:55,752:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:43:55,780:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:43:55,828:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:43:55,837:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:43:55,845:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:43:55,859:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:43:55,874:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:43:56,204:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:43:56,215:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:43:56,279:INFO:Calculating mean and std
2023-03-05 17:43:56,281:INFO:Creating metrics dataframe
2023-03-05 17:43:56,283:INFO:Uploading results into container
2023-03-05 17:43:56,284:INFO:Uploading model into container now
2023-03-05 17:43:56,284:INFO:_master_model_container: 9
2023-03-05 17:43:56,284:INFO:_display_container: 4
2023-03-05 17:43:56,285:INFO:LassoLars(random_state=1496)
2023-03-05 17:43:56,285:INFO:create_model() successfully completed......................................
2023-03-05 17:43:56,385:INFO:SubProcess create_model() end ==================================
2023-03-05 17:43:56,385:INFO:Creating metrics dataframe
2023-03-05 17:43:56,393:INFO:Initializing Orthogonal Matching Pursuit
2023-03-05 17:43:56,393:INFO:Total runtime is 0.09325749476750692 minutes
2023-03-05 17:43:56,396:INFO:SubProcess create_model() called ==================================
2023-03-05 17:43:56,397:INFO:Initializing create_model()
2023-03-05 17:43:56,397:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021E41F07BB0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021E4248B3D0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:43:56,397:INFO:Checking exceptions
2023-03-05 17:43:56,397:INFO:Importing libraries
2023-03-05 17:43:56,397:INFO:Copying training dataset
2023-03-05 17:43:56,403:INFO:Defining folds
2023-03-05 17:43:56,403:INFO:Declaring metric variables
2023-03-05 17:43:56,408:INFO:Importing untrained model
2023-03-05 17:43:56,412:INFO:Orthogonal Matching Pursuit Imported successfully
2023-03-05 17:43:56,420:INFO:Starting cross validation
2023-03-05 17:43:56,421:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:43:56,661:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:43:56,698:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:43:56,699:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:43:56,724:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:43:56,748:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:43:56,768:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:43:56,782:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:43:56,799:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:43:57,057:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:43:57,083:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:43:57,146:INFO:Calculating mean and std
2023-03-05 17:43:57,147:INFO:Creating metrics dataframe
2023-03-05 17:43:57,150:INFO:Uploading results into container
2023-03-05 17:43:57,150:INFO:Uploading model into container now
2023-03-05 17:43:57,151:INFO:_master_model_container: 10
2023-03-05 17:43:57,151:INFO:_display_container: 4
2023-03-05 17:43:57,151:INFO:OrthogonalMatchingPursuit()
2023-03-05 17:43:57,151:INFO:create_model() successfully completed......................................
2023-03-05 17:43:57,251:INFO:SubProcess create_model() end ==================================
2023-03-05 17:43:57,251:INFO:Creating metrics dataframe
2023-03-05 17:43:57,260:INFO:Initializing Bayesian Ridge
2023-03-05 17:43:57,260:INFO:Total runtime is 0.10771054824193318 minutes
2023-03-05 17:43:57,264:INFO:SubProcess create_model() called ==================================
2023-03-05 17:43:57,265:INFO:Initializing create_model()
2023-03-05 17:43:57,265:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021E41F07BB0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021E4248B3D0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:43:57,265:INFO:Checking exceptions
2023-03-05 17:43:57,265:INFO:Importing libraries
2023-03-05 17:43:57,265:INFO:Copying training dataset
2023-03-05 17:43:57,272:INFO:Defining folds
2023-03-05 17:43:57,272:INFO:Declaring metric variables
2023-03-05 17:43:57,276:INFO:Importing untrained model
2023-03-05 17:43:57,281:INFO:Bayesian Ridge Imported successfully
2023-03-05 17:43:57,290:INFO:Starting cross validation
2023-03-05 17:43:57,292:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:43:58,076:INFO:Calculating mean and std
2023-03-05 17:43:58,077:INFO:Creating metrics dataframe
2023-03-05 17:43:58,080:INFO:Uploading results into container
2023-03-05 17:43:58,080:INFO:Uploading model into container now
2023-03-05 17:43:58,081:INFO:_master_model_container: 11
2023-03-05 17:43:58,081:INFO:_display_container: 4
2023-03-05 17:43:58,081:INFO:BayesianRidge()
2023-03-05 17:43:58,081:INFO:create_model() successfully completed......................................
2023-03-05 17:43:58,180:INFO:SubProcess create_model() end ==================================
2023-03-05 17:43:58,180:INFO:Creating metrics dataframe
2023-03-05 17:43:58,189:INFO:Initializing Passive Aggressive Regressor
2023-03-05 17:43:58,189:INFO:Total runtime is 0.12319885889689128 minutes
2023-03-05 17:43:58,192:INFO:SubProcess create_model() called ==================================
2023-03-05 17:43:58,193:INFO:Initializing create_model()
2023-03-05 17:43:58,193:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021E41F07BB0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021E4248B3D0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:43:58,193:INFO:Checking exceptions
2023-03-05 17:43:58,193:INFO:Importing libraries
2023-03-05 17:43:58,193:INFO:Copying training dataset
2023-03-05 17:43:58,198:INFO:Defining folds
2023-03-05 17:43:58,199:INFO:Declaring metric variables
2023-03-05 17:43:58,203:INFO:Importing untrained model
2023-03-05 17:43:58,207:INFO:Passive Aggressive Regressor Imported successfully
2023-03-05 17:43:58,217:INFO:Starting cross validation
2023-03-05 17:43:58,220:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:43:59,015:INFO:Calculating mean and std
2023-03-05 17:43:59,016:INFO:Creating metrics dataframe
2023-03-05 17:43:59,019:INFO:Uploading results into container
2023-03-05 17:43:59,020:INFO:Uploading model into container now
2023-03-05 17:43:59,020:INFO:_master_model_container: 12
2023-03-05 17:43:59,021:INFO:_display_container: 4
2023-03-05 17:43:59,021:INFO:PassiveAggressiveRegressor(random_state=1496)
2023-03-05 17:43:59,021:INFO:create_model() successfully completed......................................
2023-03-05 17:43:59,115:INFO:SubProcess create_model() end ==================================
2023-03-05 17:43:59,115:INFO:Creating metrics dataframe
2023-03-05 17:43:59,124:INFO:Initializing Huber Regressor
2023-03-05 17:43:59,124:INFO:Total runtime is 0.13878706296284993 minutes
2023-03-05 17:43:59,130:INFO:SubProcess create_model() called ==================================
2023-03-05 17:43:59,130:INFO:Initializing create_model()
2023-03-05 17:43:59,130:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021E41F07BB0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021E4248B3D0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:43:59,130:INFO:Checking exceptions
2023-03-05 17:43:59,130:INFO:Importing libraries
2023-03-05 17:43:59,130:INFO:Copying training dataset
2023-03-05 17:43:59,136:INFO:Defining folds
2023-03-05 17:43:59,137:INFO:Declaring metric variables
2023-03-05 17:43:59,142:INFO:Importing untrained model
2023-03-05 17:43:59,146:INFO:Huber Regressor Imported successfully
2023-03-05 17:43:59,158:INFO:Starting cross validation
2023-03-05 17:43:59,159:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:44:00,028:INFO:Calculating mean and std
2023-03-05 17:44:00,029:INFO:Creating metrics dataframe
2023-03-05 17:44:00,035:INFO:Uploading results into container
2023-03-05 17:44:00,035:INFO:Uploading model into container now
2023-03-05 17:44:00,036:INFO:_master_model_container: 13
2023-03-05 17:44:00,036:INFO:_display_container: 4
2023-03-05 17:44:00,037:INFO:HuberRegressor()
2023-03-05 17:44:00,037:INFO:create_model() successfully completed......................................
2023-03-05 17:44:00,137:INFO:SubProcess create_model() end ==================================
2023-03-05 17:44:00,138:INFO:Creating metrics dataframe
2023-03-05 17:44:00,147:INFO:Initializing K Neighbors Regressor
2023-03-05 17:44:00,147:INFO:Total runtime is 0.15582482814788817 minutes
2023-03-05 17:44:00,151:INFO:SubProcess create_model() called ==================================
2023-03-05 17:44:00,151:INFO:Initializing create_model()
2023-03-05 17:44:00,152:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021E41F07BB0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021E4248B3D0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:44:00,152:INFO:Checking exceptions
2023-03-05 17:44:00,152:INFO:Importing libraries
2023-03-05 17:44:00,152:INFO:Copying training dataset
2023-03-05 17:44:00,158:INFO:Defining folds
2023-03-05 17:44:00,158:INFO:Declaring metric variables
2023-03-05 17:44:00,163:INFO:Importing untrained model
2023-03-05 17:44:00,169:INFO:K Neighbors Regressor Imported successfully
2023-03-05 17:44:00,180:INFO:Starting cross validation
2023-03-05 17:44:00,181:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:44:00,981:INFO:Calculating mean and std
2023-03-05 17:44:00,982:INFO:Creating metrics dataframe
2023-03-05 17:44:00,986:INFO:Uploading results into container
2023-03-05 17:44:00,987:INFO:Uploading model into container now
2023-03-05 17:44:00,987:INFO:_master_model_container: 14
2023-03-05 17:44:00,988:INFO:_display_container: 4
2023-03-05 17:44:00,988:INFO:KNeighborsRegressor(n_jobs=-1)
2023-03-05 17:44:00,988:INFO:create_model() successfully completed......................................
2023-03-05 17:44:01,087:INFO:SubProcess create_model() end ==================================
2023-03-05 17:44:01,087:INFO:Creating metrics dataframe
2023-03-05 17:44:01,097:INFO:Initializing Decision Tree Regressor
2023-03-05 17:44:01,097:INFO:Total runtime is 0.17166669766108195 minutes
2023-03-05 17:44:01,100:INFO:SubProcess create_model() called ==================================
2023-03-05 17:44:01,100:INFO:Initializing create_model()
2023-03-05 17:44:01,100:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021E41F07BB0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021E4248B3D0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:44:01,100:INFO:Checking exceptions
2023-03-05 17:44:01,100:INFO:Importing libraries
2023-03-05 17:44:01,101:INFO:Copying training dataset
2023-03-05 17:44:01,106:INFO:Defining folds
2023-03-05 17:44:01,106:INFO:Declaring metric variables
2023-03-05 17:44:01,109:INFO:Importing untrained model
2023-03-05 17:44:01,115:INFO:Decision Tree Regressor Imported successfully
2023-03-05 17:44:01,124:INFO:Starting cross validation
2023-03-05 17:44:01,126:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:44:01,661:INFO:Calculating mean and std
2023-03-05 17:44:01,662:INFO:Creating metrics dataframe
2023-03-05 17:44:01,665:INFO:Uploading results into container
2023-03-05 17:44:01,666:INFO:Uploading model into container now
2023-03-05 17:44:01,667:INFO:_master_model_container: 15
2023-03-05 17:44:01,667:INFO:_display_container: 4
2023-03-05 17:44:01,668:INFO:DecisionTreeRegressor(random_state=1496)
2023-03-05 17:44:01,668:INFO:create_model() successfully completed......................................
2023-03-05 17:44:01,766:INFO:SubProcess create_model() end ==================================
2023-03-05 17:44:01,766:INFO:Creating metrics dataframe
2023-03-05 17:44:01,776:INFO:Initializing Random Forest Regressor
2023-03-05 17:44:01,776:INFO:Total runtime is 0.18297927776972453 minutes
2023-03-05 17:44:01,778:INFO:SubProcess create_model() called ==================================
2023-03-05 17:44:01,779:INFO:Initializing create_model()
2023-03-05 17:44:01,779:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021E41F07BB0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021E4248B3D0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:44:01,779:INFO:Checking exceptions
2023-03-05 17:44:01,779:INFO:Importing libraries
2023-03-05 17:44:01,779:INFO:Copying training dataset
2023-03-05 17:44:01,785:INFO:Defining folds
2023-03-05 17:44:01,785:INFO:Declaring metric variables
2023-03-05 17:44:01,790:INFO:Importing untrained model
2023-03-05 17:44:01,794:INFO:Random Forest Regressor Imported successfully
2023-03-05 17:44:01,804:INFO:Starting cross validation
2023-03-05 17:44:01,807:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:44:03,719:INFO:Calculating mean and std
2023-03-05 17:44:03,720:INFO:Creating metrics dataframe
2023-03-05 17:44:03,724:INFO:Uploading results into container
2023-03-05 17:44:03,724:INFO:Uploading model into container now
2023-03-05 17:44:03,725:INFO:_master_model_container: 16
2023-03-05 17:44:03,725:INFO:_display_container: 4
2023-03-05 17:44:03,725:INFO:RandomForestRegressor(n_jobs=-1, random_state=1496)
2023-03-05 17:44:03,725:INFO:create_model() successfully completed......................................
2023-03-05 17:44:03,824:INFO:SubProcess create_model() end ==================================
2023-03-05 17:44:03,824:INFO:Creating metrics dataframe
2023-03-05 17:44:03,835:INFO:Initializing Extra Trees Regressor
2023-03-05 17:44:03,835:INFO:Total runtime is 0.21730149586995443 minutes
2023-03-05 17:44:03,840:INFO:SubProcess create_model() called ==================================
2023-03-05 17:44:03,840:INFO:Initializing create_model()
2023-03-05 17:44:03,840:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021E41F07BB0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021E4248B3D0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:44:03,840:INFO:Checking exceptions
2023-03-05 17:44:03,840:INFO:Importing libraries
2023-03-05 17:44:03,840:INFO:Copying training dataset
2023-03-05 17:44:03,845:INFO:Defining folds
2023-03-05 17:44:03,846:INFO:Declaring metric variables
2023-03-05 17:44:03,850:INFO:Importing untrained model
2023-03-05 17:44:03,855:INFO:Extra Trees Regressor Imported successfully
2023-03-05 17:44:03,863:INFO:Starting cross validation
2023-03-05 17:44:03,864:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:44:05,563:INFO:Calculating mean and std
2023-03-05 17:44:05,564:INFO:Creating metrics dataframe
2023-03-05 17:44:05,567:INFO:Uploading results into container
2023-03-05 17:44:05,567:INFO:Uploading model into container now
2023-03-05 17:44:05,568:INFO:_master_model_container: 17
2023-03-05 17:44:05,568:INFO:_display_container: 4
2023-03-05 17:44:05,568:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=1496)
2023-03-05 17:44:05,568:INFO:create_model() successfully completed......................................
2023-03-05 17:44:05,668:INFO:SubProcess create_model() end ==================================
2023-03-05 17:44:05,668:INFO:Creating metrics dataframe
2023-03-05 17:44:05,679:INFO:Initializing AdaBoost Regressor
2023-03-05 17:44:05,679:INFO:Total runtime is 0.24803640047709147 minutes
2023-03-05 17:44:05,682:INFO:SubProcess create_model() called ==================================
2023-03-05 17:44:05,682:INFO:Initializing create_model()
2023-03-05 17:44:05,683:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021E41F07BB0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021E4248B3D0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:44:05,683:INFO:Checking exceptions
2023-03-05 17:44:05,683:INFO:Importing libraries
2023-03-05 17:44:05,683:INFO:Copying training dataset
2023-03-05 17:44:05,688:INFO:Defining folds
2023-03-05 17:44:05,688:INFO:Declaring metric variables
2023-03-05 17:44:05,692:INFO:Importing untrained model
2023-03-05 17:44:05,697:INFO:AdaBoost Regressor Imported successfully
2023-03-05 17:44:05,706:INFO:Starting cross validation
2023-03-05 17:44:05,709:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:44:06,752:INFO:Calculating mean and std
2023-03-05 17:44:06,753:INFO:Creating metrics dataframe
2023-03-05 17:44:06,756:INFO:Uploading results into container
2023-03-05 17:44:06,757:INFO:Uploading model into container now
2023-03-05 17:44:06,757:INFO:_master_model_container: 18
2023-03-05 17:44:06,758:INFO:_display_container: 4
2023-03-05 17:44:06,758:INFO:AdaBoostRegressor(random_state=1496)
2023-03-05 17:44:06,758:INFO:create_model() successfully completed......................................
2023-03-05 17:44:06,855:INFO:SubProcess create_model() end ==================================
2023-03-05 17:44:06,856:INFO:Creating metrics dataframe
2023-03-05 17:44:06,866:INFO:Initializing Gradient Boosting Regressor
2023-03-05 17:44:06,866:INFO:Total runtime is 0.26781525611877444 minutes
2023-03-05 17:44:06,869:INFO:SubProcess create_model() called ==================================
2023-03-05 17:44:06,869:INFO:Initializing create_model()
2023-03-05 17:44:06,869:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021E41F07BB0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021E4248B3D0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:44:06,869:INFO:Checking exceptions
2023-03-05 17:44:06,870:INFO:Importing libraries
2023-03-05 17:44:06,870:INFO:Copying training dataset
2023-03-05 17:44:06,875:INFO:Defining folds
2023-03-05 17:44:06,875:INFO:Declaring metric variables
2023-03-05 17:44:06,879:INFO:Importing untrained model
2023-03-05 17:44:06,884:INFO:Gradient Boosting Regressor Imported successfully
2023-03-05 17:44:06,892:INFO:Starting cross validation
2023-03-05 17:44:06,894:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:44:08,010:INFO:Calculating mean and std
2023-03-05 17:44:08,012:INFO:Creating metrics dataframe
2023-03-05 17:44:08,015:INFO:Uploading results into container
2023-03-05 17:44:08,016:INFO:Uploading model into container now
2023-03-05 17:44:08,016:INFO:_master_model_container: 19
2023-03-05 17:44:08,016:INFO:_display_container: 4
2023-03-05 17:44:08,017:INFO:GradientBoostingRegressor(random_state=1496)
2023-03-05 17:44:08,017:INFO:create_model() successfully completed......................................
2023-03-05 17:44:08,115:INFO:SubProcess create_model() end ==================================
2023-03-05 17:44:08,115:INFO:Creating metrics dataframe
2023-03-05 17:44:08,126:INFO:Initializing Extreme Gradient Boosting
2023-03-05 17:44:08,126:INFO:Total runtime is 0.2888119697570801 minutes
2023-03-05 17:44:08,129:INFO:SubProcess create_model() called ==================================
2023-03-05 17:44:08,130:INFO:Initializing create_model()
2023-03-05 17:44:08,130:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021E41F07BB0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021E4248B3D0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:44:08,130:INFO:Checking exceptions
2023-03-05 17:44:08,130:INFO:Importing libraries
2023-03-05 17:44:08,130:INFO:Copying training dataset
2023-03-05 17:44:08,137:INFO:Defining folds
2023-03-05 17:44:08,137:INFO:Declaring metric variables
2023-03-05 17:44:08,142:INFO:Importing untrained model
2023-03-05 17:44:08,147:INFO:Extreme Gradient Boosting Imported successfully
2023-03-05 17:44:08,156:INFO:Starting cross validation
2023-03-05 17:44:08,158:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:44:09,924:INFO:Calculating mean and std
2023-03-05 17:44:09,926:INFO:Creating metrics dataframe
2023-03-05 17:44:09,932:INFO:Uploading results into container
2023-03-05 17:44:09,933:INFO:Uploading model into container now
2023-03-05 17:44:09,934:INFO:_master_model_container: 20
2023-03-05 17:44:09,934:INFO:_display_container: 4
2023-03-05 17:44:09,936:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=1496, ...)
2023-03-05 17:44:09,936:INFO:create_model() successfully completed......................................
2023-03-05 17:44:10,053:INFO:SubProcess create_model() end ==================================
2023-03-05 17:44:10,053:INFO:Creating metrics dataframe
2023-03-05 17:44:10,068:INFO:Initializing Light Gradient Boosting Machine
2023-03-05 17:44:10,068:INFO:Total runtime is 0.32118208805720017 minutes
2023-03-05 17:44:10,072:INFO:SubProcess create_model() called ==================================
2023-03-05 17:44:10,073:INFO:Initializing create_model()
2023-03-05 17:44:10,073:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021E41F07BB0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021E4248B3D0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:44:10,073:INFO:Checking exceptions
2023-03-05 17:44:10,073:INFO:Importing libraries
2023-03-05 17:44:10,073:INFO:Copying training dataset
2023-03-05 17:44:10,079:INFO:Defining folds
2023-03-05 17:44:10,080:INFO:Declaring metric variables
2023-03-05 17:44:10,085:INFO:Importing untrained model
2023-03-05 17:44:10,091:INFO:Light Gradient Boosting Machine Imported successfully
2023-03-05 17:44:10,101:INFO:Starting cross validation
2023-03-05 17:44:10,103:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:44:13,305:INFO:Calculating mean and std
2023-03-05 17:44:13,307:INFO:Creating metrics dataframe
2023-03-05 17:44:13,310:INFO:Uploading results into container
2023-03-05 17:44:13,311:INFO:Uploading model into container now
2023-03-05 17:44:13,311:INFO:_master_model_container: 21
2023-03-05 17:44:13,311:INFO:_display_container: 4
2023-03-05 17:44:13,312:INFO:LGBMRegressor(random_state=1496)
2023-03-05 17:44:13,312:INFO:create_model() successfully completed......................................
2023-03-05 17:44:13,426:INFO:SubProcess create_model() end ==================================
2023-03-05 17:44:13,426:INFO:Creating metrics dataframe
2023-03-05 17:44:13,440:INFO:Initializing Dummy Regressor
2023-03-05 17:44:13,440:INFO:Total runtime is 0.3773751298586528 minutes
2023-03-05 17:44:13,444:INFO:SubProcess create_model() called ==================================
2023-03-05 17:44:13,445:INFO:Initializing create_model()
2023-03-05 17:44:13,445:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021E41F07BB0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021E4248B3D0>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:44:13,445:INFO:Checking exceptions
2023-03-05 17:44:13,445:INFO:Importing libraries
2023-03-05 17:44:13,445:INFO:Copying training dataset
2023-03-05 17:44:13,451:INFO:Defining folds
2023-03-05 17:44:13,451:INFO:Declaring metric variables
2023-03-05 17:44:13,456:INFO:Importing untrained model
2023-03-05 17:44:13,461:INFO:Dummy Regressor Imported successfully
2023-03-05 17:44:13,471:INFO:Starting cross validation
2023-03-05 17:44:13,473:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:44:14,405:INFO:Calculating mean and std
2023-03-05 17:44:14,407:INFO:Creating metrics dataframe
2023-03-05 17:44:14,411:INFO:Uploading results into container
2023-03-05 17:44:14,412:INFO:Uploading model into container now
2023-03-05 17:44:14,413:INFO:_master_model_container: 22
2023-03-05 17:44:14,413:INFO:_display_container: 4
2023-03-05 17:44:14,413:INFO:DummyRegressor()
2023-03-05 17:44:14,413:INFO:create_model() successfully completed......................................
2023-03-05 17:44:14,529:INFO:SubProcess create_model() end ==================================
2023-03-05 17:44:14,529:INFO:Creating metrics dataframe
2023-03-05 17:44:14,556:INFO:Initializing create_model()
2023-03-05 17:44:14,556:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021E41F07BB0>, estimator=Ridge(random_state=1496), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:44:14,556:INFO:Checking exceptions
2023-03-05 17:44:14,558:INFO:Importing libraries
2023-03-05 17:44:14,558:INFO:Copying training dataset
2023-03-05 17:44:14,564:INFO:Defining folds
2023-03-05 17:44:14,564:INFO:Declaring metric variables
2023-03-05 17:44:14,565:INFO:Importing untrained model
2023-03-05 17:44:14,565:INFO:Declaring custom model
2023-03-05 17:44:14,565:INFO:Ridge Regression Imported successfully
2023-03-05 17:44:14,567:INFO:Cross validation set to False
2023-03-05 17:44:14,568:INFO:Fitting Model
2023-03-05 17:44:14,710:INFO:Ridge(random_state=1496)
2023-03-05 17:44:14,710:INFO:create_model() successfully completed......................................
2023-03-05 17:44:14,862:INFO:_master_model_container: 22
2023-03-05 17:44:14,862:INFO:_display_container: 4
2023-03-05 17:44:14,862:INFO:Ridge(random_state=1496)
2023-03-05 17:44:14,863:INFO:compare_models() successfully completed......................................
2023-03-05 17:44:30,624:INFO:Initializing evaluate_model()
2023-03-05 17:44:30,624:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021E41F07BB0>, estimator=Ridge(random_state=1496), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-03-05 17:44:30,648:INFO:Initializing plot_model()
2023-03-05 17:44:30,648:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=Ridge(random_state=1496), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021E41F07BB0>, system=True)
2023-03-05 17:44:30,649:INFO:Checking exceptions
2023-03-05 17:44:30,653:INFO:Preloading libraries
2023-03-05 17:44:30,653:INFO:Copying training dataset
2023-03-05 17:44:30,653:INFO:Plot type: pipeline
2023-03-05 17:44:30,794:INFO:Visual Rendered Successfully
2023-03-05 17:44:30,891:INFO:plot_model() successfully completed......................................
2023-03-05 17:44:36,301:INFO:Initializing plot_model()
2023-03-05 17:44:36,301:INFO:plot_model(plot=error, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=Ridge(random_state=1496), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021E41F07BB0>, system=True)
2023-03-05 17:44:36,301:INFO:Checking exceptions
2023-03-05 17:44:36,304:INFO:Preloading libraries
2023-03-05 17:44:36,304:INFO:Copying training dataset
2023-03-05 17:44:36,304:INFO:Plot type: error
2023-03-05 17:44:36,557:INFO:Fitting Model
2023-03-05 17:44:36,557:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\base.py:450: UserWarning: X does not have valid feature names, but Ridge was fitted with feature names
  warnings.warn(

2023-03-05 17:44:36,558:INFO:Scoring test/hold-out set
2023-03-05 17:44:37,071:INFO:Visual Rendered Successfully
2023-03-05 17:44:37,174:INFO:plot_model() successfully completed......................................
2023-03-05 17:44:42,725:INFO:Initializing predict_model()
2023-03-05 17:44:42,726:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021E41F07BB0>, estimator=Ridge(random_state=1496), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x0000021E42ACD9D0>)
2023-03-05 17:44:42,726:INFO:Checking exceptions
2023-03-05 17:44:42,726:INFO:Preloading libraries
2023-03-05 17:44:42,728:INFO:Set up data.
2023-03-05 17:44:42,733:INFO:Set up index.
2023-03-05 17:47:20,402:INFO:PyCaret RegressionExperiment
2023-03-05 17:47:20,402:INFO:Logging name: reg-default-name
2023-03-05 17:47:20,402:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-05 17:47:20,402:INFO:version 3.0.0.rc9
2023-03-05 17:47:20,402:INFO:Initializing setup()
2023-03-05 17:47:20,402:INFO:self.USI: 46f9
2023-03-05 17:47:20,402:INFO:self._variable_keys: {'USI', 'data', 'log_plots_param', '_ml_usecase', 'memory', 'transform_target_param', 'html_param', 'X_train', 'pipeline', 'fold_shuffle_param', 'n_jobs_param', 'fold_generator', 'exp_id', 'X_test', 'exp_name_log', 'logging_param', 'y_train', '_available_plots', 'X', 'gpu_param', 'idx', 'gpu_n_jobs_param', 'fold_groups_param', 'target_param', 'y_test', 'seed', 'y'}
2023-03-05 17:47:20,402:INFO:Checking environment
2023-03-05 17:47:20,402:INFO:python_version: 3.9.13
2023-03-05 17:47:20,402:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-03-05 17:47:20,402:INFO:machine: AMD64
2023-03-05 17:47:20,403:INFO:platform: Windows-10-10.0.19044-SP0
2023-03-05 17:47:20,403:INFO:Memory: svmem(total=17009516544, available=4994699264, percent=70.6, used=12014817280, free=4994699264)
2023-03-05 17:47:20,403:INFO:Physical Core: 4
2023-03-05 17:47:20,403:INFO:Logical Core: 8
2023-03-05 17:47:20,403:INFO:Checking libraries
2023-03-05 17:47:20,403:INFO:System:
2023-03-05 17:47:20,403:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-03-05 17:47:20,403:INFO:executable: C:\Users\Alvaro\anaconda3\python.exe
2023-03-05 17:47:20,403:INFO:   machine: Windows-10-10.0.19044-SP0
2023-03-05 17:47:20,403:INFO:PyCaret required dependencies:
2023-03-05 17:47:20,403:INFO:                 pip: 22.2.2
2023-03-05 17:47:20,403:INFO:          setuptools: 63.4.1
2023-03-05 17:47:20,403:INFO:             pycaret: 3.0.0rc9
2023-03-05 17:47:20,403:INFO:             IPython: 7.31.1
2023-03-05 17:47:20,403:INFO:          ipywidgets: 7.6.5
2023-03-05 17:47:20,403:INFO:                tqdm: 4.64.1
2023-03-05 17:47:20,403:INFO:               numpy: 1.21.5
2023-03-05 17:47:20,403:INFO:              pandas: 1.4.4
2023-03-05 17:47:20,403:INFO:              jinja2: 2.11.3
2023-03-05 17:47:20,404:INFO:               scipy: 1.9.1
2023-03-05 17:47:20,404:INFO:              joblib: 1.2.0
2023-03-05 17:47:20,404:INFO:             sklearn: 1.0.2
2023-03-05 17:47:20,404:INFO:                pyod: 1.0.7
2023-03-05 17:47:20,404:INFO:            imblearn: 0.10.1
2023-03-05 17:47:20,404:INFO:   category_encoders: 2.6.0
2023-03-05 17:47:20,404:INFO:            lightgbm: 3.3.5
2023-03-05 17:47:20,404:INFO:               numba: 0.55.1
2023-03-05 17:47:20,404:INFO:            requests: 2.28.1
2023-03-05 17:47:20,404:INFO:          matplotlib: 3.5.2
2023-03-05 17:47:20,404:INFO:          scikitplot: 0.3.7
2023-03-05 17:47:20,404:INFO:         yellowbrick: 1.5
2023-03-05 17:47:20,404:INFO:              plotly: 5.9.0
2023-03-05 17:47:20,404:INFO:             kaleido: 0.2.1
2023-03-05 17:47:20,404:INFO:         statsmodels: 0.13.2
2023-03-05 17:47:20,404:INFO:              sktime: 0.16.1
2023-03-05 17:47:20,404:INFO:               tbats: 1.1.2
2023-03-05 17:47:20,404:INFO:            pmdarima: 2.0.2
2023-03-05 17:47:20,404:INFO:              psutil: 5.9.0
2023-03-05 17:47:20,404:INFO:PyCaret optional dependencies:
2023-03-05 17:47:20,404:INFO:                shap: Not installed
2023-03-05 17:47:20,404:INFO:           interpret: Not installed
2023-03-05 17:47:20,405:INFO:                umap: Not installed
2023-03-05 17:47:20,405:INFO:    pandas_profiling: Not installed
2023-03-05 17:47:20,405:INFO:  explainerdashboard: Not installed
2023-03-05 17:47:20,405:INFO:             autoviz: Not installed
2023-03-05 17:47:20,405:INFO:           fairlearn: Not installed
2023-03-05 17:47:20,405:INFO:             xgboost: 1.7.4
2023-03-05 17:47:20,405:INFO:            catboost: Not installed
2023-03-05 17:47:20,405:INFO:              kmodes: Not installed
2023-03-05 17:47:20,405:INFO:             mlxtend: Not installed
2023-03-05 17:47:20,405:INFO:       statsforecast: Not installed
2023-03-05 17:47:20,405:INFO:        tune_sklearn: Not installed
2023-03-05 17:47:20,405:INFO:                 ray: Not installed
2023-03-05 17:47:20,405:INFO:            hyperopt: Not installed
2023-03-05 17:47:20,405:INFO:              optuna: Not installed
2023-03-05 17:47:20,405:INFO:               skopt: Not installed
2023-03-05 17:47:20,405:INFO:              mlflow: Not installed
2023-03-05 17:47:20,405:INFO:              gradio: Not installed
2023-03-05 17:47:20,405:INFO:             fastapi: Not installed
2023-03-05 17:47:20,405:INFO:             uvicorn: Not installed
2023-03-05 17:47:20,405:INFO:              m2cgen: Not installed
2023-03-05 17:47:20,405:INFO:           evidently: Not installed
2023-03-05 17:47:20,405:INFO:               fugue: Not installed
2023-03-05 17:47:20,405:INFO:           streamlit: Not installed
2023-03-05 17:47:20,405:INFO:             prophet: Not installed
2023-03-05 17:47:20,405:INFO:None
2023-03-05 17:47:20,406:INFO:Set up data.
2023-03-05 17:47:20,412:INFO:Set up train/test split.
2023-03-05 17:47:20,417:INFO:Set up index.
2023-03-05 17:47:20,418:INFO:Set up folding strategy.
2023-03-05 17:47:20,418:INFO:Assigning column types.
2023-03-05 17:47:20,424:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-05 17:47:20,424:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-05 17:47:20,430:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 17:47:20,436:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:47:20,488:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:47:20,527:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:47:20,528:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:47:20,530:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:47:20,530:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-05 17:47:20,534:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 17:47:20,538:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:47:20,587:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:47:20,625:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:47:20,626:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:47:20,628:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:47:20,628:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-03-05 17:47:20,631:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 17:47:20,635:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:47:20,684:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:47:20,722:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:47:20,723:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:47:20,725:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:47:20,729:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-05 17:47:20,733:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:47:20,782:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:47:20,819:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:47:20,820:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:47:20,822:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:47:20,822:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-03-05 17:47:20,830:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:47:20,878:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:47:20,916:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:47:20,917:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:47:20,919:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:47:20,927:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-05 17:47:20,976:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:47:21,014:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:47:21,015:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:47:21,017:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:47:21,017:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-03-05 17:47:21,074:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:47:21,114:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:47:21,114:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:47:21,117:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:47:21,172:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:47:21,211:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-05 17:47:21,211:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:47:21,214:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:47:21,214:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-05 17:47:21,270:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:47:21,308:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:47:21,311:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:47:21,368:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-05 17:47:21,405:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:47:21,407:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:47:21,408:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-03-05 17:47:21,504:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:47:21,507:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:47:21,606:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:47:21,609:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:47:21,610:INFO:Preparing preprocessing pipeline...
2023-03-05 17:47:21,611:INFO:Set up simple imputation.
2023-03-05 17:47:21,613:INFO:Set up encoding of categorical features.
2023-03-05 17:47:21,712:INFO:Finished creating preprocessing pipeline.
2023-03-05 17:47:21,721:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Alvaro\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['work_year', 'remote_ratio'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['experience_level',
                                             'employment_type', 'job_title',
                                             'employee_residence',
                                             'company_location',
                                             'company_size'],
                                    t...
                                    transformer=OneHotEncoder(cols=['experience_level',
                                                                    'employment_type',
                                                                    'company_size'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['job_title', 'employee_residence',
                                             'company_location'],
                                    transformer=LeaveOneOutEncoder(cols=['job_title',
                                                                         'employee_residence',
                                                                         'company_location'],
                                                                   handle_missing='return_nan',
                                                                   random_state=7278)))])
2023-03-05 17:47:21,721:INFO:Creating final display dataframe.
2023-03-05 17:47:22,168:INFO:Setup _display_container:                     Description             Value
0                    Session id              7278
1                        Target     salary_in_usd
2                   Target type        Regression
3           Original data shape          (500, 9)
4        Transformed data shape         (500, 17)
5   Transformed train set shape         (350, 17)
6    Transformed test set shape         (150, 17)
7              Numeric features                 2
8          Categorical features                 6
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator             KFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  reg-default-name
21                          USI              46f9
2023-03-05 17:47:22,280:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:47:22,282:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:47:22,378:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-05 17:47:22,380:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-05 17:47:22,381:INFO:setup() successfully completed in 1.98s...............
2023-03-05 17:47:28,053:INFO:Initializing compare_models()
2023-03-05 17:47:28,054:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-03-05 17:47:28,054:INFO:Checking exceptions
2023-03-05 17:47:28,056:INFO:Preparing display monitor
2023-03-05 17:47:28,101:INFO:Initializing Linear Regression
2023-03-05 17:47:28,101:INFO:Total runtime is 0.0 minutes
2023-03-05 17:47:28,107:INFO:SubProcess create_model() called ==================================
2023-03-05 17:47:28,107:INFO:Initializing create_model()
2023-03-05 17:47:28,108:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000153055CF310>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:47:28,108:INFO:Checking exceptions
2023-03-05 17:47:28,108:INFO:Importing libraries
2023-03-05 17:47:28,108:INFO:Copying training dataset
2023-03-05 17:47:28,116:INFO:Defining folds
2023-03-05 17:47:28,116:INFO:Declaring metric variables
2023-03-05 17:47:28,120:INFO:Importing untrained model
2023-03-05 17:47:28,125:INFO:Linear Regression Imported successfully
2023-03-05 17:47:28,134:INFO:Starting cross validation
2023-03-05 17:47:28,136:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:47:36,178:INFO:Calculating mean and std
2023-03-05 17:47:36,180:INFO:Creating metrics dataframe
2023-03-05 17:47:36,183:INFO:Uploading results into container
2023-03-05 17:47:36,184:INFO:Uploading model into container now
2023-03-05 17:47:36,184:INFO:_master_model_container: 1
2023-03-05 17:47:36,184:INFO:_display_container: 2
2023-03-05 17:47:36,185:INFO:LinearRegression(n_jobs=-1)
2023-03-05 17:47:36,185:INFO:create_model() successfully completed......................................
2023-03-05 17:47:36,370:INFO:SubProcess create_model() end ==================================
2023-03-05 17:47:36,370:INFO:Creating metrics dataframe
2023-03-05 17:47:36,377:INFO:Initializing Lasso Regression
2023-03-05 17:47:36,377:INFO:Total runtime is 0.13793449401855468 minutes
2023-03-05 17:47:36,380:INFO:SubProcess create_model() called ==================================
2023-03-05 17:47:36,381:INFO:Initializing create_model()
2023-03-05 17:47:36,381:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000153055CF310>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:47:36,381:INFO:Checking exceptions
2023-03-05 17:47:36,381:INFO:Importing libraries
2023-03-05 17:47:36,381:INFO:Copying training dataset
2023-03-05 17:47:36,387:INFO:Defining folds
2023-03-05 17:47:36,387:INFO:Declaring metric variables
2023-03-05 17:47:36,391:INFO:Importing untrained model
2023-03-05 17:47:36,395:INFO:Lasso Regression Imported successfully
2023-03-05 17:47:36,403:INFO:Starting cross validation
2023-03-05 17:47:36,405:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:47:36,727:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.257e+09, tolerance: 1.512e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-05 17:47:37,260:INFO:Calculating mean and std
2023-03-05 17:47:37,262:INFO:Creating metrics dataframe
2023-03-05 17:47:37,265:INFO:Uploading results into container
2023-03-05 17:47:37,266:INFO:Uploading model into container now
2023-03-05 17:47:37,267:INFO:_master_model_container: 2
2023-03-05 17:47:37,267:INFO:_display_container: 2
2023-03-05 17:47:37,267:INFO:Lasso(random_state=7278)
2023-03-05 17:47:37,267:INFO:create_model() successfully completed......................................
2023-03-05 17:47:37,432:INFO:SubProcess create_model() end ==================================
2023-03-05 17:47:37,432:INFO:Creating metrics dataframe
2023-03-05 17:47:37,439:INFO:Initializing Ridge Regression
2023-03-05 17:47:37,439:INFO:Total runtime is 0.15563755432764687 minutes
2023-03-05 17:47:37,442:INFO:SubProcess create_model() called ==================================
2023-03-05 17:47:37,442:INFO:Initializing create_model()
2023-03-05 17:47:37,442:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000153055CF310>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:47:37,443:INFO:Checking exceptions
2023-03-05 17:47:37,443:INFO:Importing libraries
2023-03-05 17:47:37,443:INFO:Copying training dataset
2023-03-05 17:47:37,448:INFO:Defining folds
2023-03-05 17:47:37,448:INFO:Declaring metric variables
2023-03-05 17:47:37,452:INFO:Importing untrained model
2023-03-05 17:47:37,456:INFO:Ridge Regression Imported successfully
2023-03-05 17:47:37,464:INFO:Starting cross validation
2023-03-05 17:47:37,466:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:47:38,343:INFO:Calculating mean and std
2023-03-05 17:47:38,344:INFO:Creating metrics dataframe
2023-03-05 17:47:38,347:INFO:Uploading results into container
2023-03-05 17:47:38,348:INFO:Uploading model into container now
2023-03-05 17:47:38,348:INFO:_master_model_container: 3
2023-03-05 17:47:38,349:INFO:_display_container: 2
2023-03-05 17:47:38,349:INFO:Ridge(random_state=7278)
2023-03-05 17:47:38,349:INFO:create_model() successfully completed......................................
2023-03-05 17:47:38,514:INFO:SubProcess create_model() end ==================================
2023-03-05 17:47:38,514:INFO:Creating metrics dataframe
2023-03-05 17:47:38,522:INFO:Initializing Elastic Net
2023-03-05 17:47:38,522:INFO:Total runtime is 0.17368836800257362 minutes
2023-03-05 17:47:38,525:INFO:SubProcess create_model() called ==================================
2023-03-05 17:47:38,526:INFO:Initializing create_model()
2023-03-05 17:47:38,526:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000153055CF310>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:47:38,526:INFO:Checking exceptions
2023-03-05 17:47:38,526:INFO:Importing libraries
2023-03-05 17:47:38,526:INFO:Copying training dataset
2023-03-05 17:47:38,531:INFO:Defining folds
2023-03-05 17:47:38,532:INFO:Declaring metric variables
2023-03-05 17:47:38,536:INFO:Importing untrained model
2023-03-05 17:47:38,541:INFO:Elastic Net Imported successfully
2023-03-05 17:47:38,549:INFO:Starting cross validation
2023-03-05 17:47:38,551:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:47:39,361:INFO:Calculating mean and std
2023-03-05 17:47:39,362:INFO:Creating metrics dataframe
2023-03-05 17:47:39,365:INFO:Uploading results into container
2023-03-05 17:47:39,366:INFO:Uploading model into container now
2023-03-05 17:47:39,366:INFO:_master_model_container: 4
2023-03-05 17:47:39,366:INFO:_display_container: 2
2023-03-05 17:47:39,367:INFO:ElasticNet(random_state=7278)
2023-03-05 17:47:39,367:INFO:create_model() successfully completed......................................
2023-03-05 17:47:39,530:INFO:SubProcess create_model() end ==================================
2023-03-05 17:47:39,530:INFO:Creating metrics dataframe
2023-03-05 17:47:39,538:INFO:Initializing Least Angle Regression
2023-03-05 17:47:39,538:INFO:Total runtime is 0.19061135848363236 minutes
2023-03-05 17:47:39,541:INFO:SubProcess create_model() called ==================================
2023-03-05 17:47:39,542:INFO:Initializing create_model()
2023-03-05 17:47:39,542:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000153055CF310>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:47:39,542:INFO:Checking exceptions
2023-03-05 17:47:39,542:INFO:Importing libraries
2023-03-05 17:47:39,542:INFO:Copying training dataset
2023-03-05 17:47:39,547:INFO:Defining folds
2023-03-05 17:47:39,548:INFO:Declaring metric variables
2023-03-05 17:47:39,552:INFO:Importing untrained model
2023-03-05 17:47:39,556:INFO:Least Angle Regression Imported successfully
2023-03-05 17:47:39,564:INFO:Starting cross validation
2023-03-05 17:47:39,566:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:47:39,841:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:47:39,854:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:47:39,877:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:47:39,908:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:47:39,915:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:47:39,927:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:47:39,941:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:47:39,966:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:47:40,305:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:47:40,322:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:47:40,386:INFO:Calculating mean and std
2023-03-05 17:47:40,387:INFO:Creating metrics dataframe
2023-03-05 17:47:40,390:INFO:Uploading results into container
2023-03-05 17:47:40,391:INFO:Uploading model into container now
2023-03-05 17:47:40,391:INFO:_master_model_container: 5
2023-03-05 17:47:40,392:INFO:_display_container: 2
2023-03-05 17:47:40,392:INFO:Lars(random_state=7278)
2023-03-05 17:47:40,392:INFO:create_model() successfully completed......................................
2023-03-05 17:47:40,554:INFO:SubProcess create_model() end ==================================
2023-03-05 17:47:40,554:INFO:Creating metrics dataframe
2023-03-05 17:47:40,565:INFO:Initializing Lasso Least Angle Regression
2023-03-05 17:47:40,565:INFO:Total runtime is 0.20773416360219316 minutes
2023-03-05 17:47:40,571:INFO:SubProcess create_model() called ==================================
2023-03-05 17:47:40,572:INFO:Initializing create_model()
2023-03-05 17:47:40,572:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000153055CF310>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:47:40,572:INFO:Checking exceptions
2023-03-05 17:47:40,572:INFO:Importing libraries
2023-03-05 17:47:40,572:INFO:Copying training dataset
2023-03-05 17:47:40,578:INFO:Defining folds
2023-03-05 17:47:40,578:INFO:Declaring metric variables
2023-03-05 17:47:40,582:INFO:Importing untrained model
2023-03-05 17:47:40,589:INFO:Lasso Least Angle Regression Imported successfully
2023-03-05 17:47:40,599:INFO:Starting cross validation
2023-03-05 17:47:40,601:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:47:40,877:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:47:40,885:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:47:40,904:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:47:40,934:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:47:40,954:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:47:40,955:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:47:40,972:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:47:40,988:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:47:41,346:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:47:41,354:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:47:41,418:INFO:Calculating mean and std
2023-03-05 17:47:41,419:INFO:Creating metrics dataframe
2023-03-05 17:47:41,424:INFO:Uploading results into container
2023-03-05 17:47:41,425:INFO:Uploading model into container now
2023-03-05 17:47:41,425:INFO:_master_model_container: 6
2023-03-05 17:47:41,425:INFO:_display_container: 2
2023-03-05 17:47:41,425:INFO:LassoLars(random_state=7278)
2023-03-05 17:47:41,426:INFO:create_model() successfully completed......................................
2023-03-05 17:47:41,583:INFO:SubProcess create_model() end ==================================
2023-03-05 17:47:41,583:INFO:Creating metrics dataframe
2023-03-05 17:47:41,592:INFO:Initializing Orthogonal Matching Pursuit
2023-03-05 17:47:41,592:INFO:Total runtime is 0.2248455166816711 minutes
2023-03-05 17:47:41,595:INFO:SubProcess create_model() called ==================================
2023-03-05 17:47:41,596:INFO:Initializing create_model()
2023-03-05 17:47:41,596:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000153055CF310>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:47:41,596:INFO:Checking exceptions
2023-03-05 17:47:41,596:INFO:Importing libraries
2023-03-05 17:47:41,596:INFO:Copying training dataset
2023-03-05 17:47:41,601:INFO:Defining folds
2023-03-05 17:47:41,601:INFO:Declaring metric variables
2023-03-05 17:47:41,607:INFO:Importing untrained model
2023-03-05 17:47:41,611:INFO:Orthogonal Matching Pursuit Imported successfully
2023-03-05 17:47:41,618:INFO:Starting cross validation
2023-03-05 17:47:41,621:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:47:41,909:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:47:41,924:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:47:41,936:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:47:41,955:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:47:41,961:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:47:41,981:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:47:41,998:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:47:42,016:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:47:42,346:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:47:42,353:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:47:42,416:INFO:Calculating mean and std
2023-03-05 17:47:42,417:INFO:Creating metrics dataframe
2023-03-05 17:47:42,420:INFO:Uploading results into container
2023-03-05 17:47:42,421:INFO:Uploading model into container now
2023-03-05 17:47:42,421:INFO:_master_model_container: 7
2023-03-05 17:47:42,422:INFO:_display_container: 2
2023-03-05 17:47:42,422:INFO:OrthogonalMatchingPursuit()
2023-03-05 17:47:42,422:INFO:create_model() successfully completed......................................
2023-03-05 17:47:42,585:INFO:SubProcess create_model() end ==================================
2023-03-05 17:47:42,585:INFO:Creating metrics dataframe
2023-03-05 17:47:42,594:INFO:Initializing Bayesian Ridge
2023-03-05 17:47:42,594:INFO:Total runtime is 0.2415518204371134 minutes
2023-03-05 17:47:42,597:INFO:SubProcess create_model() called ==================================
2023-03-05 17:47:42,598:INFO:Initializing create_model()
2023-03-05 17:47:42,598:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000153055CF310>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:47:42,598:INFO:Checking exceptions
2023-03-05 17:47:42,598:INFO:Importing libraries
2023-03-05 17:47:42,598:INFO:Copying training dataset
2023-03-05 17:47:42,604:INFO:Defining folds
2023-03-05 17:47:42,604:INFO:Declaring metric variables
2023-03-05 17:47:42,608:INFO:Importing untrained model
2023-03-05 17:47:42,613:INFO:Bayesian Ridge Imported successfully
2023-03-05 17:47:42,620:INFO:Starting cross validation
2023-03-05 17:47:42,622:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:47:43,420:INFO:Calculating mean and std
2023-03-05 17:47:43,421:INFO:Creating metrics dataframe
2023-03-05 17:47:43,424:INFO:Uploading results into container
2023-03-05 17:47:43,425:INFO:Uploading model into container now
2023-03-05 17:47:43,425:INFO:_master_model_container: 8
2023-03-05 17:47:43,426:INFO:_display_container: 2
2023-03-05 17:47:43,426:INFO:BayesianRidge()
2023-03-05 17:47:43,426:INFO:create_model() successfully completed......................................
2023-03-05 17:47:43,586:INFO:SubProcess create_model() end ==================================
2023-03-05 17:47:43,586:INFO:Creating metrics dataframe
2023-03-05 17:47:43,595:INFO:Initializing Passive Aggressive Regressor
2023-03-05 17:47:43,595:INFO:Total runtime is 0.25823466777801507 minutes
2023-03-05 17:47:43,598:INFO:SubProcess create_model() called ==================================
2023-03-05 17:47:43,599:INFO:Initializing create_model()
2023-03-05 17:47:43,599:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000153055CF310>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:47:43,599:INFO:Checking exceptions
2023-03-05 17:47:43,599:INFO:Importing libraries
2023-03-05 17:47:43,599:INFO:Copying training dataset
2023-03-05 17:47:43,604:INFO:Defining folds
2023-03-05 17:47:43,605:INFO:Declaring metric variables
2023-03-05 17:47:43,609:INFO:Importing untrained model
2023-03-05 17:47:43,613:INFO:Passive Aggressive Regressor Imported successfully
2023-03-05 17:47:43,621:INFO:Starting cross validation
2023-03-05 17:47:43,623:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:47:44,519:INFO:Calculating mean and std
2023-03-05 17:47:44,520:INFO:Creating metrics dataframe
2023-03-05 17:47:44,524:INFO:Uploading results into container
2023-03-05 17:47:44,525:INFO:Uploading model into container now
2023-03-05 17:47:44,525:INFO:_master_model_container: 9
2023-03-05 17:47:44,525:INFO:_display_container: 2
2023-03-05 17:47:44,525:INFO:PassiveAggressiveRegressor(random_state=7278)
2023-03-05 17:47:44,526:INFO:create_model() successfully completed......................................
2023-03-05 17:47:44,690:INFO:SubProcess create_model() end ==================================
2023-03-05 17:47:44,691:INFO:Creating metrics dataframe
2023-03-05 17:47:44,700:INFO:Initializing Huber Regressor
2023-03-05 17:47:44,700:INFO:Total runtime is 0.276651394367218 minutes
2023-03-05 17:47:44,703:INFO:SubProcess create_model() called ==================================
2023-03-05 17:47:44,703:INFO:Initializing create_model()
2023-03-05 17:47:44,703:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000153055CF310>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:47:44,703:INFO:Checking exceptions
2023-03-05 17:47:44,703:INFO:Importing libraries
2023-03-05 17:47:44,703:INFO:Copying training dataset
2023-03-05 17:47:44,710:INFO:Defining folds
2023-03-05 17:47:44,710:INFO:Declaring metric variables
2023-03-05 17:47:44,714:INFO:Importing untrained model
2023-03-05 17:47:44,719:INFO:Huber Regressor Imported successfully
2023-03-05 17:47:44,728:INFO:Starting cross validation
2023-03-05 17:47:44,731:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:47:45,649:INFO:Calculating mean and std
2023-03-05 17:47:45,651:INFO:Creating metrics dataframe
2023-03-05 17:47:45,654:INFO:Uploading results into container
2023-03-05 17:47:45,654:INFO:Uploading model into container now
2023-03-05 17:47:45,654:INFO:_master_model_container: 10
2023-03-05 17:47:45,654:INFO:_display_container: 2
2023-03-05 17:47:45,655:INFO:HuberRegressor()
2023-03-05 17:47:45,655:INFO:create_model() successfully completed......................................
2023-03-05 17:47:45,817:INFO:SubProcess create_model() end ==================================
2023-03-05 17:47:45,818:INFO:Creating metrics dataframe
2023-03-05 17:47:45,828:INFO:Initializing K Neighbors Regressor
2023-03-05 17:47:45,829:INFO:Total runtime is 0.29546588659286493 minutes
2023-03-05 17:47:45,833:INFO:SubProcess create_model() called ==================================
2023-03-05 17:47:45,833:INFO:Initializing create_model()
2023-03-05 17:47:45,833:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000153055CF310>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:47:45,833:INFO:Checking exceptions
2023-03-05 17:47:45,833:INFO:Importing libraries
2023-03-05 17:47:45,833:INFO:Copying training dataset
2023-03-05 17:47:45,838:INFO:Defining folds
2023-03-05 17:47:45,839:INFO:Declaring metric variables
2023-03-05 17:47:45,844:INFO:Importing untrained model
2023-03-05 17:47:45,848:INFO:K Neighbors Regressor Imported successfully
2023-03-05 17:47:45,856:INFO:Starting cross validation
2023-03-05 17:47:45,859:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:47:46,698:INFO:Calculating mean and std
2023-03-05 17:47:46,699:INFO:Creating metrics dataframe
2023-03-05 17:47:46,702:INFO:Uploading results into container
2023-03-05 17:47:46,703:INFO:Uploading model into container now
2023-03-05 17:47:46,703:INFO:_master_model_container: 11
2023-03-05 17:47:46,703:INFO:_display_container: 2
2023-03-05 17:47:46,704:INFO:KNeighborsRegressor(n_jobs=-1)
2023-03-05 17:47:46,704:INFO:create_model() successfully completed......................................
2023-03-05 17:47:46,867:INFO:SubProcess create_model() end ==================================
2023-03-05 17:47:46,867:INFO:Creating metrics dataframe
2023-03-05 17:47:46,877:INFO:Initializing Decision Tree Regressor
2023-03-05 17:47:46,877:INFO:Total runtime is 0.3129241228103637 minutes
2023-03-05 17:47:46,880:INFO:SubProcess create_model() called ==================================
2023-03-05 17:47:46,881:INFO:Initializing create_model()
2023-03-05 17:47:46,881:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000153055CF310>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:47:46,881:INFO:Checking exceptions
2023-03-05 17:47:46,881:INFO:Importing libraries
2023-03-05 17:47:46,881:INFO:Copying training dataset
2023-03-05 17:47:46,886:INFO:Defining folds
2023-03-05 17:47:46,887:INFO:Declaring metric variables
2023-03-05 17:47:46,891:INFO:Importing untrained model
2023-03-05 17:47:46,896:INFO:Decision Tree Regressor Imported successfully
2023-03-05 17:47:46,904:INFO:Starting cross validation
2023-03-05 17:47:46,906:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:47:47,732:INFO:Calculating mean and std
2023-03-05 17:47:47,733:INFO:Creating metrics dataframe
2023-03-05 17:47:47,736:INFO:Uploading results into container
2023-03-05 17:47:47,737:INFO:Uploading model into container now
2023-03-05 17:47:47,737:INFO:_master_model_container: 12
2023-03-05 17:47:47,737:INFO:_display_container: 2
2023-03-05 17:47:47,737:INFO:DecisionTreeRegressor(random_state=7278)
2023-03-05 17:47:47,737:INFO:create_model() successfully completed......................................
2023-03-05 17:47:47,893:INFO:SubProcess create_model() end ==================================
2023-03-05 17:47:47,893:INFO:Creating metrics dataframe
2023-03-05 17:47:47,906:INFO:Initializing Random Forest Regressor
2023-03-05 17:47:47,906:INFO:Total runtime is 0.3300743341445922 minutes
2023-03-05 17:47:47,910:INFO:SubProcess create_model() called ==================================
2023-03-05 17:47:47,911:INFO:Initializing create_model()
2023-03-05 17:47:47,911:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000153055CF310>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:47:47,911:INFO:Checking exceptions
2023-03-05 17:47:47,911:INFO:Importing libraries
2023-03-05 17:47:47,911:INFO:Copying training dataset
2023-03-05 17:47:47,917:INFO:Defining folds
2023-03-05 17:47:47,917:INFO:Declaring metric variables
2023-03-05 17:47:47,921:INFO:Importing untrained model
2023-03-05 17:47:47,926:INFO:Random Forest Regressor Imported successfully
2023-03-05 17:47:47,935:INFO:Starting cross validation
2023-03-05 17:47:47,937:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:47:50,097:INFO:Calculating mean and std
2023-03-05 17:47:50,099:INFO:Creating metrics dataframe
2023-03-05 17:47:50,103:INFO:Uploading results into container
2023-03-05 17:47:50,103:INFO:Uploading model into container now
2023-03-05 17:47:50,104:INFO:_master_model_container: 13
2023-03-05 17:47:50,104:INFO:_display_container: 2
2023-03-05 17:47:50,104:INFO:RandomForestRegressor(n_jobs=-1, random_state=7278)
2023-03-05 17:47:50,104:INFO:create_model() successfully completed......................................
2023-03-05 17:47:50,280:INFO:SubProcess create_model() end ==================================
2023-03-05 17:47:50,281:INFO:Creating metrics dataframe
2023-03-05 17:47:50,293:INFO:Initializing Extra Trees Regressor
2023-03-05 17:47:50,293:INFO:Total runtime is 0.3698590954144795 minutes
2023-03-05 17:47:50,296:INFO:SubProcess create_model() called ==================================
2023-03-05 17:47:50,297:INFO:Initializing create_model()
2023-03-05 17:47:50,297:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000153055CF310>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:47:50,297:INFO:Checking exceptions
2023-03-05 17:47:50,297:INFO:Importing libraries
2023-03-05 17:47:50,297:INFO:Copying training dataset
2023-03-05 17:47:50,303:INFO:Defining folds
2023-03-05 17:47:50,304:INFO:Declaring metric variables
2023-03-05 17:47:50,308:INFO:Importing untrained model
2023-03-05 17:47:50,312:INFO:Extra Trees Regressor Imported successfully
2023-03-05 17:47:50,321:INFO:Starting cross validation
2023-03-05 17:47:50,323:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:47:52,646:INFO:Calculating mean and std
2023-03-05 17:47:52,647:INFO:Creating metrics dataframe
2023-03-05 17:47:52,651:INFO:Uploading results into container
2023-03-05 17:47:52,651:INFO:Uploading model into container now
2023-03-05 17:47:52,652:INFO:_master_model_container: 14
2023-03-05 17:47:52,652:INFO:_display_container: 2
2023-03-05 17:47:52,652:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=7278)
2023-03-05 17:47:52,652:INFO:create_model() successfully completed......................................
2023-03-05 17:47:52,827:INFO:SubProcess create_model() end ==================================
2023-03-05 17:47:52,827:INFO:Creating metrics dataframe
2023-03-05 17:47:52,839:INFO:Initializing AdaBoost Regressor
2023-03-05 17:47:52,839:INFO:Total runtime is 0.41229961315790803 minutes
2023-03-05 17:47:52,842:INFO:SubProcess create_model() called ==================================
2023-03-05 17:47:52,842:INFO:Initializing create_model()
2023-03-05 17:47:52,842:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000153055CF310>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:47:52,843:INFO:Checking exceptions
2023-03-05 17:47:52,843:INFO:Importing libraries
2023-03-05 17:47:52,843:INFO:Copying training dataset
2023-03-05 17:47:52,849:INFO:Defining folds
2023-03-05 17:47:52,849:INFO:Declaring metric variables
2023-03-05 17:47:52,854:INFO:Importing untrained model
2023-03-05 17:47:52,858:INFO:AdaBoost Regressor Imported successfully
2023-03-05 17:47:52,867:INFO:Starting cross validation
2023-03-05 17:47:52,870:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:47:54,383:INFO:Calculating mean and std
2023-03-05 17:47:54,385:INFO:Creating metrics dataframe
2023-03-05 17:47:54,388:INFO:Uploading results into container
2023-03-05 17:47:54,389:INFO:Uploading model into container now
2023-03-05 17:47:54,389:INFO:_master_model_container: 15
2023-03-05 17:47:54,389:INFO:_display_container: 2
2023-03-05 17:47:54,389:INFO:AdaBoostRegressor(random_state=7278)
2023-03-05 17:47:54,390:INFO:create_model() successfully completed......................................
2023-03-05 17:47:54,556:INFO:SubProcess create_model() end ==================================
2023-03-05 17:47:54,556:INFO:Creating metrics dataframe
2023-03-05 17:47:54,575:INFO:Initializing Gradient Boosting Regressor
2023-03-05 17:47:54,576:INFO:Total runtime is 0.44124781290690096 minutes
2023-03-05 17:47:54,579:INFO:SubProcess create_model() called ==================================
2023-03-05 17:47:54,579:INFO:Initializing create_model()
2023-03-05 17:47:54,579:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000153055CF310>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:47:54,580:INFO:Checking exceptions
2023-03-05 17:47:54,580:INFO:Importing libraries
2023-03-05 17:47:54,580:INFO:Copying training dataset
2023-03-05 17:47:54,589:INFO:Defining folds
2023-03-05 17:47:54,589:INFO:Declaring metric variables
2023-03-05 17:47:54,593:INFO:Importing untrained model
2023-03-05 17:47:54,598:INFO:Gradient Boosting Regressor Imported successfully
2023-03-05 17:47:54,607:INFO:Starting cross validation
2023-03-05 17:47:54,609:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:47:56,258:INFO:Calculating mean and std
2023-03-05 17:47:56,260:INFO:Creating metrics dataframe
2023-03-05 17:47:56,263:INFO:Uploading results into container
2023-03-05 17:47:56,264:INFO:Uploading model into container now
2023-03-05 17:47:56,264:INFO:_master_model_container: 16
2023-03-05 17:47:56,264:INFO:_display_container: 2
2023-03-05 17:47:56,265:INFO:GradientBoostingRegressor(random_state=7278)
2023-03-05 17:47:56,265:INFO:create_model() successfully completed......................................
2023-03-05 17:47:56,451:INFO:SubProcess create_model() end ==================================
2023-03-05 17:47:56,451:INFO:Creating metrics dataframe
2023-03-05 17:47:56,467:INFO:Initializing Extreme Gradient Boosting
2023-03-05 17:47:56,467:INFO:Total runtime is 0.4727617422739664 minutes
2023-03-05 17:47:56,473:INFO:SubProcess create_model() called ==================================
2023-03-05 17:47:56,474:INFO:Initializing create_model()
2023-03-05 17:47:56,474:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000153055CF310>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:47:56,474:INFO:Checking exceptions
2023-03-05 17:47:56,475:INFO:Importing libraries
2023-03-05 17:47:56,475:INFO:Copying training dataset
2023-03-05 17:47:56,481:INFO:Defining folds
2023-03-05 17:47:56,481:INFO:Declaring metric variables
2023-03-05 17:47:56,486:INFO:Importing untrained model
2023-03-05 17:47:56,492:INFO:Extreme Gradient Boosting Imported successfully
2023-03-05 17:47:56,500:INFO:Starting cross validation
2023-03-05 17:47:56,505:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:47:58,316:INFO:Calculating mean and std
2023-03-05 17:47:58,318:INFO:Creating metrics dataframe
2023-03-05 17:47:58,322:INFO:Uploading results into container
2023-03-05 17:47:58,322:INFO:Uploading model into container now
2023-03-05 17:47:58,323:INFO:_master_model_container: 17
2023-03-05 17:47:58,323:INFO:_display_container: 2
2023-03-05 17:47:58,324:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=7278, ...)
2023-03-05 17:47:58,325:INFO:create_model() successfully completed......................................
2023-03-05 17:47:58,499:INFO:SubProcess create_model() end ==================================
2023-03-05 17:47:58,499:INFO:Creating metrics dataframe
2023-03-05 17:47:58,511:INFO:Initializing Light Gradient Boosting Machine
2023-03-05 17:47:58,512:INFO:Total runtime is 0.5068423668543497 minutes
2023-03-05 17:47:58,515:INFO:SubProcess create_model() called ==================================
2023-03-05 17:47:58,515:INFO:Initializing create_model()
2023-03-05 17:47:58,515:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000153055CF310>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:47:58,515:INFO:Checking exceptions
2023-03-05 17:47:58,515:INFO:Importing libraries
2023-03-05 17:47:58,515:INFO:Copying training dataset
2023-03-05 17:47:58,521:INFO:Defining folds
2023-03-05 17:47:58,521:INFO:Declaring metric variables
2023-03-05 17:47:58,525:INFO:Importing untrained model
2023-03-05 17:47:58,530:INFO:Light Gradient Boosting Machine Imported successfully
2023-03-05 17:47:58,537:INFO:Starting cross validation
2023-03-05 17:47:58,539:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:48:01,828:INFO:Calculating mean and std
2023-03-05 17:48:01,830:INFO:Creating metrics dataframe
2023-03-05 17:48:01,834:INFO:Uploading results into container
2023-03-05 17:48:01,835:INFO:Uploading model into container now
2023-03-05 17:48:01,836:INFO:_master_model_container: 18
2023-03-05 17:48:01,836:INFO:_display_container: 2
2023-03-05 17:48:01,836:INFO:LGBMRegressor(random_state=7278)
2023-03-05 17:48:01,836:INFO:create_model() successfully completed......................................
2023-03-05 17:48:02,029:INFO:SubProcess create_model() end ==================================
2023-03-05 17:48:02,029:INFO:Creating metrics dataframe
2023-03-05 17:48:02,045:INFO:Initializing Dummy Regressor
2023-03-05 17:48:02,045:INFO:Total runtime is 0.5657269199689229 minutes
2023-03-05 17:48:02,049:INFO:SubProcess create_model() called ==================================
2023-03-05 17:48:02,050:INFO:Initializing create_model()
2023-03-05 17:48:02,050:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000153055CF310>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:48:02,050:INFO:Checking exceptions
2023-03-05 17:48:02,050:INFO:Importing libraries
2023-03-05 17:48:02,050:INFO:Copying training dataset
2023-03-05 17:48:02,057:INFO:Defining folds
2023-03-05 17:48:02,058:INFO:Declaring metric variables
2023-03-05 17:48:02,062:INFO:Importing untrained model
2023-03-05 17:48:02,068:INFO:Dummy Regressor Imported successfully
2023-03-05 17:48:02,077:INFO:Starting cross validation
2023-03-05 17:48:02,079:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:48:03,099:INFO:Calculating mean and std
2023-03-05 17:48:03,101:INFO:Creating metrics dataframe
2023-03-05 17:48:03,105:INFO:Uploading results into container
2023-03-05 17:48:03,106:INFO:Uploading model into container now
2023-03-05 17:48:03,107:INFO:_master_model_container: 19
2023-03-05 17:48:03,107:INFO:_display_container: 2
2023-03-05 17:48:03,107:INFO:DummyRegressor()
2023-03-05 17:48:03,107:INFO:create_model() successfully completed......................................
2023-03-05 17:48:03,281:INFO:SubProcess create_model() end ==================================
2023-03-05 17:48:03,282:INFO:Creating metrics dataframe
2023-03-05 17:48:03,308:INFO:Initializing create_model()
2023-03-05 17:48:03,308:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=GradientBoostingRegressor(random_state=7278), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:48:03,308:INFO:Checking exceptions
2023-03-05 17:48:03,311:INFO:Importing libraries
2023-03-05 17:48:03,311:INFO:Copying training dataset
2023-03-05 17:48:03,315:INFO:Defining folds
2023-03-05 17:48:03,315:INFO:Declaring metric variables
2023-03-05 17:48:03,316:INFO:Importing untrained model
2023-03-05 17:48:03,316:INFO:Declaring custom model
2023-03-05 17:48:03,317:INFO:Gradient Boosting Regressor Imported successfully
2023-03-05 17:48:03,318:INFO:Cross validation set to False
2023-03-05 17:48:03,318:INFO:Fitting Model
2023-03-05 17:48:03,498:INFO:GradientBoostingRegressor(random_state=7278)
2023-03-05 17:48:03,499:INFO:create_model() successfully completed......................................
2023-03-05 17:48:03,691:INFO:_master_model_container: 19
2023-03-05 17:48:03,691:INFO:_display_container: 2
2023-03-05 17:48:03,692:INFO:GradientBoostingRegressor(random_state=7278)
2023-03-05 17:48:03,693:INFO:compare_models() successfully completed......................................
2023-03-05 17:48:03,707:INFO:Initializing evaluate_model()
2023-03-05 17:48:03,707:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=GradientBoostingRegressor(random_state=7278), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-03-05 17:48:03,733:INFO:Initializing plot_model()
2023-03-05 17:48:03,734:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=7278), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, system=True)
2023-03-05 17:48:03,735:INFO:Checking exceptions
2023-03-05 17:48:03,740:INFO:Preloading libraries
2023-03-05 17:48:03,752:INFO:Copying training dataset
2023-03-05 17:48:03,752:INFO:Plot type: pipeline
2023-03-05 17:48:03,838:INFO:Visual Rendered Successfully
2023-03-05 17:48:04,002:INFO:plot_model() successfully completed......................................
2023-03-05 17:48:04,022:INFO:Initializing predict_model()
2023-03-05 17:48:04,022:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=GradientBoostingRegressor(random_state=7278), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x0000015304670DC0>)
2023-03-05 17:48:04,022:INFO:Checking exceptions
2023-03-05 17:48:04,022:INFO:Preloading libraries
2023-03-05 17:48:04,025:INFO:Set up data.
2023-03-05 17:48:04,030:INFO:Set up index.
2023-03-05 17:49:18,278:INFO:Initializing compare_models()
2023-03-05 17:49:18,278:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, include=None, fold=None, round=4, cross_validation=True, sort=RMSE, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'RMSE', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-03-05 17:49:18,278:INFO:Checking exceptions
2023-03-05 17:49:18,281:INFO:Preparing display monitor
2023-03-05 17:49:18,324:INFO:Initializing Linear Regression
2023-03-05 17:49:18,324:INFO:Total runtime is 0.0 minutes
2023-03-05 17:49:18,330:INFO:SubProcess create_model() called ==================================
2023-03-05 17:49:18,331:INFO:Initializing create_model()
2023-03-05 17:49:18,331:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015303091280>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:49:18,331:INFO:Checking exceptions
2023-03-05 17:49:18,331:INFO:Importing libraries
2023-03-05 17:49:18,331:INFO:Copying training dataset
2023-03-05 17:49:18,342:INFO:Defining folds
2023-03-05 17:49:18,342:INFO:Declaring metric variables
2023-03-05 17:49:18,346:INFO:Importing untrained model
2023-03-05 17:49:18,352:INFO:Linear Regression Imported successfully
2023-03-05 17:49:18,363:INFO:Starting cross validation
2023-03-05 17:49:18,365:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:49:18,982:INFO:Calculating mean and std
2023-03-05 17:49:18,983:INFO:Creating metrics dataframe
2023-03-05 17:49:18,985:INFO:Uploading results into container
2023-03-05 17:49:18,986:INFO:Uploading model into container now
2023-03-05 17:49:18,986:INFO:_master_model_container: 20
2023-03-05 17:49:18,986:INFO:_display_container: 3
2023-03-05 17:49:18,986:INFO:LinearRegression(n_jobs=-1)
2023-03-05 17:49:18,987:INFO:create_model() successfully completed......................................
2023-03-05 17:49:19,140:INFO:SubProcess create_model() end ==================================
2023-03-05 17:49:19,140:INFO:Creating metrics dataframe
2023-03-05 17:49:19,147:INFO:Initializing Lasso Regression
2023-03-05 17:49:19,147:INFO:Total runtime is 0.013713300228118896 minutes
2023-03-05 17:49:19,149:INFO:SubProcess create_model() called ==================================
2023-03-05 17:49:19,150:INFO:Initializing create_model()
2023-03-05 17:49:19,150:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015303091280>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:49:19,150:INFO:Checking exceptions
2023-03-05 17:49:19,150:INFO:Importing libraries
2023-03-05 17:49:19,150:INFO:Copying training dataset
2023-03-05 17:49:19,155:INFO:Defining folds
2023-03-05 17:49:19,155:INFO:Declaring metric variables
2023-03-05 17:49:19,158:INFO:Importing untrained model
2023-03-05 17:49:19,162:INFO:Lasso Regression Imported successfully
2023-03-05 17:49:19,170:INFO:Starting cross validation
2023-03-05 17:49:19,172:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:49:19,422:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.257e+09, tolerance: 1.512e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-05 17:49:19,750:INFO:Calculating mean and std
2023-03-05 17:49:19,751:INFO:Creating metrics dataframe
2023-03-05 17:49:19,754:INFO:Uploading results into container
2023-03-05 17:49:19,754:INFO:Uploading model into container now
2023-03-05 17:49:19,754:INFO:_master_model_container: 21
2023-03-05 17:49:19,754:INFO:_display_container: 3
2023-03-05 17:49:19,755:INFO:Lasso(random_state=7278)
2023-03-05 17:49:19,755:INFO:create_model() successfully completed......................................
2023-03-05 17:49:19,909:INFO:SubProcess create_model() end ==================================
2023-03-05 17:49:19,909:INFO:Creating metrics dataframe
2023-03-05 17:49:19,919:INFO:Initializing Ridge Regression
2023-03-05 17:49:19,920:INFO:Total runtime is 0.02658976713816325 minutes
2023-03-05 17:49:19,924:INFO:SubProcess create_model() called ==================================
2023-03-05 17:49:19,924:INFO:Initializing create_model()
2023-03-05 17:49:19,925:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015303091280>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:49:19,925:INFO:Checking exceptions
2023-03-05 17:49:19,925:INFO:Importing libraries
2023-03-05 17:49:19,925:INFO:Copying training dataset
2023-03-05 17:49:19,930:INFO:Defining folds
2023-03-05 17:49:19,930:INFO:Declaring metric variables
2023-03-05 17:49:19,934:INFO:Importing untrained model
2023-03-05 17:49:19,941:INFO:Ridge Regression Imported successfully
2023-03-05 17:49:19,948:INFO:Starting cross validation
2023-03-05 17:49:19,950:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:49:20,528:INFO:Calculating mean and std
2023-03-05 17:49:20,530:INFO:Creating metrics dataframe
2023-03-05 17:49:20,532:INFO:Uploading results into container
2023-03-05 17:49:20,533:INFO:Uploading model into container now
2023-03-05 17:49:20,533:INFO:_master_model_container: 22
2023-03-05 17:49:20,533:INFO:_display_container: 3
2023-03-05 17:49:20,533:INFO:Ridge(random_state=7278)
2023-03-05 17:49:20,533:INFO:create_model() successfully completed......................................
2023-03-05 17:49:20,689:INFO:SubProcess create_model() end ==================================
2023-03-05 17:49:20,689:INFO:Creating metrics dataframe
2023-03-05 17:49:20,700:INFO:Initializing Elastic Net
2023-03-05 17:49:20,700:INFO:Total runtime is 0.039585487047831214 minutes
2023-03-05 17:49:20,706:INFO:SubProcess create_model() called ==================================
2023-03-05 17:49:20,707:INFO:Initializing create_model()
2023-03-05 17:49:20,707:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015303091280>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:49:20,707:INFO:Checking exceptions
2023-03-05 17:49:20,707:INFO:Importing libraries
2023-03-05 17:49:20,707:INFO:Copying training dataset
2023-03-05 17:49:20,714:INFO:Defining folds
2023-03-05 17:49:20,714:INFO:Declaring metric variables
2023-03-05 17:49:20,717:INFO:Importing untrained model
2023-03-05 17:49:20,721:INFO:Elastic Net Imported successfully
2023-03-05 17:49:20,729:INFO:Starting cross validation
2023-03-05 17:49:20,730:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:49:21,314:INFO:Calculating mean and std
2023-03-05 17:49:21,316:INFO:Creating metrics dataframe
2023-03-05 17:49:21,318:INFO:Uploading results into container
2023-03-05 17:49:21,319:INFO:Uploading model into container now
2023-03-05 17:49:21,319:INFO:_master_model_container: 23
2023-03-05 17:49:21,319:INFO:_display_container: 3
2023-03-05 17:49:21,320:INFO:ElasticNet(random_state=7278)
2023-03-05 17:49:21,320:INFO:create_model() successfully completed......................................
2023-03-05 17:49:21,483:INFO:SubProcess create_model() end ==================================
2023-03-05 17:49:21,483:INFO:Creating metrics dataframe
2023-03-05 17:49:21,492:INFO:Initializing Least Angle Regression
2023-03-05 17:49:21,492:INFO:Total runtime is 0.05279167493184407 minutes
2023-03-05 17:49:21,495:INFO:SubProcess create_model() called ==================================
2023-03-05 17:49:21,495:INFO:Initializing create_model()
2023-03-05 17:49:21,496:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015303091280>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:49:21,496:INFO:Checking exceptions
2023-03-05 17:49:21,496:INFO:Importing libraries
2023-03-05 17:49:21,496:INFO:Copying training dataset
2023-03-05 17:49:21,501:INFO:Defining folds
2023-03-05 17:49:21,501:INFO:Declaring metric variables
2023-03-05 17:49:21,505:INFO:Importing untrained model
2023-03-05 17:49:21,508:INFO:Least Angle Regression Imported successfully
2023-03-05 17:49:21,516:INFO:Starting cross validation
2023-03-05 17:49:21,519:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:49:21,764:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:49:21,812:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:49:21,846:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:49:21,851:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:49:21,867:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:49:21,896:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:49:21,896:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:49:21,906:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:49:22,028:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:49:22,047:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:49:22,110:INFO:Calculating mean and std
2023-03-05 17:49:22,112:INFO:Creating metrics dataframe
2023-03-05 17:49:22,115:INFO:Uploading results into container
2023-03-05 17:49:22,115:INFO:Uploading model into container now
2023-03-05 17:49:22,116:INFO:_master_model_container: 24
2023-03-05 17:49:22,116:INFO:_display_container: 3
2023-03-05 17:49:22,116:INFO:Lars(random_state=7278)
2023-03-05 17:49:22,116:INFO:create_model() successfully completed......................................
2023-03-05 17:49:22,279:INFO:SubProcess create_model() end ==================================
2023-03-05 17:49:22,279:INFO:Creating metrics dataframe
2023-03-05 17:49:22,288:INFO:Initializing Lasso Least Angle Regression
2023-03-05 17:49:22,288:INFO:Total runtime is 0.06605863173802694 minutes
2023-03-05 17:49:22,291:INFO:SubProcess create_model() called ==================================
2023-03-05 17:49:22,292:INFO:Initializing create_model()
2023-03-05 17:49:22,292:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015303091280>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:49:22,292:INFO:Checking exceptions
2023-03-05 17:49:22,292:INFO:Importing libraries
2023-03-05 17:49:22,292:INFO:Copying training dataset
2023-03-05 17:49:22,297:INFO:Defining folds
2023-03-05 17:49:22,298:INFO:Declaring metric variables
2023-03-05 17:49:22,302:INFO:Importing untrained model
2023-03-05 17:49:22,306:INFO:Lasso Least Angle Regression Imported successfully
2023-03-05 17:49:22,313:INFO:Starting cross validation
2023-03-05 17:49:22,315:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:49:22,551:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:49:22,605:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:49:22,629:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:49:22,651:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:49:22,667:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:49:22,691:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:49:22,712:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:49:22,834:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:49:22,842:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:49:22,905:INFO:Calculating mean and std
2023-03-05 17:49:22,907:INFO:Creating metrics dataframe
2023-03-05 17:49:22,909:INFO:Uploading results into container
2023-03-05 17:49:22,910:INFO:Uploading model into container now
2023-03-05 17:49:22,910:INFO:_master_model_container: 25
2023-03-05 17:49:22,910:INFO:_display_container: 3
2023-03-05 17:49:22,910:INFO:LassoLars(random_state=7278)
2023-03-05 17:49:22,910:INFO:create_model() successfully completed......................................
2023-03-05 17:49:23,065:INFO:SubProcess create_model() end ==================================
2023-03-05 17:49:23,065:INFO:Creating metrics dataframe
2023-03-05 17:49:23,078:INFO:Initializing Orthogonal Matching Pursuit
2023-03-05 17:49:23,078:INFO:Total runtime is 0.07922439972559611 minutes
2023-03-05 17:49:23,082:INFO:SubProcess create_model() called ==================================
2023-03-05 17:49:23,083:INFO:Initializing create_model()
2023-03-05 17:49:23,083:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015303091280>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:49:23,083:INFO:Checking exceptions
2023-03-05 17:49:23,083:INFO:Importing libraries
2023-03-05 17:49:23,083:INFO:Copying training dataset
2023-03-05 17:49:23,090:INFO:Defining folds
2023-03-05 17:49:23,090:INFO:Declaring metric variables
2023-03-05 17:49:23,096:INFO:Importing untrained model
2023-03-05 17:49:23,100:INFO:Orthogonal Matching Pursuit Imported successfully
2023-03-05 17:49:23,109:INFO:Starting cross validation
2023-03-05 17:49:23,113:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:49:23,375:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:49:23,409:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:49:23,421:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:49:23,431:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:49:23,456:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:49:23,473:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:49:23,474:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:49:23,490:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:49:23,611:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:49:23,626:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:49:23,688:INFO:Calculating mean and std
2023-03-05 17:49:23,689:INFO:Creating metrics dataframe
2023-03-05 17:49:23,693:INFO:Uploading results into container
2023-03-05 17:49:23,693:INFO:Uploading model into container now
2023-03-05 17:49:23,694:INFO:_master_model_container: 26
2023-03-05 17:49:23,694:INFO:_display_container: 3
2023-03-05 17:49:23,694:INFO:OrthogonalMatchingPursuit()
2023-03-05 17:49:23,694:INFO:create_model() successfully completed......................................
2023-03-05 17:49:23,849:INFO:SubProcess create_model() end ==================================
2023-03-05 17:49:23,849:INFO:Creating metrics dataframe
2023-03-05 17:49:23,861:INFO:Initializing Bayesian Ridge
2023-03-05 17:49:23,861:INFO:Total runtime is 0.09227150678634644 minutes
2023-03-05 17:49:23,866:INFO:SubProcess create_model() called ==================================
2023-03-05 17:49:23,867:INFO:Initializing create_model()
2023-03-05 17:49:23,867:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015303091280>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:49:23,867:INFO:Checking exceptions
2023-03-05 17:49:23,867:INFO:Importing libraries
2023-03-05 17:49:23,867:INFO:Copying training dataset
2023-03-05 17:49:23,872:INFO:Defining folds
2023-03-05 17:49:23,873:INFO:Declaring metric variables
2023-03-05 17:49:23,877:INFO:Importing untrained model
2023-03-05 17:49:23,881:INFO:Bayesian Ridge Imported successfully
2023-03-05 17:49:23,889:INFO:Starting cross validation
2023-03-05 17:49:23,892:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:49:24,473:INFO:Calculating mean and std
2023-03-05 17:49:24,474:INFO:Creating metrics dataframe
2023-03-05 17:49:24,477:INFO:Uploading results into container
2023-03-05 17:49:24,477:INFO:Uploading model into container now
2023-03-05 17:49:24,478:INFO:_master_model_container: 27
2023-03-05 17:49:24,478:INFO:_display_container: 3
2023-03-05 17:49:24,478:INFO:BayesianRidge()
2023-03-05 17:49:24,478:INFO:create_model() successfully completed......................................
2023-03-05 17:49:24,631:INFO:SubProcess create_model() end ==================================
2023-03-05 17:49:24,631:INFO:Creating metrics dataframe
2023-03-05 17:49:24,642:INFO:Initializing Passive Aggressive Regressor
2023-03-05 17:49:24,642:INFO:Total runtime is 0.10530128081639607 minutes
2023-03-05 17:49:24,646:INFO:SubProcess create_model() called ==================================
2023-03-05 17:49:24,647:INFO:Initializing create_model()
2023-03-05 17:49:24,647:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015303091280>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:49:24,647:INFO:Checking exceptions
2023-03-05 17:49:24,647:INFO:Importing libraries
2023-03-05 17:49:24,647:INFO:Copying training dataset
2023-03-05 17:49:24,653:INFO:Defining folds
2023-03-05 17:49:24,653:INFO:Declaring metric variables
2023-03-05 17:49:24,658:INFO:Importing untrained model
2023-03-05 17:49:24,664:INFO:Passive Aggressive Regressor Imported successfully
2023-03-05 17:49:24,673:INFO:Starting cross validation
2023-03-05 17:49:24,676:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:49:25,248:INFO:Calculating mean and std
2023-03-05 17:49:25,250:INFO:Creating metrics dataframe
2023-03-05 17:49:25,253:INFO:Uploading results into container
2023-03-05 17:49:25,253:INFO:Uploading model into container now
2023-03-05 17:49:25,254:INFO:_master_model_container: 28
2023-03-05 17:49:25,254:INFO:_display_container: 3
2023-03-05 17:49:25,254:INFO:PassiveAggressiveRegressor(random_state=7278)
2023-03-05 17:49:25,254:INFO:create_model() successfully completed......................................
2023-03-05 17:49:25,410:INFO:SubProcess create_model() end ==================================
2023-03-05 17:49:25,410:INFO:Creating metrics dataframe
2023-03-05 17:49:25,420:INFO:Initializing Huber Regressor
2023-03-05 17:49:25,420:INFO:Total runtime is 0.11826515595118205 minutes
2023-03-05 17:49:25,423:INFO:SubProcess create_model() called ==================================
2023-03-05 17:49:25,423:INFO:Initializing create_model()
2023-03-05 17:49:25,424:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015303091280>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:49:25,424:INFO:Checking exceptions
2023-03-05 17:49:25,424:INFO:Importing libraries
2023-03-05 17:49:25,424:INFO:Copying training dataset
2023-03-05 17:49:25,430:INFO:Defining folds
2023-03-05 17:49:25,430:INFO:Declaring metric variables
2023-03-05 17:49:25,434:INFO:Importing untrained model
2023-03-05 17:49:25,438:INFO:Huber Regressor Imported successfully
2023-03-05 17:49:25,448:INFO:Starting cross validation
2023-03-05 17:49:25,449:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:49:26,097:INFO:Calculating mean and std
2023-03-05 17:49:26,098:INFO:Creating metrics dataframe
2023-03-05 17:49:26,101:INFO:Uploading results into container
2023-03-05 17:49:26,101:INFO:Uploading model into container now
2023-03-05 17:49:26,101:INFO:_master_model_container: 29
2023-03-05 17:49:26,102:INFO:_display_container: 3
2023-03-05 17:49:26,102:INFO:HuberRegressor()
2023-03-05 17:49:26,102:INFO:create_model() successfully completed......................................
2023-03-05 17:49:26,262:INFO:SubProcess create_model() end ==================================
2023-03-05 17:49:26,262:INFO:Creating metrics dataframe
2023-03-05 17:49:26,272:INFO:Initializing K Neighbors Regressor
2023-03-05 17:49:26,272:INFO:Total runtime is 0.13245190779368082 minutes
2023-03-05 17:49:26,275:INFO:SubProcess create_model() called ==================================
2023-03-05 17:49:26,275:INFO:Initializing create_model()
2023-03-05 17:49:26,275:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015303091280>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:49:26,275:INFO:Checking exceptions
2023-03-05 17:49:26,276:INFO:Importing libraries
2023-03-05 17:49:26,276:INFO:Copying training dataset
2023-03-05 17:49:26,280:INFO:Defining folds
2023-03-05 17:49:26,280:INFO:Declaring metric variables
2023-03-05 17:49:26,284:INFO:Importing untrained model
2023-03-05 17:49:26,288:INFO:K Neighbors Regressor Imported successfully
2023-03-05 17:49:26,296:INFO:Starting cross validation
2023-03-05 17:49:26,298:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:49:26,869:INFO:Calculating mean and std
2023-03-05 17:49:26,870:INFO:Creating metrics dataframe
2023-03-05 17:49:26,873:INFO:Uploading results into container
2023-03-05 17:49:26,873:INFO:Uploading model into container now
2023-03-05 17:49:26,874:INFO:_master_model_container: 30
2023-03-05 17:49:26,874:INFO:_display_container: 3
2023-03-05 17:49:26,874:INFO:KNeighborsRegressor(n_jobs=-1)
2023-03-05 17:49:26,874:INFO:create_model() successfully completed......................................
2023-03-05 17:49:27,040:INFO:SubProcess create_model() end ==================================
2023-03-05 17:49:27,040:INFO:Creating metrics dataframe
2023-03-05 17:49:27,049:INFO:Initializing Decision Tree Regressor
2023-03-05 17:49:27,049:INFO:Total runtime is 0.14541816314061481 minutes
2023-03-05 17:49:27,053:INFO:SubProcess create_model() called ==================================
2023-03-05 17:49:27,053:INFO:Initializing create_model()
2023-03-05 17:49:27,053:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015303091280>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:49:27,053:INFO:Checking exceptions
2023-03-05 17:49:27,053:INFO:Importing libraries
2023-03-05 17:49:27,053:INFO:Copying training dataset
2023-03-05 17:49:27,060:INFO:Defining folds
2023-03-05 17:49:27,060:INFO:Declaring metric variables
2023-03-05 17:49:27,064:INFO:Importing untrained model
2023-03-05 17:49:27,068:INFO:Decision Tree Regressor Imported successfully
2023-03-05 17:49:27,077:INFO:Starting cross validation
2023-03-05 17:49:27,079:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:49:27,646:INFO:Calculating mean and std
2023-03-05 17:49:27,647:INFO:Creating metrics dataframe
2023-03-05 17:49:27,650:INFO:Uploading results into container
2023-03-05 17:49:27,651:INFO:Uploading model into container now
2023-03-05 17:49:27,651:INFO:_master_model_container: 31
2023-03-05 17:49:27,651:INFO:_display_container: 3
2023-03-05 17:49:27,652:INFO:DecisionTreeRegressor(random_state=7278)
2023-03-05 17:49:27,652:INFO:create_model() successfully completed......................................
2023-03-05 17:49:27,810:INFO:SubProcess create_model() end ==================================
2023-03-05 17:49:27,810:INFO:Creating metrics dataframe
2023-03-05 17:49:27,820:INFO:Initializing Random Forest Regressor
2023-03-05 17:49:27,821:INFO:Total runtime is 0.15828439791997273 minutes
2023-03-05 17:49:27,824:INFO:SubProcess create_model() called ==================================
2023-03-05 17:49:27,825:INFO:Initializing create_model()
2023-03-05 17:49:27,825:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015303091280>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:49:27,825:INFO:Checking exceptions
2023-03-05 17:49:27,825:INFO:Importing libraries
2023-03-05 17:49:27,825:INFO:Copying training dataset
2023-03-05 17:49:27,830:INFO:Defining folds
2023-03-05 17:49:27,831:INFO:Declaring metric variables
2023-03-05 17:49:27,835:INFO:Importing untrained model
2023-03-05 17:49:27,839:INFO:Random Forest Regressor Imported successfully
2023-03-05 17:49:27,847:INFO:Starting cross validation
2023-03-05 17:49:27,850:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:49:28,692:INFO:Calculating mean and std
2023-03-05 17:49:28,694:INFO:Creating metrics dataframe
2023-03-05 17:49:28,697:INFO:Uploading results into container
2023-03-05 17:49:28,698:INFO:Uploading model into container now
2023-03-05 17:49:28,698:INFO:_master_model_container: 32
2023-03-05 17:49:28,698:INFO:_display_container: 3
2023-03-05 17:49:28,699:INFO:RandomForestRegressor(n_jobs=-1, random_state=7278)
2023-03-05 17:49:28,699:INFO:create_model() successfully completed......................................
2023-03-05 17:49:28,887:INFO:SubProcess create_model() end ==================================
2023-03-05 17:49:28,887:INFO:Creating metrics dataframe
2023-03-05 17:49:28,897:INFO:Initializing Extra Trees Regressor
2023-03-05 17:49:28,897:INFO:Total runtime is 0.1762134790420532 minutes
2023-03-05 17:49:28,900:INFO:SubProcess create_model() called ==================================
2023-03-05 17:49:28,901:INFO:Initializing create_model()
2023-03-05 17:49:28,901:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015303091280>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:49:28,901:INFO:Checking exceptions
2023-03-05 17:49:28,901:INFO:Importing libraries
2023-03-05 17:49:28,901:INFO:Copying training dataset
2023-03-05 17:49:28,906:INFO:Defining folds
2023-03-05 17:49:28,907:INFO:Declaring metric variables
2023-03-05 17:49:28,911:INFO:Importing untrained model
2023-03-05 17:49:28,916:INFO:Extra Trees Regressor Imported successfully
2023-03-05 17:49:28,923:INFO:Starting cross validation
2023-03-05 17:49:28,925:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:49:29,765:INFO:Calculating mean and std
2023-03-05 17:49:29,767:INFO:Creating metrics dataframe
2023-03-05 17:49:29,770:INFO:Uploading results into container
2023-03-05 17:49:29,770:INFO:Uploading model into container now
2023-03-05 17:49:29,770:INFO:_master_model_container: 33
2023-03-05 17:49:29,770:INFO:_display_container: 3
2023-03-05 17:49:29,771:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=7278)
2023-03-05 17:49:29,771:INFO:create_model() successfully completed......................................
2023-03-05 17:49:29,932:INFO:SubProcess create_model() end ==================================
2023-03-05 17:49:29,932:INFO:Creating metrics dataframe
2023-03-05 17:49:29,943:INFO:Initializing AdaBoost Regressor
2023-03-05 17:49:29,943:INFO:Total runtime is 0.19364476601282754 minutes
2023-03-05 17:49:29,946:INFO:SubProcess create_model() called ==================================
2023-03-05 17:49:29,947:INFO:Initializing create_model()
2023-03-05 17:49:29,947:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015303091280>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:49:29,947:INFO:Checking exceptions
2023-03-05 17:49:29,947:INFO:Importing libraries
2023-03-05 17:49:29,947:INFO:Copying training dataset
2023-03-05 17:49:29,952:INFO:Defining folds
2023-03-05 17:49:29,953:INFO:Declaring metric variables
2023-03-05 17:49:29,957:INFO:Importing untrained model
2023-03-05 17:49:29,962:INFO:AdaBoost Regressor Imported successfully
2023-03-05 17:49:29,970:INFO:Starting cross validation
2023-03-05 17:49:29,973:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:49:30,675:INFO:Calculating mean and std
2023-03-05 17:49:30,676:INFO:Creating metrics dataframe
2023-03-05 17:49:30,680:INFO:Uploading results into container
2023-03-05 17:49:30,680:INFO:Uploading model into container now
2023-03-05 17:49:30,681:INFO:_master_model_container: 34
2023-03-05 17:49:30,681:INFO:_display_container: 3
2023-03-05 17:49:30,681:INFO:AdaBoostRegressor(random_state=7278)
2023-03-05 17:49:30,681:INFO:create_model() successfully completed......................................
2023-03-05 17:49:30,856:INFO:SubProcess create_model() end ==================================
2023-03-05 17:49:30,856:INFO:Creating metrics dataframe
2023-03-05 17:49:30,867:INFO:Initializing Gradient Boosting Regressor
2023-03-05 17:49:30,867:INFO:Total runtime is 0.20904768308003743 minutes
2023-03-05 17:49:30,871:INFO:SubProcess create_model() called ==================================
2023-03-05 17:49:30,871:INFO:Initializing create_model()
2023-03-05 17:49:30,872:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015303091280>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:49:30,872:INFO:Checking exceptions
2023-03-05 17:49:30,872:INFO:Importing libraries
2023-03-05 17:49:30,872:INFO:Copying training dataset
2023-03-05 17:49:30,877:INFO:Defining folds
2023-03-05 17:49:30,877:INFO:Declaring metric variables
2023-03-05 17:49:30,881:INFO:Importing untrained model
2023-03-05 17:49:30,886:INFO:Gradient Boosting Regressor Imported successfully
2023-03-05 17:49:30,894:INFO:Starting cross validation
2023-03-05 17:49:30,896:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:49:31,950:INFO:Calculating mean and std
2023-03-05 17:49:31,952:INFO:Creating metrics dataframe
2023-03-05 17:49:31,955:INFO:Uploading results into container
2023-03-05 17:49:31,956:INFO:Uploading model into container now
2023-03-05 17:49:31,956:INFO:_master_model_container: 35
2023-03-05 17:49:31,957:INFO:_display_container: 3
2023-03-05 17:49:31,957:INFO:GradientBoostingRegressor(random_state=7278)
2023-03-05 17:49:31,957:INFO:create_model() successfully completed......................................
2023-03-05 17:49:32,117:INFO:SubProcess create_model() end ==================================
2023-03-05 17:49:32,117:INFO:Creating metrics dataframe
2023-03-05 17:49:32,128:INFO:Initializing Extreme Gradient Boosting
2023-03-05 17:49:32,128:INFO:Total runtime is 0.23005958795547485 minutes
2023-03-05 17:49:32,131:INFO:SubProcess create_model() called ==================================
2023-03-05 17:49:32,132:INFO:Initializing create_model()
2023-03-05 17:49:32,132:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015303091280>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:49:32,132:INFO:Checking exceptions
2023-03-05 17:49:32,132:INFO:Importing libraries
2023-03-05 17:49:32,132:INFO:Copying training dataset
2023-03-05 17:49:32,138:INFO:Defining folds
2023-03-05 17:49:32,138:INFO:Declaring metric variables
2023-03-05 17:49:32,141:INFO:Importing untrained model
2023-03-05 17:49:32,146:INFO:Extreme Gradient Boosting Imported successfully
2023-03-05 17:49:32,154:INFO:Starting cross validation
2023-03-05 17:49:32,156:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:49:32,779:INFO:Calculating mean and std
2023-03-05 17:49:32,781:INFO:Creating metrics dataframe
2023-03-05 17:49:32,784:INFO:Uploading results into container
2023-03-05 17:49:32,785:INFO:Uploading model into container now
2023-03-05 17:49:32,785:INFO:_master_model_container: 36
2023-03-05 17:49:32,785:INFO:_display_container: 3
2023-03-05 17:49:32,786:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=7278, ...)
2023-03-05 17:49:32,786:INFO:create_model() successfully completed......................................
2023-03-05 17:49:32,946:INFO:SubProcess create_model() end ==================================
2023-03-05 17:49:32,947:INFO:Creating metrics dataframe
2023-03-05 17:49:32,958:INFO:Initializing Light Gradient Boosting Machine
2023-03-05 17:49:32,959:INFO:Total runtime is 0.24390710592269896 minutes
2023-03-05 17:49:32,965:INFO:SubProcess create_model() called ==================================
2023-03-05 17:49:32,965:INFO:Initializing create_model()
2023-03-05 17:49:32,965:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015303091280>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:49:32,966:INFO:Checking exceptions
2023-03-05 17:49:32,966:INFO:Importing libraries
2023-03-05 17:49:32,966:INFO:Copying training dataset
2023-03-05 17:49:32,971:INFO:Defining folds
2023-03-05 17:49:32,971:INFO:Declaring metric variables
2023-03-05 17:49:32,975:INFO:Importing untrained model
2023-03-05 17:49:32,980:INFO:Light Gradient Boosting Machine Imported successfully
2023-03-05 17:49:32,989:INFO:Starting cross validation
2023-03-05 17:49:32,990:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:49:33,612:INFO:Calculating mean and std
2023-03-05 17:49:33,613:INFO:Creating metrics dataframe
2023-03-05 17:49:33,616:INFO:Uploading results into container
2023-03-05 17:49:33,616:INFO:Uploading model into container now
2023-03-05 17:49:33,616:INFO:_master_model_container: 37
2023-03-05 17:49:33,616:INFO:_display_container: 3
2023-03-05 17:49:33,617:INFO:LGBMRegressor(random_state=7278)
2023-03-05 17:49:33,617:INFO:create_model() successfully completed......................................
2023-03-05 17:49:33,775:INFO:SubProcess create_model() end ==================================
2023-03-05 17:49:33,775:INFO:Creating metrics dataframe
2023-03-05 17:49:33,788:INFO:Initializing Dummy Regressor
2023-03-05 17:49:33,788:INFO:Total runtime is 0.25772735675175984 minutes
2023-03-05 17:49:33,791:INFO:SubProcess create_model() called ==================================
2023-03-05 17:49:33,792:INFO:Initializing create_model()
2023-03-05 17:49:33,792:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015303091280>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:49:33,792:INFO:Checking exceptions
2023-03-05 17:49:33,792:INFO:Importing libraries
2023-03-05 17:49:33,792:INFO:Copying training dataset
2023-03-05 17:49:33,797:INFO:Defining folds
2023-03-05 17:49:33,797:INFO:Declaring metric variables
2023-03-05 17:49:33,802:INFO:Importing untrained model
2023-03-05 17:49:33,806:INFO:Dummy Regressor Imported successfully
2023-03-05 17:49:33,814:INFO:Starting cross validation
2023-03-05 17:49:33,817:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:49:34,357:INFO:Calculating mean and std
2023-03-05 17:49:34,358:INFO:Creating metrics dataframe
2023-03-05 17:49:34,361:INFO:Uploading results into container
2023-03-05 17:49:34,362:INFO:Uploading model into container now
2023-03-05 17:49:34,362:INFO:_master_model_container: 38
2023-03-05 17:49:34,363:INFO:_display_container: 3
2023-03-05 17:49:34,363:INFO:DummyRegressor()
2023-03-05 17:49:34,363:INFO:create_model() successfully completed......................................
2023-03-05 17:49:34,521:INFO:SubProcess create_model() end ==================================
2023-03-05 17:49:34,521:INFO:Creating metrics dataframe
2023-03-05 17:49:34,543:INFO:Initializing create_model()
2023-03-05 17:49:34,543:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015304609B80>, estimator=GradientBoostingRegressor(random_state=7278), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:49:34,543:INFO:Checking exceptions
2023-03-05 17:49:34,545:INFO:Importing libraries
2023-03-05 17:49:34,545:INFO:Copying training dataset
2023-03-05 17:49:34,550:INFO:Defining folds
2023-03-05 17:49:34,551:INFO:Declaring metric variables
2023-03-05 17:49:34,551:INFO:Importing untrained model
2023-03-05 17:49:34,551:INFO:Declaring custom model
2023-03-05 17:49:34,552:INFO:Gradient Boosting Regressor Imported successfully
2023-03-05 17:49:34,553:INFO:Cross validation set to False
2023-03-05 17:49:34,553:INFO:Fitting Model
2023-03-05 17:49:34,707:INFO:GradientBoostingRegressor(random_state=7278)
2023-03-05 17:49:34,707:INFO:create_model() successfully completed......................................
2023-03-05 17:49:34,896:INFO:_master_model_container: 38
2023-03-05 17:49:34,896:INFO:_display_container: 3
2023-03-05 17:49:34,896:INFO:GradientBoostingRegressor(random_state=7278)
2023-03-05 17:49:34,896:INFO:compare_models() successfully completed......................................
2023-03-05 17:50:20,665:INFO:Initializing compare_models()
2023-03-05 17:50:20,665:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, include=None, fold=None, round=4, cross_validation=True, sort=RMSE, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'RMSE', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-03-05 17:50:20,665:INFO:Checking exceptions
2023-03-05 17:50:20,668:INFO:Preparing display monitor
2023-03-05 17:50:20,709:INFO:Initializing Linear Regression
2023-03-05 17:50:20,709:INFO:Total runtime is 0.0 minutes
2023-03-05 17:50:20,717:INFO:SubProcess create_model() called ==================================
2023-03-05 17:50:20,717:INFO:Initializing create_model()
2023-03-05 17:50:20,717:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A56008550>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:50:20,718:INFO:Checking exceptions
2023-03-05 17:50:20,718:INFO:Importing libraries
2023-03-05 17:50:20,718:INFO:Copying training dataset
2023-03-05 17:50:20,729:INFO:Defining folds
2023-03-05 17:50:20,730:INFO:Declaring metric variables
2023-03-05 17:50:20,734:INFO:Importing untrained model
2023-03-05 17:50:20,738:INFO:Linear Regression Imported successfully
2023-03-05 17:50:20,746:INFO:Starting cross validation
2023-03-05 17:50:20,750:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:50:28,735:INFO:Calculating mean and std
2023-03-05 17:50:28,736:INFO:Creating metrics dataframe
2023-03-05 17:50:28,739:INFO:Uploading results into container
2023-03-05 17:50:28,740:INFO:Uploading model into container now
2023-03-05 17:50:28,741:INFO:_master_model_container: 20
2023-03-05 17:50:28,741:INFO:_display_container: 3
2023-03-05 17:50:28,741:INFO:LinearRegression(n_jobs=-1)
2023-03-05 17:50:28,741:INFO:create_model() successfully completed......................................
2023-03-05 17:50:28,829:INFO:SubProcess create_model() end ==================================
2023-03-05 17:50:28,829:INFO:Creating metrics dataframe
2023-03-05 17:50:28,836:INFO:Initializing Lasso Regression
2023-03-05 17:50:28,836:INFO:Total runtime is 0.13544274965922037 minutes
2023-03-05 17:50:28,839:INFO:SubProcess create_model() called ==================================
2023-03-05 17:50:28,840:INFO:Initializing create_model()
2023-03-05 17:50:28,840:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A56008550>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:50:28,840:INFO:Checking exceptions
2023-03-05 17:50:28,840:INFO:Importing libraries
2023-03-05 17:50:28,840:INFO:Copying training dataset
2023-03-05 17:50:28,846:INFO:Defining folds
2023-03-05 17:50:28,846:INFO:Declaring metric variables
2023-03-05 17:50:28,851:INFO:Importing untrained model
2023-03-05 17:50:28,856:INFO:Lasso Regression Imported successfully
2023-03-05 17:50:28,864:INFO:Starting cross validation
2023-03-05 17:50:28,866:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:50:29,135:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.275e+09, tolerance: 1.570e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-05 17:50:29,170:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.372e+09, tolerance: 1.529e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-05 17:50:29,179:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.249e+09, tolerance: 1.533e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-05 17:50:29,598:INFO:Calculating mean and std
2023-03-05 17:50:29,600:INFO:Creating metrics dataframe
2023-03-05 17:50:29,603:INFO:Uploading results into container
2023-03-05 17:50:29,604:INFO:Uploading model into container now
2023-03-05 17:50:29,604:INFO:_master_model_container: 21
2023-03-05 17:50:29,604:INFO:_display_container: 3
2023-03-05 17:50:29,604:INFO:Lasso(random_state=4313)
2023-03-05 17:50:29,604:INFO:create_model() successfully completed......................................
2023-03-05 17:50:29,683:INFO:SubProcess create_model() end ==================================
2023-03-05 17:50:29,683:INFO:Creating metrics dataframe
2023-03-05 17:50:29,692:INFO:Initializing Ridge Regression
2023-03-05 17:50:29,693:INFO:Total runtime is 0.1497253139813741 minutes
2023-03-05 17:50:29,696:INFO:SubProcess create_model() called ==================================
2023-03-05 17:50:29,697:INFO:Initializing create_model()
2023-03-05 17:50:29,697:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A56008550>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:50:29,697:INFO:Checking exceptions
2023-03-05 17:50:29,697:INFO:Importing libraries
2023-03-05 17:50:29,697:INFO:Copying training dataset
2023-03-05 17:50:29,702:INFO:Defining folds
2023-03-05 17:50:29,703:INFO:Declaring metric variables
2023-03-05 17:50:29,707:INFO:Importing untrained model
2023-03-05 17:50:29,711:INFO:Ridge Regression Imported successfully
2023-03-05 17:50:29,718:INFO:Starting cross validation
2023-03-05 17:50:29,719:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:50:30,452:INFO:Calculating mean and std
2023-03-05 17:50:30,453:INFO:Creating metrics dataframe
2023-03-05 17:50:30,456:INFO:Uploading results into container
2023-03-05 17:50:30,456:INFO:Uploading model into container now
2023-03-05 17:50:30,457:INFO:_master_model_container: 22
2023-03-05 17:50:30,457:INFO:_display_container: 3
2023-03-05 17:50:30,457:INFO:Ridge(random_state=4313)
2023-03-05 17:50:30,457:INFO:create_model() successfully completed......................................
2023-03-05 17:50:30,533:INFO:SubProcess create_model() end ==================================
2023-03-05 17:50:30,533:INFO:Creating metrics dataframe
2023-03-05 17:50:30,543:INFO:Initializing Elastic Net
2023-03-05 17:50:30,544:INFO:Total runtime is 0.16391239563624063 minutes
2023-03-05 17:50:30,548:INFO:SubProcess create_model() called ==================================
2023-03-05 17:50:30,548:INFO:Initializing create_model()
2023-03-05 17:50:30,548:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A56008550>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:50:30,549:INFO:Checking exceptions
2023-03-05 17:50:30,549:INFO:Importing libraries
2023-03-05 17:50:30,549:INFO:Copying training dataset
2023-03-05 17:50:30,557:INFO:Defining folds
2023-03-05 17:50:30,557:INFO:Declaring metric variables
2023-03-05 17:50:30,561:INFO:Importing untrained model
2023-03-05 17:50:30,565:INFO:Elastic Net Imported successfully
2023-03-05 17:50:30,578:INFO:Starting cross validation
2023-03-05 17:50:30,580:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:50:31,312:INFO:Calculating mean and std
2023-03-05 17:50:31,313:INFO:Creating metrics dataframe
2023-03-05 17:50:31,316:INFO:Uploading results into container
2023-03-05 17:50:31,316:INFO:Uploading model into container now
2023-03-05 17:50:31,316:INFO:_master_model_container: 23
2023-03-05 17:50:31,316:INFO:_display_container: 3
2023-03-05 17:50:31,317:INFO:ElasticNet(random_state=4313)
2023-03-05 17:50:31,317:INFO:create_model() successfully completed......................................
2023-03-05 17:50:31,390:INFO:SubProcess create_model() end ==================================
2023-03-05 17:50:31,390:INFO:Creating metrics dataframe
2023-03-05 17:50:31,401:INFO:Initializing Least Angle Regression
2023-03-05 17:50:31,402:INFO:Total runtime is 0.17821675539016724 minutes
2023-03-05 17:50:31,405:INFO:SubProcess create_model() called ==================================
2023-03-05 17:50:31,406:INFO:Initializing create_model()
2023-03-05 17:50:31,406:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A56008550>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:50:31,407:INFO:Checking exceptions
2023-03-05 17:50:31,407:INFO:Importing libraries
2023-03-05 17:50:31,407:INFO:Copying training dataset
2023-03-05 17:50:31,413:INFO:Defining folds
2023-03-05 17:50:31,413:INFO:Declaring metric variables
2023-03-05 17:50:31,417:INFO:Importing untrained model
2023-03-05 17:50:31,421:INFO:Least Angle Regression Imported successfully
2023-03-05 17:50:31,432:INFO:Starting cross validation
2023-03-05 17:50:31,433:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:50:31,669:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:50:31,699:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:50:31,711:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:50:31,739:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:50:31,748:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:50:31,761:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:50:31,772:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:50:31,826:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:50:31,885:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:50:31,909:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:50:31,973:INFO:Calculating mean and std
2023-03-05 17:50:31,973:INFO:Creating metrics dataframe
2023-03-05 17:50:31,977:INFO:Uploading results into container
2023-03-05 17:50:31,978:INFO:Uploading model into container now
2023-03-05 17:50:31,978:INFO:_master_model_container: 24
2023-03-05 17:50:31,978:INFO:_display_container: 3
2023-03-05 17:50:31,979:INFO:Lars(random_state=4313)
2023-03-05 17:50:31,979:INFO:create_model() successfully completed......................................
2023-03-05 17:50:32,060:INFO:SubProcess create_model() end ==================================
2023-03-05 17:50:32,061:INFO:Creating metrics dataframe
2023-03-05 17:50:32,069:INFO:Initializing Lasso Least Angle Regression
2023-03-05 17:50:32,069:INFO:Total runtime is 0.18933717409769693 minutes
2023-03-05 17:50:32,072:INFO:SubProcess create_model() called ==================================
2023-03-05 17:50:32,073:INFO:Initializing create_model()
2023-03-05 17:50:32,073:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A56008550>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:50:32,073:INFO:Checking exceptions
2023-03-05 17:50:32,073:INFO:Importing libraries
2023-03-05 17:50:32,073:INFO:Copying training dataset
2023-03-05 17:50:32,077:INFO:Defining folds
2023-03-05 17:50:32,077:INFO:Declaring metric variables
2023-03-05 17:50:32,081:INFO:Importing untrained model
2023-03-05 17:50:32,085:INFO:Lasso Least Angle Regression Imported successfully
2023-03-05 17:50:32,097:INFO:Starting cross validation
2023-03-05 17:50:32,100:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:50:32,332:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:50:32,361:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:50:32,365:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:50:32,412:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:50:32,416:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:50:32,421:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:50:32,441:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:50:32,466:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:50:32,546:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:50:32,567:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-05 17:50:32,631:INFO:Calculating mean and std
2023-03-05 17:50:32,631:INFO:Creating metrics dataframe
2023-03-05 17:50:32,635:INFO:Uploading results into container
2023-03-05 17:50:32,636:INFO:Uploading model into container now
2023-03-05 17:50:32,636:INFO:_master_model_container: 25
2023-03-05 17:50:32,636:INFO:_display_container: 3
2023-03-05 17:50:32,636:INFO:LassoLars(random_state=4313)
2023-03-05 17:50:32,637:INFO:create_model() successfully completed......................................
2023-03-05 17:50:32,717:INFO:SubProcess create_model() end ==================================
2023-03-05 17:50:32,718:INFO:Creating metrics dataframe
2023-03-05 17:50:32,726:INFO:Initializing Orthogonal Matching Pursuit
2023-03-05 17:50:32,726:INFO:Total runtime is 0.2002782622973124 minutes
2023-03-05 17:50:32,729:INFO:SubProcess create_model() called ==================================
2023-03-05 17:50:32,729:INFO:Initializing create_model()
2023-03-05 17:50:32,729:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A56008550>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:50:32,729:INFO:Checking exceptions
2023-03-05 17:50:32,730:INFO:Importing libraries
2023-03-05 17:50:32,730:INFO:Copying training dataset
2023-03-05 17:50:32,734:INFO:Defining folds
2023-03-05 17:50:32,734:INFO:Declaring metric variables
2023-03-05 17:50:32,738:INFO:Importing untrained model
2023-03-05 17:50:32,742:INFO:Orthogonal Matching Pursuit Imported successfully
2023-03-05 17:50:32,754:INFO:Starting cross validation
2023-03-05 17:50:32,756:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:50:32,988:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:50:33,009:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:50:33,018:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:50:33,052:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:50:33,054:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:50:33,072:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:50:33,080:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:50:33,105:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:50:33,188:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:50:33,214:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-05 17:50:33,273:INFO:Calculating mean and std
2023-03-05 17:50:33,273:INFO:Creating metrics dataframe
2023-03-05 17:50:33,277:INFO:Uploading results into container
2023-03-05 17:50:33,277:INFO:Uploading model into container now
2023-03-05 17:50:33,277:INFO:_master_model_container: 26
2023-03-05 17:50:33,277:INFO:_display_container: 3
2023-03-05 17:50:33,278:INFO:OrthogonalMatchingPursuit()
2023-03-05 17:50:33,278:INFO:create_model() successfully completed......................................
2023-03-05 17:50:33,352:INFO:SubProcess create_model() end ==================================
2023-03-05 17:50:33,353:INFO:Creating metrics dataframe
2023-03-05 17:50:33,363:INFO:Initializing Bayesian Ridge
2023-03-05 17:50:33,363:INFO:Total runtime is 0.21089946031570433 minutes
2023-03-05 17:50:33,368:INFO:SubProcess create_model() called ==================================
2023-03-05 17:50:33,368:INFO:Initializing create_model()
2023-03-05 17:50:33,369:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A56008550>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:50:33,369:INFO:Checking exceptions
2023-03-05 17:50:33,369:INFO:Importing libraries
2023-03-05 17:50:33,369:INFO:Copying training dataset
2023-03-05 17:50:33,373:INFO:Defining folds
2023-03-05 17:50:33,373:INFO:Declaring metric variables
2023-03-05 17:50:33,378:INFO:Importing untrained model
2023-03-05 17:50:33,384:INFO:Bayesian Ridge Imported successfully
2023-03-05 17:50:33,398:INFO:Starting cross validation
2023-03-05 17:50:33,401:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:50:33,920:INFO:Calculating mean and std
2023-03-05 17:50:33,921:INFO:Creating metrics dataframe
2023-03-05 17:50:33,924:INFO:Uploading results into container
2023-03-05 17:50:33,924:INFO:Uploading model into container now
2023-03-05 17:50:33,925:INFO:_master_model_container: 27
2023-03-05 17:50:33,925:INFO:_display_container: 3
2023-03-05 17:50:33,925:INFO:BayesianRidge()
2023-03-05 17:50:33,925:INFO:create_model() successfully completed......................................
2023-03-05 17:50:34,000:INFO:SubProcess create_model() end ==================================
2023-03-05 17:50:34,001:INFO:Creating metrics dataframe
2023-03-05 17:50:34,013:INFO:Initializing Passive Aggressive Regressor
2023-03-05 17:50:34,013:INFO:Total runtime is 0.22173320055007933 minutes
2023-03-05 17:50:34,017:INFO:SubProcess create_model() called ==================================
2023-03-05 17:50:34,017:INFO:Initializing create_model()
2023-03-05 17:50:34,017:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A56008550>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:50:34,018:INFO:Checking exceptions
2023-03-05 17:50:34,018:INFO:Importing libraries
2023-03-05 17:50:34,018:INFO:Copying training dataset
2023-03-05 17:50:34,022:INFO:Defining folds
2023-03-05 17:50:34,022:INFO:Declaring metric variables
2023-03-05 17:50:34,027:INFO:Importing untrained model
2023-03-05 17:50:34,030:INFO:Passive Aggressive Regressor Imported successfully
2023-03-05 17:50:34,040:INFO:Starting cross validation
2023-03-05 17:50:34,043:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:50:34,573:INFO:Calculating mean and std
2023-03-05 17:50:34,573:INFO:Creating metrics dataframe
2023-03-05 17:50:34,577:INFO:Uploading results into container
2023-03-05 17:50:34,577:INFO:Uploading model into container now
2023-03-05 17:50:34,578:INFO:_master_model_container: 28
2023-03-05 17:50:34,578:INFO:_display_container: 3
2023-03-05 17:50:34,578:INFO:PassiveAggressiveRegressor(random_state=4313)
2023-03-05 17:50:34,579:INFO:create_model() successfully completed......................................
2023-03-05 17:50:34,655:INFO:SubProcess create_model() end ==================================
2023-03-05 17:50:34,655:INFO:Creating metrics dataframe
2023-03-05 17:50:34,664:INFO:Initializing Huber Regressor
2023-03-05 17:50:34,664:INFO:Total runtime is 0.23258672555287677 minutes
2023-03-05 17:50:34,668:INFO:SubProcess create_model() called ==================================
2023-03-05 17:50:34,668:INFO:Initializing create_model()
2023-03-05 17:50:34,668:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A56008550>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:50:34,669:INFO:Checking exceptions
2023-03-05 17:50:34,669:INFO:Importing libraries
2023-03-05 17:50:34,669:INFO:Copying training dataset
2023-03-05 17:50:34,674:INFO:Defining folds
2023-03-05 17:50:34,674:INFO:Declaring metric variables
2023-03-05 17:50:34,678:INFO:Importing untrained model
2023-03-05 17:50:34,682:INFO:Huber Regressor Imported successfully
2023-03-05 17:50:34,691:INFO:Starting cross validation
2023-03-05 17:50:34,693:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:50:35,260:INFO:Calculating mean and std
2023-03-05 17:50:35,261:INFO:Creating metrics dataframe
2023-03-05 17:50:35,264:INFO:Uploading results into container
2023-03-05 17:50:35,265:INFO:Uploading model into container now
2023-03-05 17:50:35,265:INFO:_master_model_container: 29
2023-03-05 17:50:35,266:INFO:_display_container: 3
2023-03-05 17:50:35,266:INFO:HuberRegressor()
2023-03-05 17:50:35,266:INFO:create_model() successfully completed......................................
2023-03-05 17:50:35,348:INFO:SubProcess create_model() end ==================================
2023-03-05 17:50:35,348:INFO:Creating metrics dataframe
2023-03-05 17:50:35,357:INFO:Initializing K Neighbors Regressor
2023-03-05 17:50:35,358:INFO:Total runtime is 0.24414121707280476 minutes
2023-03-05 17:50:35,362:INFO:SubProcess create_model() called ==================================
2023-03-05 17:50:35,362:INFO:Initializing create_model()
2023-03-05 17:50:35,362:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A56008550>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:50:35,362:INFO:Checking exceptions
2023-03-05 17:50:35,362:INFO:Importing libraries
2023-03-05 17:50:35,362:INFO:Copying training dataset
2023-03-05 17:50:35,368:INFO:Defining folds
2023-03-05 17:50:35,368:INFO:Declaring metric variables
2023-03-05 17:50:35,372:INFO:Importing untrained model
2023-03-05 17:50:35,377:INFO:K Neighbors Regressor Imported successfully
2023-03-05 17:50:35,387:INFO:Starting cross validation
2023-03-05 17:50:35,389:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:50:35,947:INFO:Calculating mean and std
2023-03-05 17:50:35,948:INFO:Creating metrics dataframe
2023-03-05 17:50:35,951:INFO:Uploading results into container
2023-03-05 17:50:35,951:INFO:Uploading model into container now
2023-03-05 17:50:35,952:INFO:_master_model_container: 30
2023-03-05 17:50:35,952:INFO:_display_container: 3
2023-03-05 17:50:35,952:INFO:KNeighborsRegressor(n_jobs=-1)
2023-03-05 17:50:35,952:INFO:create_model() successfully completed......................................
2023-03-05 17:50:36,036:INFO:SubProcess create_model() end ==================================
2023-03-05 17:50:36,036:INFO:Creating metrics dataframe
2023-03-05 17:50:36,046:INFO:Initializing Decision Tree Regressor
2023-03-05 17:50:36,047:INFO:Total runtime is 0.2556315819422404 minutes
2023-03-05 17:50:36,050:INFO:SubProcess create_model() called ==================================
2023-03-05 17:50:36,051:INFO:Initializing create_model()
2023-03-05 17:50:36,051:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A56008550>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:50:36,051:INFO:Checking exceptions
2023-03-05 17:50:36,051:INFO:Importing libraries
2023-03-05 17:50:36,051:INFO:Copying training dataset
2023-03-05 17:50:36,056:INFO:Defining folds
2023-03-05 17:50:36,057:INFO:Declaring metric variables
2023-03-05 17:50:36,061:INFO:Importing untrained model
2023-03-05 17:50:36,067:INFO:Decision Tree Regressor Imported successfully
2023-03-05 17:50:36,074:INFO:Starting cross validation
2023-03-05 17:50:36,075:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:50:36,606:INFO:Calculating mean and std
2023-03-05 17:50:36,607:INFO:Creating metrics dataframe
2023-03-05 17:50:36,612:INFO:Uploading results into container
2023-03-05 17:50:36,613:INFO:Uploading model into container now
2023-03-05 17:50:36,613:INFO:_master_model_container: 31
2023-03-05 17:50:36,613:INFO:_display_container: 3
2023-03-05 17:50:36,614:INFO:DecisionTreeRegressor(random_state=4313)
2023-03-05 17:50:36,614:INFO:create_model() successfully completed......................................
2023-03-05 17:50:36,693:INFO:SubProcess create_model() end ==================================
2023-03-05 17:50:36,693:INFO:Creating metrics dataframe
2023-03-05 17:50:36,703:INFO:Initializing Random Forest Regressor
2023-03-05 17:50:36,703:INFO:Total runtime is 0.2665611187616984 minutes
2023-03-05 17:50:36,706:INFO:SubProcess create_model() called ==================================
2023-03-05 17:50:36,706:INFO:Initializing create_model()
2023-03-05 17:50:36,706:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A56008550>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:50:36,706:INFO:Checking exceptions
2023-03-05 17:50:36,706:INFO:Importing libraries
2023-03-05 17:50:36,706:INFO:Copying training dataset
2023-03-05 17:50:36,712:INFO:Defining folds
2023-03-05 17:50:36,712:INFO:Declaring metric variables
2023-03-05 17:50:36,716:INFO:Importing untrained model
2023-03-05 17:50:36,720:INFO:Random Forest Regressor Imported successfully
2023-03-05 17:50:36,728:INFO:Starting cross validation
2023-03-05 17:50:36,730:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:50:37,545:INFO:Calculating mean and std
2023-03-05 17:50:37,547:INFO:Creating metrics dataframe
2023-03-05 17:50:37,550:INFO:Uploading results into container
2023-03-05 17:50:37,550:INFO:Uploading model into container now
2023-03-05 17:50:37,550:INFO:_master_model_container: 32
2023-03-05 17:50:37,551:INFO:_display_container: 3
2023-03-05 17:50:37,551:INFO:RandomForestRegressor(n_jobs=-1, random_state=4313)
2023-03-05 17:50:37,551:INFO:create_model() successfully completed......................................
2023-03-05 17:50:37,631:INFO:SubProcess create_model() end ==================================
2023-03-05 17:50:37,631:INFO:Creating metrics dataframe
2023-03-05 17:50:37,641:INFO:Initializing Extra Trees Regressor
2023-03-05 17:50:37,641:INFO:Total runtime is 0.2821893930435181 minutes
2023-03-05 17:50:37,644:INFO:SubProcess create_model() called ==================================
2023-03-05 17:50:37,644:INFO:Initializing create_model()
2023-03-05 17:50:37,644:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A56008550>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:50:37,645:INFO:Checking exceptions
2023-03-05 17:50:37,645:INFO:Importing libraries
2023-03-05 17:50:37,645:INFO:Copying training dataset
2023-03-05 17:50:37,651:INFO:Defining folds
2023-03-05 17:50:37,652:INFO:Declaring metric variables
2023-03-05 17:50:37,656:INFO:Importing untrained model
2023-03-05 17:50:37,659:INFO:Extra Trees Regressor Imported successfully
2023-03-05 17:50:37,668:INFO:Starting cross validation
2023-03-05 17:50:37,672:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:50:38,467:INFO:Calculating mean and std
2023-03-05 17:50:38,469:INFO:Creating metrics dataframe
2023-03-05 17:50:38,471:INFO:Uploading results into container
2023-03-05 17:50:38,472:INFO:Uploading model into container now
2023-03-05 17:50:38,472:INFO:_master_model_container: 33
2023-03-05 17:50:38,472:INFO:_display_container: 3
2023-03-05 17:50:38,473:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=4313)
2023-03-05 17:50:38,473:INFO:create_model() successfully completed......................................
2023-03-05 17:50:38,553:INFO:SubProcess create_model() end ==================================
2023-03-05 17:50:38,554:INFO:Creating metrics dataframe
2023-03-05 17:50:38,564:INFO:Initializing AdaBoost Regressor
2023-03-05 17:50:38,564:INFO:Total runtime is 0.29758632580439254 minutes
2023-03-05 17:50:38,567:INFO:SubProcess create_model() called ==================================
2023-03-05 17:50:38,567:INFO:Initializing create_model()
2023-03-05 17:50:38,567:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A56008550>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:50:38,567:INFO:Checking exceptions
2023-03-05 17:50:38,567:INFO:Importing libraries
2023-03-05 17:50:38,568:INFO:Copying training dataset
2023-03-05 17:50:38,572:INFO:Defining folds
2023-03-05 17:50:38,573:INFO:Declaring metric variables
2023-03-05 17:50:38,577:INFO:Importing untrained model
2023-03-05 17:50:38,581:INFO:AdaBoost Regressor Imported successfully
2023-03-05 17:50:38,590:INFO:Starting cross validation
2023-03-05 17:50:38,592:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:50:39,219:INFO:Calculating mean and std
2023-03-05 17:50:39,220:INFO:Creating metrics dataframe
2023-03-05 17:50:39,224:INFO:Uploading results into container
2023-03-05 17:50:39,224:INFO:Uploading model into container now
2023-03-05 17:50:39,225:INFO:_master_model_container: 34
2023-03-05 17:50:39,225:INFO:_display_container: 3
2023-03-05 17:50:39,226:INFO:AdaBoostRegressor(random_state=4313)
2023-03-05 17:50:39,226:INFO:create_model() successfully completed......................................
2023-03-05 17:50:39,305:INFO:SubProcess create_model() end ==================================
2023-03-05 17:50:39,305:INFO:Creating metrics dataframe
2023-03-05 17:50:39,315:INFO:Initializing Gradient Boosting Regressor
2023-03-05 17:50:39,315:INFO:Total runtime is 0.31010370651880903 minutes
2023-03-05 17:50:39,318:INFO:SubProcess create_model() called ==================================
2023-03-05 17:50:39,319:INFO:Initializing create_model()
2023-03-05 17:50:39,319:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A56008550>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:50:39,319:INFO:Checking exceptions
2023-03-05 17:50:39,319:INFO:Importing libraries
2023-03-05 17:50:39,319:INFO:Copying training dataset
2023-03-05 17:50:39,325:INFO:Defining folds
2023-03-05 17:50:39,325:INFO:Declaring metric variables
2023-03-05 17:50:39,329:INFO:Importing untrained model
2023-03-05 17:50:39,333:INFO:Gradient Boosting Regressor Imported successfully
2023-03-05 17:50:39,341:INFO:Starting cross validation
2023-03-05 17:50:39,343:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:50:40,489:INFO:Calculating mean and std
2023-03-05 17:50:40,491:INFO:Creating metrics dataframe
2023-03-05 17:50:40,495:INFO:Uploading results into container
2023-03-05 17:50:40,496:INFO:Uploading model into container now
2023-03-05 17:50:40,496:INFO:_master_model_container: 35
2023-03-05 17:50:40,496:INFO:_display_container: 3
2023-03-05 17:50:40,497:INFO:GradientBoostingRegressor(random_state=4313)
2023-03-05 17:50:40,497:INFO:create_model() successfully completed......................................
2023-03-05 17:50:40,581:INFO:SubProcess create_model() end ==================================
2023-03-05 17:50:40,581:INFO:Creating metrics dataframe
2023-03-05 17:50:40,595:INFO:Initializing Extreme Gradient Boosting
2023-03-05 17:50:40,595:INFO:Total runtime is 0.3314293424288432 minutes
2023-03-05 17:50:40,601:INFO:SubProcess create_model() called ==================================
2023-03-05 17:50:40,601:INFO:Initializing create_model()
2023-03-05 17:50:40,601:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A56008550>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:50:40,601:INFO:Checking exceptions
2023-03-05 17:50:40,602:INFO:Importing libraries
2023-03-05 17:50:40,602:INFO:Copying training dataset
2023-03-05 17:50:40,610:INFO:Defining folds
2023-03-05 17:50:40,611:INFO:Declaring metric variables
2023-03-05 17:50:40,615:INFO:Importing untrained model
2023-03-05 17:50:40,621:INFO:Extreme Gradient Boosting Imported successfully
2023-03-05 17:50:40,630:INFO:Starting cross validation
2023-03-05 17:50:40,632:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:50:41,808:INFO:Calculating mean and std
2023-03-05 17:50:41,809:INFO:Creating metrics dataframe
2023-03-05 17:50:41,812:INFO:Uploading results into container
2023-03-05 17:50:41,813:INFO:Uploading model into container now
2023-03-05 17:50:41,813:INFO:_master_model_container: 36
2023-03-05 17:50:41,813:INFO:_display_container: 3
2023-03-05 17:50:41,814:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=4313, ...)
2023-03-05 17:50:41,814:INFO:create_model() successfully completed......................................
2023-03-05 17:50:41,891:INFO:SubProcess create_model() end ==================================
2023-03-05 17:50:41,891:INFO:Creating metrics dataframe
2023-03-05 17:50:41,906:INFO:Initializing Light Gradient Boosting Machine
2023-03-05 17:50:41,906:INFO:Total runtime is 0.35328163305918375 minutes
2023-03-05 17:50:41,909:INFO:SubProcess create_model() called ==================================
2023-03-05 17:50:41,910:INFO:Initializing create_model()
2023-03-05 17:50:41,910:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A56008550>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:50:41,910:INFO:Checking exceptions
2023-03-05 17:50:41,910:INFO:Importing libraries
2023-03-05 17:50:41,910:INFO:Copying training dataset
2023-03-05 17:50:41,915:INFO:Defining folds
2023-03-05 17:50:41,915:INFO:Declaring metric variables
2023-03-05 17:50:41,922:INFO:Importing untrained model
2023-03-05 17:50:41,926:INFO:Light Gradient Boosting Machine Imported successfully
2023-03-05 17:50:41,933:INFO:Starting cross validation
2023-03-05 17:50:41,935:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:50:44,623:INFO:Calculating mean and std
2023-03-05 17:50:44,625:INFO:Creating metrics dataframe
2023-03-05 17:50:44,628:INFO:Uploading results into container
2023-03-05 17:50:44,629:INFO:Uploading model into container now
2023-03-05 17:50:44,629:INFO:_master_model_container: 37
2023-03-05 17:50:44,629:INFO:_display_container: 3
2023-03-05 17:50:44,630:INFO:LGBMRegressor(random_state=4313)
2023-03-05 17:50:44,630:INFO:create_model() successfully completed......................................
2023-03-05 17:50:44,715:INFO:SubProcess create_model() end ==================================
2023-03-05 17:50:44,715:INFO:Creating metrics dataframe
2023-03-05 17:50:44,730:INFO:Initializing Dummy Regressor
2023-03-05 17:50:44,730:INFO:Total runtime is 0.4003441572189331 minutes
2023-03-05 17:50:44,735:INFO:SubProcess create_model() called ==================================
2023-03-05 17:50:44,736:INFO:Initializing create_model()
2023-03-05 17:50:44,736:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023A56008550>, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:50:44,736:INFO:Checking exceptions
2023-03-05 17:50:44,736:INFO:Importing libraries
2023-03-05 17:50:44,736:INFO:Copying training dataset
2023-03-05 17:50:44,744:INFO:Defining folds
2023-03-05 17:50:44,744:INFO:Declaring metric variables
2023-03-05 17:50:44,748:INFO:Importing untrained model
2023-03-05 17:50:44,754:INFO:Dummy Regressor Imported successfully
2023-03-05 17:50:44,764:INFO:Starting cross validation
2023-03-05 17:50:44,766:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-05 17:50:45,450:INFO:Calculating mean and std
2023-03-05 17:50:45,452:INFO:Creating metrics dataframe
2023-03-05 17:50:45,455:INFO:Uploading results into container
2023-03-05 17:50:45,456:INFO:Uploading model into container now
2023-03-05 17:50:45,456:INFO:_master_model_container: 38
2023-03-05 17:50:45,457:INFO:_display_container: 3
2023-03-05 17:50:45,457:INFO:DummyRegressor()
2023-03-05 17:50:45,457:INFO:create_model() successfully completed......................................
2023-03-05 17:50:45,548:INFO:SubProcess create_model() end ==================================
2023-03-05 17:50:45,548:INFO:Creating metrics dataframe
2023-03-05 17:50:45,573:INFO:Initializing create_model()
2023-03-05 17:50:45,573:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000023A55CF5E80>, estimator=GradientBoostingRegressor(random_state=4313), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-05 17:50:45,575:INFO:Checking exceptions
2023-03-05 17:50:45,577:INFO:Importing libraries
2023-03-05 17:50:45,577:INFO:Copying training dataset
2023-03-05 17:50:45,582:INFO:Defining folds
2023-03-05 17:50:45,582:INFO:Declaring metric variables
2023-03-05 17:50:45,582:INFO:Importing untrained model
2023-03-05 17:50:45,582:INFO:Declaring custom model
2023-03-05 17:50:45,583:INFO:Gradient Boosting Regressor Imported successfully
2023-03-05 17:50:45,584:INFO:Cross validation set to False
2023-03-05 17:50:45,584:INFO:Fitting Model
2023-03-05 17:50:45,768:INFO:GradientBoostingRegressor(random_state=4313)
2023-03-05 17:50:45,768:INFO:create_model() successfully completed......................................
2023-03-05 17:50:45,882:INFO:_master_model_container: 38
2023-03-05 17:50:45,882:INFO:_display_container: 3
2023-03-05 17:50:45,883:INFO:GradientBoostingRegressor(random_state=4313)
2023-03-05 17:50:45,883:INFO:compare_models() successfully completed......................................
2023-03-05 18:22:29,036:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_aa39456e9a8249218bcda100eb46a9c1_e2ace5160ae14ad7b9127a230c1c7f95
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,036:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_aeab7e02a0d24b74a8412b3c038d0662_4b2f8e90e01d4831966a5146d1b7ae58
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,037:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_aa39456e9a8249218bcda100eb46a9c1_880fe8e1c9af491e97c65f32e11e1537
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,037:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_024e19bd75474347b4786ded60a8844d_b139a86b757b4dacabc2407b2ac1734e
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,038:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_aa39456e9a8249218bcda100eb46a9c1_8db2c83132ac4c0fa6ab64ea59088dd1
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,038:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_73da6b88588a41cb9e37667a99da8ba8_043e3d9b084f4ca6ae9840d6a725250d
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,038:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_aa39456e9a8249218bcda100eb46a9c1_3fbe7cb796f1496c85f5ac4676b58e2c
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,039:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_7a916400cd324335bd786c2616587622_12fa512040554285bbb3eb098e419760
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,039:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_aa39456e9a8249218bcda100eb46a9c1_6f99be7699384fec9077a3a979ef2a44
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,039:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_88c0b6d165d9408a9d6f2f00ecd9d52f_330c1928bcb346cca4657eea70236ac2
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,040:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_aa39456e9a8249218bcda100eb46a9c1_8edcd8c7a7c34a479ed95bb9b4014a4a
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,040:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_39a063f5c9184528ae2f13c874d45160_8052aa6fd2d24864b4fe85c79e0be86a
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,040:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_aa39456e9a8249218bcda100eb46a9c1_3108e3b6c8f1414a8514dac3ae2ab667
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,041:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_7b26a46169d54364aa41db2abf29508f_504d465bb48d47eabeb636f8d40c2398
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,041:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_aa39456e9a8249218bcda100eb46a9c1_b1a859577fc547c589500b1a5b0ab0d0
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,042:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_f9771786b0464c55886cb0cfe146dd3c_41c593908fc940998e17faa5e95ccc23
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,042:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_aa39456e9a8249218bcda100eb46a9c1_7b9b46c8bb664cb992b205979d27a20a
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,042:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_6e17c52d3df5444cb094ef1ab1eb726b_8b4c91e6b2824b25ac574bd2ef6e8580
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,042:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_aa39456e9a8249218bcda100eb46a9c1_dc2744d4f0b9450eb1bce1443db3edaf
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,043:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_625e07f125ee4057a874a9b85de2a325_f18424d9e15b4b95839ab286e3c804c3
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,043:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_aa39456e9a8249218bcda100eb46a9c1_e6e4df8012644139bc9e900bf303f34f
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,043:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_661594d20baa45aea80e3f90c803d972_f50573e543124dd6a546597b24de271a
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,044:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_aa39456e9a8249218bcda100eb46a9c1_6addaa556b9c4ca9a586a9a737b1e720
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,044:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_0790849eb7a24cb388d8df72c738b304_00f7896ad8e948229a3f2dffb7e09f04
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,044:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_aa39456e9a8249218bcda100eb46a9c1_2638da480ebc472e8720be3588ce4c16
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,044:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_587ed89f647a447a9c4d21f8b85f3884_d8c287391118431eacf18c86d720206c
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,045:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_aa39456e9a8249218bcda100eb46a9c1_8ab9005ac8c94a10917d3d8c513d4b4d
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,045:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_7dc2d8054ebb427e8cc3e8008b7fc0e2_43b602a4de584e779b79da9a447aeb14
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,045:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_aa39456e9a8249218bcda100eb46a9c1_c591f744eb7d47c3a50749afcf85320c
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,046:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_ba01766dfae54ee996f8f2f9ff28178d_a8c836183fa34181a5e4bb7c8c8546cc
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,046:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_aa39456e9a8249218bcda100eb46a9c1_b586192cc2eb4e42825fb6ce5bfb9fc7
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,046:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_ff32fae83e4a4b85b082d5c1a43f8955_29d0e86008ea4da4b7b7006da5a3fa7d
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,047:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_aa39456e9a8249218bcda100eb46a9c1_9dc9a92a787b498aa9c16671644e8030
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,047:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_d2da57c2bbd34bcdad0a972164c59d7d_57b11fb7d8ea46b4bb68f48624de14aa
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,047:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_aa39456e9a8249218bcda100eb46a9c1_c64e72eb853249a082f9014d9c7e0d7a
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,047:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_15d2c305e40c4dee8094ab0efcca9e1e_0375a2cdd7d0494ea5a68975558f4989
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,048:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_aa39456e9a8249218bcda100eb46a9c1_f56a49b90db64e949345d389d37f93bf
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,048:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_c197466a7d6d4527862af0e1592e0ddc_8314e4bcb8db40eb9920bedc90a6711d
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,049:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_aa39456e9a8249218bcda100eb46a9c1_a10f29b4948641389c51394b267ef775
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,049:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_ede670a0f1bc421498dcf8c4297c3cd8_66f0d91f7c034eabacb1b7a8ee24e8ad
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,049:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_aa39456e9a8249218bcda100eb46a9c1_ec0e59b716dd4e8c95bf790111cba622
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,049:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_0b60aee3ba5a4751be8d5f6538de8b89_a56935e3ac504f48bda6c18d46d57e3d
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,049:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_aa39456e9a8249218bcda100eb46a9c1_922cef5dd7a24157bfed6935106186bc
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,050:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_1c57579f08fe40b79819ef7cf38cb0df_d57fa69c084d421bb22d735e30a2e603
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,050:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_aa39456e9a8249218bcda100eb46a9c1_5adef0bc38184b5c9c9b1013d2e3d946
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-05 18:22:29,050:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\Alvaro\AppData\Local\Temp\joblib_memmapping_folder_9092_aa39456e9a8249218bcda100eb46a9c1_2c8b3bc917d14479988d7fe117a979fb
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-06 12:33:59,610:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-06 12:33:59,611:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-06 12:33:59,611:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-06 12:33:59,611:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-06 12:34:03,793:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-03-06 12:34:04,409:INFO:PyCaret RegressionExperiment
2023-03-06 12:34:04,409:INFO:Logging name: reg-default-name
2023-03-06 12:34:04,409:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-06 12:34:04,409:INFO:version 3.0.0.rc9
2023-03-06 12:34:04,409:INFO:Initializing setup()
2023-03-06 12:34:04,409:INFO:self.USI: 65ab
2023-03-06 12:34:04,409:INFO:self._variable_keys: {'pipeline', 'idx', 'gpu_param', 'fold_generator', 'transform_target_param', 'y_test', 'target_param', '_available_plots', 'fold_groups_param', 'logging_param', 'X_test', 'html_param', 'n_jobs_param', 'seed', 'fold_shuffle_param', 'X_train', 'log_plots_param', 'gpu_n_jobs_param', 'exp_name_log', 'y', 'USI', 'y_train', 'memory', 'data', '_ml_usecase', 'exp_id', 'X'}
2023-03-06 12:34:04,410:INFO:Checking environment
2023-03-06 12:34:04,410:INFO:python_version: 3.9.13
2023-03-06 12:34:04,410:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-03-06 12:34:04,410:INFO:machine: AMD64
2023-03-06 12:34:04,410:INFO:platform: Windows-10-10.0.19044-SP0
2023-03-06 12:34:04,410:INFO:Memory: svmem(total=17009516544, available=6748692480, percent=60.3, used=10260824064, free=6748692480)
2023-03-06 12:34:04,410:INFO:Physical Core: 4
2023-03-06 12:34:04,410:INFO:Logical Core: 8
2023-03-06 12:34:04,410:INFO:Checking libraries
2023-03-06 12:34:04,410:INFO:System:
2023-03-06 12:34:04,410:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-03-06 12:34:04,410:INFO:executable: C:\Users\Alvaro\anaconda3\python.exe
2023-03-06 12:34:04,410:INFO:   machine: Windows-10-10.0.19044-SP0
2023-03-06 12:34:04,410:INFO:PyCaret required dependencies:
2023-03-06 12:34:04,410:INFO:                 pip: 22.2.2
2023-03-06 12:34:04,410:INFO:          setuptools: 63.4.1
2023-03-06 12:34:04,410:INFO:             pycaret: 3.0.0rc9
2023-03-06 12:34:04,410:INFO:             IPython: 7.31.1
2023-03-06 12:34:04,410:INFO:          ipywidgets: 7.6.5
2023-03-06 12:34:04,410:INFO:                tqdm: 4.64.1
2023-03-06 12:34:04,410:INFO:               numpy: 1.21.5
2023-03-06 12:34:04,410:INFO:              pandas: 1.4.4
2023-03-06 12:34:04,411:INFO:              jinja2: 2.11.3
2023-03-06 12:34:04,411:INFO:               scipy: 1.9.1
2023-03-06 12:34:04,411:INFO:              joblib: 1.2.0
2023-03-06 12:34:04,411:INFO:             sklearn: 1.0.2
2023-03-06 12:34:04,411:INFO:                pyod: 1.0.7
2023-03-06 12:34:04,411:INFO:            imblearn: 0.10.1
2023-03-06 12:34:04,411:INFO:   category_encoders: 2.6.0
2023-03-06 12:34:04,411:INFO:            lightgbm: 3.3.5
2023-03-06 12:34:04,411:INFO:               numba: 0.55.1
2023-03-06 12:34:04,411:INFO:            requests: 2.28.1
2023-03-06 12:34:04,411:INFO:          matplotlib: 3.5.2
2023-03-06 12:34:04,411:INFO:          scikitplot: 0.3.7
2023-03-06 12:34:04,411:INFO:         yellowbrick: 1.5
2023-03-06 12:34:04,411:INFO:              plotly: 5.9.0
2023-03-06 12:34:04,411:INFO:             kaleido: 0.2.1
2023-03-06 12:34:04,411:INFO:         statsmodels: 0.13.2
2023-03-06 12:34:04,411:INFO:              sktime: 0.16.1
2023-03-06 12:34:04,411:INFO:               tbats: 1.1.2
2023-03-06 12:34:04,411:INFO:            pmdarima: 2.0.2
2023-03-06 12:34:04,411:INFO:              psutil: 5.9.0
2023-03-06 12:34:04,411:INFO:PyCaret optional dependencies:
2023-03-06 12:34:04,429:INFO:                shap: Not installed
2023-03-06 12:34:04,429:INFO:           interpret: Not installed
2023-03-06 12:34:04,429:INFO:                umap: Not installed
2023-03-06 12:34:04,429:INFO:    pandas_profiling: Not installed
2023-03-06 12:34:04,429:INFO:  explainerdashboard: Not installed
2023-03-06 12:34:04,430:INFO:             autoviz: Not installed
2023-03-06 12:34:04,430:INFO:           fairlearn: Not installed
2023-03-06 12:34:04,430:INFO:             xgboost: 1.7.4
2023-03-06 12:34:04,430:INFO:            catboost: Not installed
2023-03-06 12:34:04,430:INFO:              kmodes: Not installed
2023-03-06 12:34:04,430:INFO:             mlxtend: Not installed
2023-03-06 12:34:04,430:INFO:       statsforecast: Not installed
2023-03-06 12:34:04,430:INFO:        tune_sklearn: Not installed
2023-03-06 12:34:04,430:INFO:                 ray: Not installed
2023-03-06 12:34:04,430:INFO:            hyperopt: Not installed
2023-03-06 12:34:04,430:INFO:              optuna: Not installed
2023-03-06 12:34:04,430:INFO:               skopt: Not installed
2023-03-06 12:34:04,430:INFO:              mlflow: Not installed
2023-03-06 12:34:04,430:INFO:              gradio: Not installed
2023-03-06 12:34:04,430:INFO:             fastapi: Not installed
2023-03-06 12:34:04,430:INFO:             uvicorn: Not installed
2023-03-06 12:34:04,430:INFO:              m2cgen: Not installed
2023-03-06 12:34:04,430:INFO:           evidently: Not installed
2023-03-06 12:34:04,430:INFO:               fugue: Not installed
2023-03-06 12:34:04,430:INFO:           streamlit: Not installed
2023-03-06 12:34:04,430:INFO:             prophet: Not installed
2023-03-06 12:34:04,430:INFO:None
2023-03-06 12:34:04,430:INFO:Set up data.
2023-03-06 12:34:04,460:INFO:Set up train/test split.
2023-03-06 12:34:04,467:INFO:Set up index.
2023-03-06 12:34:04,467:INFO:Set up folding strategy.
2023-03-06 12:34:04,467:INFO:Assigning column types.
2023-03-06 12:34:04,470:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-06 12:34:04,470:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-06 12:34:04,474:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-06 12:34:04,478:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-06 12:34:04,534:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 12:34:04,573:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 12:34:04,574:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 12:34:04,790:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 12:34:04,790:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-06 12:34:04,794:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-06 12:34:04,798:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-06 12:34:04,850:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 12:34:04,890:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 12:34:04,890:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 12:34:04,893:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 12:34:04,893:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-03-06 12:34:04,897:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-06 12:34:04,901:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-06 12:34:04,953:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 12:34:04,993:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 12:34:04,993:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 12:34:04,996:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 12:34:05,000:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-06 12:34:05,005:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-06 12:34:05,057:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 12:34:05,097:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 12:34:05,098:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 12:34:05,100:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 12:34:05,100:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-03-06 12:34:05,109:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-06 12:34:05,166:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 12:34:05,206:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 12:34:05,207:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 12:34:05,209:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 12:34:05,218:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-06 12:34:05,270:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 12:34:05,309:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 12:34:05,310:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 12:34:05,312:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 12:34:05,312:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-03-06 12:34:05,371:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 12:34:05,410:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 12:34:05,410:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 12:34:05,413:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 12:34:05,470:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 12:34:05,509:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 12:34:05,510:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 12:34:05,512:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 12:34:05,512:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-06 12:34:05,571:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 12:34:05,611:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 12:34:05,613:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 12:34:05,673:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 12:34:05,715:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 12:34:05,718:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 12:34:05,718:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-03-06 12:34:05,815:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 12:34:05,817:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 12:34:05,915:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 12:34:05,917:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 12:34:05,922:INFO:Preparing preprocessing pipeline...
2023-03-06 12:34:05,923:INFO:Set up column name cleaning.
2023-03-06 12:34:05,923:INFO:Set up simple imputation.
2023-03-06 12:34:05,957:INFO:Finished creating preprocessing pipeline.
2023-03-06 12:34:05,966:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Alvaro\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('numerical_imputer',
                 TransformerWrapper(include=['work_year', 'remote_ratio',
                                             'experience_level_EX',
                                             'experience_level_MI',
                                             'experience_level_SE',
                                             'employment_type_FL',
                                             'employment_type_FT',
                                             'emplo...
                                             'job_title_Data Architect',
                                             'job_title_Data Engineer',
                                             'job_title_Data Engineering '
                                             'Manager',
                                             'job_title_Data Science '
                                             'Consultant',
                                             'job_title_Data Science Engineer',
                                             'job_title_Data Science Manager',
                                             'job_title_Data Scientist', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent')))])
2023-03-06 12:34:05,967:INFO:Creating final display dataframe.
2023-03-06 12:34:06,143:INFO:Setup _display_container:                     Description             Value
0                    Session id              1822
1                        Target     salary_in_usd
2                   Target type        Regression
3           Original data shape        (500, 148)
4        Transformed data shape        (500, 148)
5   Transformed train set shape        (350, 148)
6    Transformed test set shape        (150, 148)
7              Numeric features               147
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator             KFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  reg-default-name
18                          USI              65ab
2023-03-06 12:34:06,264:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 12:34:06,267:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 12:34:06,366:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 12:34:06,368:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 12:34:06,369:INFO:setup() successfully completed in 1.97s...............
2023-03-06 12:34:40,013:INFO:Initializing compare_models()
2023-03-06 12:34:40,013:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, include=None, fold=None, round=4, cross_validation=True, sort=RMSE, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'RMSE', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-03-06 12:34:40,014:INFO:Checking exceptions
2023-03-06 12:34:40,016:INFO:Preparing display monitor
2023-03-06 12:34:40,063:INFO:Initializing Linear Regression
2023-03-06 12:34:40,063:INFO:Total runtime is 0.0 minutes
2023-03-06 12:34:40,067:INFO:SubProcess create_model() called ==================================
2023-03-06 12:34:40,068:INFO:Initializing create_model()
2023-03-06 12:34:40,068:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636DD6040>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:34:40,068:INFO:Checking exceptions
2023-03-06 12:34:40,068:INFO:Importing libraries
2023-03-06 12:34:40,068:INFO:Copying training dataset
2023-03-06 12:34:40,076:INFO:Defining folds
2023-03-06 12:34:40,076:INFO:Declaring metric variables
2023-03-06 12:34:40,079:INFO:Importing untrained model
2023-03-06 12:34:40,084:INFO:Linear Regression Imported successfully
2023-03-06 12:34:40,093:INFO:Starting cross validation
2023-03-06 12:34:40,115:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:34:47,664:INFO:Calculating mean and std
2023-03-06 12:34:47,668:INFO:Creating metrics dataframe
2023-03-06 12:34:47,675:INFO:Uploading results into container
2023-03-06 12:34:47,677:INFO:Uploading model into container now
2023-03-06 12:34:47,678:INFO:_master_model_container: 1
2023-03-06 12:34:47,678:INFO:_display_container: 2
2023-03-06 12:34:47,679:INFO:LinearRegression(n_jobs=-1)
2023-03-06 12:34:47,679:INFO:create_model() successfully completed......................................
2023-03-06 12:34:47,818:INFO:SubProcess create_model() end ==================================
2023-03-06 12:34:47,820:INFO:Creating metrics dataframe
2023-03-06 12:34:47,827:INFO:Initializing Lasso Regression
2023-03-06 12:34:47,827:INFO:Total runtime is 0.12939078807830812 minutes
2023-03-06 12:34:47,829:INFO:SubProcess create_model() called ==================================
2023-03-06 12:34:47,829:INFO:Initializing create_model()
2023-03-06 12:34:47,829:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636DD6040>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:34:47,829:INFO:Checking exceptions
2023-03-06 12:34:47,829:INFO:Importing libraries
2023-03-06 12:34:47,829:INFO:Copying training dataset
2023-03-06 12:34:47,835:INFO:Defining folds
2023-03-06 12:34:47,836:INFO:Declaring metric variables
2023-03-06 12:34:47,841:INFO:Importing untrained model
2023-03-06 12:34:47,845:INFO:Lasso Regression Imported successfully
2023-03-06 12:34:47,852:INFO:Starting cross validation
2023-03-06 12:34:47,854:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:34:47,996:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.548e+10, tolerance: 1.481e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-06 12:34:48,027:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.078e+10, tolerance: 1.482e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-06 12:34:48,034:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.784e+10, tolerance: 1.446e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-06 12:34:48,035:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.282e+10, tolerance: 1.318e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-06 12:34:48,051:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.102e+10, tolerance: 1.431e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-06 12:34:48,064:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.090e+09, tolerance: 1.498e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-06 12:34:48,076:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.573e+10, tolerance: 1.284e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-06 12:34:48,098:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.329e+10, tolerance: 1.424e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-06 12:34:48,152:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.537e+10, tolerance: 1.381e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-06 12:34:48,166:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.389e+09, tolerance: 1.448e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-06 12:34:48,191:INFO:Calculating mean and std
2023-03-06 12:34:48,192:INFO:Creating metrics dataframe
2023-03-06 12:34:48,195:INFO:Uploading results into container
2023-03-06 12:34:48,195:INFO:Uploading model into container now
2023-03-06 12:34:48,196:INFO:_master_model_container: 2
2023-03-06 12:34:48,196:INFO:_display_container: 2
2023-03-06 12:34:48,196:INFO:Lasso(random_state=1822)
2023-03-06 12:34:48,196:INFO:create_model() successfully completed......................................
2023-03-06 12:34:48,276:INFO:SubProcess create_model() end ==================================
2023-03-06 12:34:48,277:INFO:Creating metrics dataframe
2023-03-06 12:34:48,285:INFO:Initializing Ridge Regression
2023-03-06 12:34:48,285:INFO:Total runtime is 0.13703662951787313 minutes
2023-03-06 12:34:48,290:INFO:SubProcess create_model() called ==================================
2023-03-06 12:34:48,290:INFO:Initializing create_model()
2023-03-06 12:34:48,290:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636DD6040>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:34:48,290:INFO:Checking exceptions
2023-03-06 12:34:48,291:INFO:Importing libraries
2023-03-06 12:34:48,291:INFO:Copying training dataset
2023-03-06 12:34:48,298:INFO:Defining folds
2023-03-06 12:34:48,298:INFO:Declaring metric variables
2023-03-06 12:34:48,302:INFO:Importing untrained model
2023-03-06 12:34:48,309:INFO:Ridge Regression Imported successfully
2023-03-06 12:34:48,317:INFO:Starting cross validation
2023-03-06 12:34:48,318:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:34:48,580:INFO:Calculating mean and std
2023-03-06 12:34:48,581:INFO:Creating metrics dataframe
2023-03-06 12:34:48,585:INFO:Uploading results into container
2023-03-06 12:34:48,586:INFO:Uploading model into container now
2023-03-06 12:34:48,587:INFO:_master_model_container: 3
2023-03-06 12:34:48,587:INFO:_display_container: 2
2023-03-06 12:34:48,587:INFO:Ridge(random_state=1822)
2023-03-06 12:34:48,587:INFO:create_model() successfully completed......................................
2023-03-06 12:34:48,675:INFO:SubProcess create_model() end ==================================
2023-03-06 12:34:48,675:INFO:Creating metrics dataframe
2023-03-06 12:34:48,683:INFO:Initializing Elastic Net
2023-03-06 12:34:48,683:INFO:Total runtime is 0.1436699350674947 minutes
2023-03-06 12:34:48,686:INFO:SubProcess create_model() called ==================================
2023-03-06 12:34:48,686:INFO:Initializing create_model()
2023-03-06 12:34:48,686:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636DD6040>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:34:48,686:INFO:Checking exceptions
2023-03-06 12:34:48,686:INFO:Importing libraries
2023-03-06 12:34:48,687:INFO:Copying training dataset
2023-03-06 12:34:48,693:INFO:Defining folds
2023-03-06 12:34:48,693:INFO:Declaring metric variables
2023-03-06 12:34:48,698:INFO:Importing untrained model
2023-03-06 12:34:48,703:INFO:Elastic Net Imported successfully
2023-03-06 12:34:48,712:INFO:Starting cross validation
2023-03-06 12:34:48,713:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:34:48,964:INFO:Calculating mean and std
2023-03-06 12:34:48,966:INFO:Creating metrics dataframe
2023-03-06 12:34:48,969:INFO:Uploading results into container
2023-03-06 12:34:48,970:INFO:Uploading model into container now
2023-03-06 12:34:48,970:INFO:_master_model_container: 4
2023-03-06 12:34:48,970:INFO:_display_container: 2
2023-03-06 12:34:48,970:INFO:ElasticNet(random_state=1822)
2023-03-06 12:34:48,971:INFO:create_model() successfully completed......................................
2023-03-06 12:34:49,060:INFO:SubProcess create_model() end ==================================
2023-03-06 12:34:49,060:INFO:Creating metrics dataframe
2023-03-06 12:34:49,068:INFO:Initializing Least Angle Regression
2023-03-06 12:34:49,068:INFO:Total runtime is 0.15008610486984253 minutes
2023-03-06 12:34:49,072:INFO:SubProcess create_model() called ==================================
2023-03-06 12:34:49,072:INFO:Initializing create_model()
2023-03-06 12:34:49,072:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636DD6040>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:34:49,072:INFO:Checking exceptions
2023-03-06 12:34:49,072:INFO:Importing libraries
2023-03-06 12:34:49,073:INFO:Copying training dataset
2023-03-06 12:34:49,078:INFO:Defining folds
2023-03-06 12:34:49,079:INFO:Declaring metric variables
2023-03-06 12:34:49,083:INFO:Importing untrained model
2023-03-06 12:34:49,088:INFO:Least Angle Regression Imported successfully
2023-03-06 12:34:49,098:INFO:Starting cross validation
2023-03-06 12:34:49,099:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:34:49,199:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:34:49,200:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:34:49,223:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.796e+02, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,223:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.321e+02, with an active set of 33 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,224:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.675e+02, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,225:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:34:49,225:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.646e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,227:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=9.608e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,228:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=9.608e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,228:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:34:49,229:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=8.607e+01, with an active set of 46 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,230:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.171e+02, with an active set of 39 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,231:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.034e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,232:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.034e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,233:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=9.835e+01, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,233:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=9.835e+01, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,233:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=6.058e+01, with an active set of 56 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,234:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.970e+02, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,235:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=5.568e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,236:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=5.496e+01, with an active set of 59 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,236:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.646e+02, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,237:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.851e+01, with an active set of 60 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,237:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=7.725e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,237:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.851e+01, with an active set of 60 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,237:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=7.725e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,237:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.627e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,238:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=7.352e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,238:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=4.475e+01, with an active set of 61 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,239:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=4.475e+01, with an active set of 61 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,239:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=6.447e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,239:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=4.373e+01, with an active set of 62 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,240:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=6.447e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,240:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=4.339e+01, with an active set of 62 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,240:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.859e+02, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,240:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=6.306e+01, with an active set of 59 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,240:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=3.707e+01, with an active set of 63 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,241:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=6.306e+01, with an active set of 59 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,241:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=3.651e+01, with an active set of 63 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,241:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=3.603e+01, with an active set of 63 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,241:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:34:49,241:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=6.145e+01, with an active set of 60 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,242:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.686e+02, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,242:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=3.343e+01, with an active set of 65 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,242:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=5.078e+01, with an active set of 62 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,243:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.004e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,243:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=3.121e+01, with an active set of 67 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,244:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.587e+02, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,244:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=3.061e+01, with an active set of 67 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,244:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=4.467e+01, with an active set of 65 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,244:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=9.819e+01, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,244:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=4.355e+01, with an active set of 65 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,245:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=2.710e+01, with an active set of 68 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,245:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=9.819e+01, with an active set of 45 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,245:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.215e+02, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,245:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=4.151e+01, with an active set of 66 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,245:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.215e+02, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,245:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=4.151e+01, with an active set of 66 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,246:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=2.438e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,246:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.785e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,246:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=2.081e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,247:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=1.979e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,247:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=3.955e+01, with an active set of 68 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,247:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=3.853e+01, with an active set of 68 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,247:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.102e+02, with an active set of 40 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,247:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=1.979e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,248:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.102e+02, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,248:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=1.889e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,248:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=1.830e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,249:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=3.521e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,249:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=8.079e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,249:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=1.724e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,249:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=8.079e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,250:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.018e+02, with an active set of 44 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,250:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=1.724e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,250:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=7.719e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,250:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=1.643e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,250:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=3.109e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,251:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=2.968e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,251:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=9.942e+01, with an active set of 46 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,251:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=2.865e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,251:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=6.963e+01, with an active set of 57 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,251:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=1.402e+01, with an active set of 75 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,251:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:34:49,252:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=2.738e+01, with an active set of 74 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,252:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=6.630e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,252:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=9.702e+01, with an active set of 48 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,252:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=2.576e+01, with an active set of 74 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,253:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=2.221e+01, with an active set of 74 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,253:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=1.906e+01, with an active set of 74 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,253:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=1.895e+01, with an active set of 74 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,253:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.529e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,253:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=1.322e+01, with an active set of 79 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,254:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=5.804e+01, with an active set of 61 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,254:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.529e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,254:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=1.322e+01, with an active set of 79 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,254:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=1.290e+01, with an active set of 79 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,255:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=1.702e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,255:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=8.541e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,255:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=1.208e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,255:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=7.547e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,255:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=5.416e+01, with an active set of 64 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,255:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=7.547e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,256:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=1.109e+01, with an active set of 81 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,256:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=9.362e+00, with an active set of 81 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,257:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=1.587e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,257:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=1.257e+02, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,257:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=4.949e+01, with an active set of 68 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,258:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=4.820e+01, with an active set of 69 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,259:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 89 iterations, i.e. alpha=1.528e+01, with an active set of 82 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,259:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 92 iterations, i.e. alpha=7.680e+00, with an active set of 86 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,259:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 92 iterations, i.e. alpha=6.334e+00, with an active set of 86 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,259:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=4.648e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,259:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=1.487e+01, with an active set of 83 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,260:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.033e+02, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,260:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=5.829e+00, with an active set of 87 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,260:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=5.833e+01, with an active set of 63 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,260:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:34:49,260:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=4.513e+01, with an active set of 72 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,260:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=5.263e+00, with an active set of 87 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,261:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=5.037e+00, with an active set of 87 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,261:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=5.756e+01, with an active set of 64 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,261:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 92 iterations, i.e. alpha=1.311e+01, with an active set of 84 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,261:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=5.037e+00, with an active set of 87 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,261:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 92 iterations, i.e. alpha=1.311e+01, with an active set of 84 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,261:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=4.480e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,262:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.364e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,262:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=5.620e+01, with an active set of 66 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,262:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.364e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,262:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=5.604e+01, with an active set of 66 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,262:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.364e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,262:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=4.648e+00, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,263:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=1.122e+01, with an active set of 86 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,263:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.741e+02, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,263:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=5.486e+01, with an active set of 67 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,263:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=4.648e+00, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,264:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=4.148e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,264:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=4.529e+00, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,264:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=3.609e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,264:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=3.499e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,264:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.670e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,265:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=3.499e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,265:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 98 iterations, i.e. alpha=4.552e+00, with an active set of 89 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,265:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=5.242e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 8.752e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,266:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=5.017e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,266:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=7.885e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,266:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=4.992e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,266:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=4.951e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,267:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=4.929e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,267:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=3.307e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,267:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=3.506e+02, with an active set of 90 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,267:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=1.155e+02, with an active set of 90 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,268:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.707e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,268:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.598e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,269:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.572e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,269:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.556e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,269:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.512e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,269:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=4.417e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,270:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.477e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,270:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=4.245e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,270:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.472e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,270:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.392e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,270:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.149e+02, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,270:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.330e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,271:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=6.022e+01, with an active set of 64 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,271:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.329e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,271:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=5.683e+01, with an active set of 64 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,271:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.323e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,271:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.310e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,271:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.145e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,272:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=9.491e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,272:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.308e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,272:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=7.908e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,272:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.300e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,272:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.565e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,272:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.299e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,272:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=7.507e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,273:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.295e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,273:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=5.151e+01, with an active set of 66 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,273:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=7.488e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,273:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=5.141e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,273:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=7.471e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,273:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.264e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,273:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=5.141e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,273:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=7.387e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,274:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.262e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,274:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 91 iterations, i.e. alpha=1.109e+02, with an active set of 80 regressors, and the smallest cholesky pivot element being 8.752e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,274:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=7.347e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,274:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=9.647e+01, with an active set of 49 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,274:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.240e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,274:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=7.301e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,274:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=9.647e+01, with an active set of 49 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,274:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.141e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,274:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=8.621e+00, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,274:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=4.778e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-06 12:34:49,275:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=4.778e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-06 12:34:49,275:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=4.497e+01, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,275:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=2.693e+00, with an active set of 91 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,275:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=7.206e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,275:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=8.879e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,276:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=7.187e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,276:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.545e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,276:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=7.174e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,276:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=4.411e+01, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,276:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.374e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,276:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=7.147e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,276:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=4.403e+01, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,276:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 99 iterations, i.e. alpha=7.045e+03, with an active set of 81 regressors, and the smallest cholesky pivot element being 9.246e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,276:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=7.979e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,277:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=7.124e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,277:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.340e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,277:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 99 iterations, i.e. alpha=3.467e+03, with an active set of 81 regressors, and the smallest cholesky pivot element being 8.752e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,277:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=7.979e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,277:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=4.033e+01, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,277:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=7.038e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,277:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=7.979e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,277:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.340e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

023-03-06 12:34:49,277:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=6.893e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,278:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=6.776e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,278:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=4.088e+01, with an active set of 74 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,278:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.181e+02, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,278:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.323e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,278:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=6.075e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,278:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=3.580e+01, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,278:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.323e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,278:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=7.136e+01, with an active set of 56 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,279:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.306e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,279:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=3.053e+03, with an active set of 85 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,279:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.306e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,279:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=3.052e+03, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,279:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=2.963e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,280:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.306e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,280:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=3.384e+01, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,280:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.289e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,280:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=3.330e+01, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,280:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=2.864e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,280:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.289e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,280:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=2.768e+01, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,280:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=2.864e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,280:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.272e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,280:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=2.764e+01, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,281:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=2.714e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,281:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=2.698e+01, with an active set of 96 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,281:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.272e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,281:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=2.275e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,281:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=2.420e+01, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,281:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.255e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,281:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=1.779e+04, with an active set of 87 regressors, and the smallest cholesky pivot element being 9.246e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,281:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.238e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,282:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=1.982e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,282:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=7.222e+03, with an active set of 87 regressors, and the smallest cholesky pivot element being 8.752e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,282:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.238e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,282:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=4.121e+03, with an active set of 87 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,282:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:34:49,282:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.434e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,282:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=9.354e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,283:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=5.410e+01, with an active set of 65 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,283:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=1.868e+01, with an active set of 79 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-06 12:34:49,283:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.076e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,283:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=3.315e+03, with an active set of 89 regressors, and the smallest cholesky pivot element being 9.186e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,284:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=7.693e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,284:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=4.822e+01, with an active set of 67 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,284:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=7.003e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,284:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=6.873e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,285:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=3.154e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,285:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=6.817e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,285:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=4.393e+01, with an active set of 69 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,285:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=2.951e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 9.186e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,285:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=8.881e+01, with an active set of 56 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-06 12:34:49,285:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=1.717e+01, with an active set of 81 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,285:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=4.393e+01, with an active set of 69 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-06 12:34:49,285:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=6.765e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,285:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=1.544e+01, with an active set of 81 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,286:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.665e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,286:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=1.497e+01, with an active set of 81 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-06 12:34:49,286:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.591e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,287:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.590e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,287:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=4.131e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,287:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=5.414e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,287:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.585e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,287:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=5.194e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,288:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.577e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,288:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=5.158e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,288:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.573e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,288:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=4.573e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,288:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.567e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,288:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=4.544e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,288:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 91 iterations, i.e. alpha=1.183e+01, with an active set of 85 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,288:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.552e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,288:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=4.447e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,289:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 91 iterations, i.e. alpha=1.183e+01, with an active set of 85 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,289:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.549e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,289:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=3.404e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,289:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.540e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,289:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.538e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,289:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=3.837e+01, with an active set of 75 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,290:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.515e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,290:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.499e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,290:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.459e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,291:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.340e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,291:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.183e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,291:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.162e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,291:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.161e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,292:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=6.700e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,292:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 100 iterations, i.e. alpha=2.768e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,292:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 115 iterations, i.e. alpha=1.084e+03, with an active set of 93 regressors, and the smallest cholesky pivot element being 9.186e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,292:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 89 iterations, i.e. alpha=4.388e+01, with an active set of 81 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,293:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=3.391e+01, with an active set of 91 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,293:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 91 iterations, i.e. alpha=4.361e+01, with an active set of 82 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,294:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.063e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,294:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.033e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 9.186e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,294:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.566e+02, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,294:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=4.202e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,294:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=7.130e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,295:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=3.497e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,295:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=4.110e+02, with an active set of 93 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,295:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.104e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,295:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.269e+02, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,295:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.100e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,296:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.257e+02, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,296:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.097e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,296:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.237e+02, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,296:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.093e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,296:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.231e+02, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,296:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.090e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,296:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.215e+02, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,297:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.080e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,297:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.200e+02, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,297:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.077e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,297:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.179e+02, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,297:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.281e+02, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-06 12:34:49,297:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.176e+02, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,298:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.073e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,298:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.176e+02, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,298:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.167e+02, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,298:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.162e+02, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,298:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.063e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,298:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.152e+02, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,299:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.062e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,299:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.123e+02, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,299:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.058e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,299:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.110e+02, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,299:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.049e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,299:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.043e+02, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,299:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=1.278e+02, with an active set of 85 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,299:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.038e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,299:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.036e+02, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,300:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.028e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,300:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=9.896e+01, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,300:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.022e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,300:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=9.338e+01, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,300:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=9.542e+01, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,300:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=8.408e+01, with an active set of 87 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,301:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=9.071e+01, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,301:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=5.506e+01, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,301:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=8.066e+01, with an active set of 87 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,301:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=7.386e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,302:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=7.202e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,302:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=6.238e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,302:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=5.740e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,302:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=1.047e+02, with an active set of 51 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,303:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=5.617e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,303:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=1.047e+02, with an active set of 51 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,303:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=1.047e+02, with an active set of 51 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-06 12:34:49,304:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=3.785e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,304:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=3.218e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,305:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=3.181e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,305:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=3.046e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,305:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=3.008e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,306:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=2.788e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,306:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=2.530e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,306:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=2.449e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,306:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=1.333e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,307:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=1.246e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,307:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=1.186e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,307:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=1.121e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,308:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=1.057e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,308:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=1.048e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,308:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=1.040e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,308:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=9.189e+00, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,309:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=8.065e+00, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,309:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=5.099e+00, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,309:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=3.129e+00, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,310:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=2.988e+00, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,310:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=2.725e+00, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,310:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=2.585e+00, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,311:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=4.500e-01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,317:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=6.920e+08, with an active set of 75 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,319:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=6.803e+08, with an active set of 78 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,319:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=6.803e+08, with an active set of 78 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,319:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=6.802e+08, with an active set of 78 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,320:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=6.558e+08, with an active set of 79 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,321:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=6.536e+08, with an active set of 80 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,327:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 128 iterations, i.e. alpha=2.938e+10, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,328:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=2.370e+10, with an active set of 95 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,328:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=1.604e+10, with an active set of 95 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,329:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=1.009e+10, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,329:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=9.713e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,329:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=9.684e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,330:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=9.519e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,330:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=9.492e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,330:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=8.807e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,331:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=8.756e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,331:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=8.691e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,331:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=8.285e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,332:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=8.091e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,332:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=7.984e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,332:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=7.944e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,332:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=7.869e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,333:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=7.813e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,333:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=7.778e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,333:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=7.716e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,334:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=7.687e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,334:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=7.557e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,334:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=7.482e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,335:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=6.438e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,335:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=6.090e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,372:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:776: RuntimeWarning: overflow encountered in multiply
  coef[active] = prev_coef[active] + gamma_ * least_squares

2023-03-06 12:34:49,373:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:735: RuntimeWarning: overflow encountered in true_divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-03-06 12:34:49,373:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:739: RuntimeWarning: overflow encountered in true_divide
  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny32))

2023-03-06 12:34:49,374:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:740: RuntimeWarning: overflow encountered in double_scalars
  gamma_ = min(g1, g2, C / AA)

2023-03-06 12:34:49,375:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:744: RuntimeWarning: overflow encountered in true_divide
  z = -coef[active] / (least_squares + tiny32)

2023-03-06 12:34:49,376:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 412 iterations, i.e. alpha=1.387e+299, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,376:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 412 iterations, i.e. alpha=9.066e+298, with an active set of 91 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,377:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 412 iterations, i.e. alpha=6.185e+298, with an active set of 91 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,377:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 412 iterations, i.e. alpha=5.212e+298, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,377:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 412 iterations, i.e. alpha=5.204e+298, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,377:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 412 iterations, i.e. alpha=5.198e+298, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,377:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 412 iterations, i.e. alpha=5.197e+298, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,377:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 412 iterations, i.e. alpha=5.166e+298, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,378:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 412 iterations, i.e. alpha=5.152e+298, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,378:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 412 iterations, i.e. alpha=5.151e+298, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,378:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 412 iterations, i.e. alpha=5.145e+298, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,378:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 412 iterations, i.e. alpha=5.143e+298, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,378:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 412 iterations, i.e. alpha=5.141e+298, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,378:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 412 iterations, i.e. alpha=5.136e+298, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,379:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 412 iterations, i.e. alpha=5.135e+298, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,379:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 412 iterations, i.e. alpha=5.133e+298, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,379:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 412 iterations, i.e. alpha=5.124e+298, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,379:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 412 iterations, i.e. alpha=5.118e+298, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,379:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 412 iterations, i.e. alpha=5.101e+298, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,379:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 412 iterations, i.e. alpha=5.097e+298, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,380:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 412 iterations, i.e. alpha=5.088e+298, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,411:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:34:49,412:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:34:49,416:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.756e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,416:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.774e+02, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,417:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.668e+02, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,417:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.621e+02, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,417:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.605e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,417:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.605e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,418:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.204e+02, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,418:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.204e+02, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,419:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.246e+02, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,419:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.034e+02, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,420:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.718e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,420:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.718e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,421:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=8.069e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,421:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=8.023e+01, with an active set of 48 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,421:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=8.538e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,421:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=8.023e+01, with an active set of 48 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,421:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=8.538e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,421:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=8.023e+01, with an active set of 48 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,422:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=8.538e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,422:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=7.701e+01, with an active set of 49 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,422:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=6.898e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,423:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=6.716e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,423:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=7.136e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,423:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=5.958e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,424:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=5.685e+01, with an active set of 62 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,424:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=4.400e+01, with an active set of 64 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,425:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=4.400e+01, with an active set of 64 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,425:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.642e+01, with an active set of 62 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,425:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.642e+01, with an active set of 62 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,425:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.642e+01, with an active set of 62 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,425:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=3.994e+01, with an active set of 68 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,426:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=3.994e+01, with an active set of 68 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,426:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=4.012e+01, with an active set of 65 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,426:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=4.052e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,426:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=3.834e+01, with an active set of 65 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,426:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=4.052e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,427:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=3.980e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,427:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=3.239e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,427:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=3.239e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,428:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=3.209e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,428:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=3.239e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,428:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=3.140e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,428:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=3.213e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,429:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=2.941e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,429:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=2.733e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,429:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=2.951e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,429:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=2.651e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,429:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=2.291e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,429:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=2.706e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,430:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=2.706e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,430:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=2.164e+01, with an active set of 79 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,430:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=1.950e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,430:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=2.115e+01, with an active set of 78 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,430:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=1.950e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,431:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=1.802e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,431:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=1.802e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,431:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=1.802e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,432:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 93 iterations, i.e. alpha=1.824e+01, with an active set of 84 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,433:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 100 iterations, i.e. alpha=1.839e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,433:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 100 iterations, i.e. alpha=1.816e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,434:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=1.697e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,434:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=1.686e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,434:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=1.625e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,434:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=1.580e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,434:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=1.479e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,434:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=1.439e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,434:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.780e+02, with an active set of 89 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,435:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=1.005e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,435:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=1.005e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,435:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=3.770e+02, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,435:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=9.621e+00, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,435:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=9.496e+00, with an active set of 90 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,436:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=7.165e+00, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,436:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=7.165e+00, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,436:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=6.286e+00, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,436:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=6.229e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,436:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=6.197e+00, with an active set of 91 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,436:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=4.924e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,436:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=5.321e+00, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,436:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=4.549e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,437:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=5.321e+00, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,437:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.876e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,437:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=2.897e+00, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,437:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.865e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,437:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=2.781e+00, with an active set of 91 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,437:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.823e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,437:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=2.338e+00, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,437:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.656e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,437:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.650e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,437:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=1.754e+00, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,437:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.583e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,437:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=1.754e+00, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,438:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.563e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,438:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=9.981e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,438:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=9.981e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,438:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.496e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,438:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.456e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,438:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=9.981e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,438:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.446e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,438:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=6.778e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,438:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.445e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,438:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=6.778e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,439:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=6.282e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,439:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.382e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,439:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.346e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,439:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.326e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,439:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.297e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,439:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.111e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,439:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.076e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,440:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=2.896e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,573:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\utils\extmath.py:153: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2023-03-06 12:34:49,586:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 264, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\metrics\_regression.py", line 191, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\metrics\_regression.py", line 96, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 800, in check_array
    _assert_all_finite(array, allow_nan=force_all_finite == "allow-nan")
  File "C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 114, in _assert_all_finite
    raise ValueError(
ValueError: Input contains NaN, infinity or a value too large for dtype('float64').

  warnings.warn(

2023-03-06 12:34:49,587:INFO:Calculating mean and std
2023-03-06 12:34:49,588:INFO:Creating metrics dataframe
2023-03-06 12:34:49,591:INFO:Uploading results into container
2023-03-06 12:34:49,591:INFO:Uploading model into container now
2023-03-06 12:34:49,591:INFO:_master_model_container: 5
2023-03-06 12:34:49,592:INFO:_display_container: 2
2023-03-06 12:34:49,592:INFO:Lars(random_state=1822)
2023-03-06 12:34:49,592:INFO:create_model() successfully completed......................................
2023-03-06 12:34:49,673:INFO:SubProcess create_model() end ==================================
2023-03-06 12:34:49,673:INFO:Creating metrics dataframe
2023-03-06 12:34:49,682:INFO:Initializing Lasso Least Angle Regression
2023-03-06 12:34:49,682:INFO:Total runtime is 0.16031595468521118 minutes
2023-03-06 12:34:49,687:INFO:SubProcess create_model() called ==================================
2023-03-06 12:34:49,687:INFO:Initializing create_model()
2023-03-06 12:34:49,687:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636DD6040>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:34:49,687:INFO:Checking exceptions
2023-03-06 12:34:49,687:INFO:Importing libraries
2023-03-06 12:34:49,688:INFO:Copying training dataset
2023-03-06 12:34:49,694:INFO:Defining folds
2023-03-06 12:34:49,694:INFO:Declaring metric variables
2023-03-06 12:34:49,697:INFO:Importing untrained model
2023-03-06 12:34:49,703:INFO:Lasso Least Angle Regression Imported successfully
2023-03-06 12:34:49,712:INFO:Starting cross validation
2023-03-06 12:34:49,713:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:34:49,803:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 12:34:49,815:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.321e+02, with an active set of 33 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,829:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 12:34:49,829:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 12:34:49,830:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=9.608e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,831:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=9.608e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,832:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=8.607e+01, with an active set of 46 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,838:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.796e+02, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,839:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=5.332e+01, with an active set of 57 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,839:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.675e+02, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,839:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=5.086e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,840:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.970e+02, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,840:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=5.085e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,840:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.646e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,840:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=5.009e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,841:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.635e+01, with an active set of 59 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,841:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.635e+01, with an active set of 59 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,841:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.646e+02, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,842:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.635e+01, with an active set of 59 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,842:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.627e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,843:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 64 iterations, alpha=4.708e+01, previous alpha=4.515e+01, with an active set of 59 regressors.
  warnings.warn(

2023-03-06 12:34:49,844:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.171e+02, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,844:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.171e+02, with an active set of 39 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,845:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 12:34:49,846:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.034e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,846:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.034e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,847:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=9.835e+01, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,847:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=9.835e+01, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,848:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.004e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,849:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=9.819e+01, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,849:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=9.819e+01, with an active set of 45 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,850:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.785e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,851:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=7.725e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,851:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=7.725e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,851:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=7.352e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,852:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=8.729e+01, with an active set of 50 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,853:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=6.447e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,853:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=6.447e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,853:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=8.079e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,854:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=8.079e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,854:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=6.306e+01, with an active set of 59 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,854:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.859e+02, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,854:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=6.306e+01, with an active set of 59 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,854:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=7.719e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,855:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=6.145e+01, with an active set of 60 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,855:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=6.963e+01, with an active set of 57 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,855:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.686e+02, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,856:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=6.630e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,857:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.587e+02, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,857:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=5.804e+01, with an active set of 61 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,858:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.215e+02, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,858:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.215e+02, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,859:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=5.416e+01, with an active set of 64 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,860:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 12:34:49,860:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.102e+02, with an active set of 40 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,860:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 67 iterations, alpha=5.379e+01, previous alpha=5.084e+01, with an active set of 66 regressors.
  warnings.warn(

2023-03-06 12:34:49,861:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.102e+02, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,861:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=5.078e+01, with an active set of 62 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,862:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.018e+02, with an active set of 44 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,863:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 64 iterations, alpha=6.249e+01, previous alpha=4.689e+01, with an active set of 63 regressors.
  warnings.warn(

2023-03-06 12:34:49,863:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=9.942e+01, with an active set of 46 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,865:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=9.702e+01, with an active set of 48 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,867:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 12:34:49,867:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=9.023e+01, previous alpha=8.269e+01, with an active set of 53 regressors.
  warnings.warn(

2023-03-06 12:34:49,870:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.529e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,870:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.529e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,873:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=1.257e+02, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,876:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.033e+02, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,876:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.741e+02, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,877:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.364e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,877:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.670e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,877:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.364e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,878:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.364e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,878:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.364e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,878:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.522e+02, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,879:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.522e+02, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,881:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.166e+02, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,881:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=7.637e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,881:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=7.637e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,882:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.149e+02, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,883:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.145e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,885:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 12:34:49,885:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=9.647e+01, with an active set of 49 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,885:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=9.647e+01, with an active set of 49 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,886:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 66 iterations, alpha=6.242e+01, previous alpha=6.235e+01, with an active set of 59 regressors.
  warnings.warn(

2023-03-06 12:34:49,887:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=7.979e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,888:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=7.979e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,888:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=7.979e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,888:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=7.979e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,889:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=7.136e+01, with an active set of 56 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,891:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 59 iterations, alpha=7.735e+01, previous alpha=6.879e+01, with an active set of 58 regressors.
  warnings.warn(

2023-03-06 12:34:49,894:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.565e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

odel = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 12:34:49,900:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.122e+02, with an active set of 40 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,901:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=9.691e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,903:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=8.842e+01, with an active set of 43 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,904:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=8.824e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,904:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=8.824e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,905:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=8.824e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,905:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=8.824e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,905:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.615e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,906:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.479e+02, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,907:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=7.826e+01, with an active set of 50 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,908:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=7.826e+01, with an active set of 50 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,908:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.218e+02, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,910:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.105e+02, with an active set of 40 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,910:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.105e+02, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,911:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.032e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,911:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.032e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,911:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 66 iterations, alpha=6.251e+01, previous alpha=6.164e+01, with an active set of 57 regressors.
  warnings.warn(

2023-03-06 12:34:49,914:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=8.096e+01, with an active set of 49 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,914:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=8.096e+01, with an active set of 49 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,915:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=8.096e+01, with an active set of 49 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,915:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=8.051e+01, with an active set of 49 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,919:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=5.525e+01, with an active set of 57 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,920:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=5.053e+01, with an active set of 59 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,921:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=4.738e+01, with an active set of 60 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,921:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=4.728e+01, with an active set of 60 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,922:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=3.749e+01, with an active set of 62 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,923:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=3.749e+01, with an active set of 62 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,924:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=3.474e+01, with an active set of 64 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,925:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=3.042e+01, with an active set of 67 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,927:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=2.687e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,928:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 75 iterations, alpha=2.901e+01, previous alpha=2.687e+01, with an active set of 70 regressors.
  warnings.warn(

2023-03-06 12:34:49,974:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 12:34:49,978:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.774e+02, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,979:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.605e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,979:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.605e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,980:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.246e+02, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,981:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.034e+02, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,982:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=8.069e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,983:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=8.023e+01, with an active set of 48 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,983:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 12:34:49,983:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=8.023e+01, with an active set of 48 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,983:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=8.023e+01, with an active set of 48 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,984:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=7.701e+01, with an active set of 49 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,984:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=7.136e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,985:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=5.958e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,986:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 61 iterations, alpha=5.126e+01, previous alpha=4.932e+01, with an active set of 60 regressors.
  warnings.warn(

2023-03-06 12:34:49,987:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.756e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,988:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.668e+02, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,988:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.621e+02, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,989:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.204e+02, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,989:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.204e+02, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,991:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.718e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,991:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.718e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,992:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=8.538e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,992:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=8.538e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,992:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=8.538e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:34:49,993:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=8.696e+01, previous alpha=8.294e+01, with an active set of 53 regressors.
  warnings.warn(

2023-03-06 12:34:50,022:INFO:Calculating mean and std
2023-03-06 12:34:50,023:INFO:Creating metrics dataframe
2023-03-06 12:34:50,026:INFO:Uploading results into container
2023-03-06 12:34:50,027:INFO:Uploading model into container now
2023-03-06 12:34:50,027:INFO:_master_model_container: 6
2023-03-06 12:34:50,027:INFO:_display_container: 2
2023-03-06 12:34:50,028:INFO:LassoLars(random_state=1822)
2023-03-06 12:34:50,028:INFO:create_model() successfully completed......................................
2023-03-06 12:34:50,116:INFO:SubProcess create_model() end ==================================
2023-03-06 12:34:50,116:INFO:Creating metrics dataframe
2023-03-06 12:34:50,125:INFO:Initializing Orthogonal Matching Pursuit
2023-03-06 12:34:50,125:INFO:Total runtime is 0.1677064577738444 minutes
2023-03-06 12:34:50,127:INFO:SubProcess create_model() called ==================================
2023-03-06 12:34:50,128:INFO:Initializing create_model()
2023-03-06 12:34:50,128:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636DD6040>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:34:50,128:INFO:Checking exceptions
2023-03-06 12:34:50,128:INFO:Importing libraries
2023-03-06 12:34:50,129:INFO:Copying training dataset
2023-03-06 12:34:50,134:INFO:Defining folds
2023-03-06 12:34:50,135:INFO:Declaring metric variables
2023-03-06 12:34:50,138:INFO:Importing untrained model
2023-03-06 12:34:50,143:INFO:Orthogonal Matching Pursuit Imported successfully
2023-03-06 12:34:50,150:INFO:Starting cross validation
2023-03-06 12:34:50,151:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:34:50,239:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:34:50,246:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:34:50,269:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:34:50,279:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:34:50,297:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:34:50,307:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:34:50,317:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:34:50,332:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:34:50,380:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:34:50,384:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:34:50,424:INFO:Calculating mean and std
2023-03-06 12:34:50,425:INFO:Creating metrics dataframe
2023-03-06 12:34:50,428:INFO:Uploading results into container
2023-03-06 12:34:50,429:INFO:Uploading model into container now
2023-03-06 12:34:50,429:INFO:_master_model_container: 7
2023-03-06 12:34:50,429:INFO:_display_container: 2
2023-03-06 12:34:50,430:INFO:OrthogonalMatchingPursuit()
2023-03-06 12:34:50,430:INFO:create_model() successfully completed......................................
2023-03-06 12:34:50,517:INFO:SubProcess create_model() end ==================================
2023-03-06 12:34:50,517:INFO:Creating metrics dataframe
2023-03-06 12:34:50,526:INFO:Initializing Bayesian Ridge
2023-03-06 12:34:50,526:INFO:Total runtime is 0.17438775300979614 minutes
2023-03-06 12:34:50,529:INFO:SubProcess create_model() called ==================================
2023-03-06 12:34:50,530:INFO:Initializing create_model()
2023-03-06 12:34:50,530:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636DD6040>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:34:50,530:INFO:Checking exceptions
2023-03-06 12:34:50,530:INFO:Importing libraries
2023-03-06 12:34:50,530:INFO:Copying training dataset
2023-03-06 12:34:50,536:INFO:Defining folds
2023-03-06 12:34:50,537:INFO:Declaring metric variables
2023-03-06 12:34:50,541:INFO:Importing untrained model
2023-03-06 12:34:50,545:INFO:Bayesian Ridge Imported successfully
2023-03-06 12:34:50,554:INFO:Starting cross validation
2023-03-06 12:34:50,556:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:34:50,831:INFO:Calculating mean and std
2023-03-06 12:34:50,833:INFO:Creating metrics dataframe
2023-03-06 12:34:50,836:INFO:Uploading results into container
2023-03-06 12:34:50,837:INFO:Uploading model into container now
2023-03-06 12:34:50,837:INFO:_master_model_container: 8
2023-03-06 12:34:50,837:INFO:_display_container: 2
2023-03-06 12:34:50,838:INFO:BayesianRidge()
2023-03-06 12:34:50,838:INFO:create_model() successfully completed......................................
2023-03-06 12:34:50,926:INFO:SubProcess create_model() end ==================================
2023-03-06 12:34:50,927:INFO:Creating metrics dataframe
2023-03-06 12:34:50,936:INFO:Initializing Passive Aggressive Regressor
2023-03-06 12:34:50,936:INFO:Total runtime is 0.18121947050094606 minutes
2023-03-06 12:34:50,940:INFO:SubProcess create_model() called ==================================
2023-03-06 12:34:50,940:INFO:Initializing create_model()
2023-03-06 12:34:50,940:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636DD6040>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:34:50,940:INFO:Checking exceptions
2023-03-06 12:34:50,940:INFO:Importing libraries
2023-03-06 12:34:50,940:INFO:Copying training dataset
2023-03-06 12:34:50,946:INFO:Defining folds
2023-03-06 12:34:50,946:INFO:Declaring metric variables
2023-03-06 12:34:50,950:INFO:Importing untrained model
2023-03-06 12:34:50,958:INFO:Passive Aggressive Regressor Imported successfully
2023-03-06 12:34:50,965:INFO:Starting cross validation
2023-03-06 12:34:50,968:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:34:51,235:INFO:Calculating mean and std
2023-03-06 12:34:51,236:INFO:Creating metrics dataframe
2023-03-06 12:34:51,239:INFO:Uploading results into container
2023-03-06 12:34:51,240:INFO:Uploading model into container now
2023-03-06 12:34:51,240:INFO:_master_model_container: 9
2023-03-06 12:34:51,240:INFO:_display_container: 2
2023-03-06 12:34:51,241:INFO:PassiveAggressiveRegressor(random_state=1822)
2023-03-06 12:34:51,241:INFO:create_model() successfully completed......................................
2023-03-06 12:34:51,329:INFO:SubProcess create_model() end ==================================
2023-03-06 12:34:51,329:INFO:Creating metrics dataframe
2023-03-06 12:34:51,341:INFO:Initializing Huber Regressor
2023-03-06 12:34:51,341:INFO:Total runtime is 0.187968651453654 minutes
2023-03-06 12:34:51,344:INFO:SubProcess create_model() called ==================================
2023-03-06 12:34:51,345:INFO:Initializing create_model()
2023-03-06 12:34:51,345:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636DD6040>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:34:51,345:INFO:Checking exceptions
2023-03-06 12:34:51,345:INFO:Importing libraries
2023-03-06 12:34:51,345:INFO:Copying training dataset
2023-03-06 12:34:51,351:INFO:Defining folds
2023-03-06 12:34:51,351:INFO:Declaring metric variables
2023-03-06 12:34:51,356:INFO:Importing untrained model
2023-03-06 12:34:51,360:INFO:Huber Regressor Imported successfully
2023-03-06 12:34:51,367:INFO:Starting cross validation
2023-03-06 12:34:51,369:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:34:51,580:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-06 12:34:51,627:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-06 12:34:51,654:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-06 12:34:51,664:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-06 12:34:51,676:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-06 12:34:51,702:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-06 12:34:51,704:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-06 12:34:51,704:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-06 12:34:52,094:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-06 12:34:52,107:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-06 12:34:52,132:INFO:Calculating mean and std
2023-03-06 12:34:52,134:INFO:Creating metrics dataframe
2023-03-06 12:34:52,136:INFO:Uploading results into container
2023-03-06 12:34:52,137:INFO:Uploading model into container now
2023-03-06 12:34:52,138:INFO:_master_model_container: 10
2023-03-06 12:34:52,138:INFO:_display_container: 2
2023-03-06 12:34:52,138:INFO:HuberRegressor()
2023-03-06 12:34:52,139:INFO:create_model() successfully completed......................................
2023-03-06 12:34:52,227:INFO:SubProcess create_model() end ==================================
2023-03-06 12:34:52,227:INFO:Creating metrics dataframe
2023-03-06 12:34:52,237:INFO:Initializing K Neighbors Regressor
2023-03-06 12:34:52,237:INFO:Total runtime is 0.20290261507034305 minutes
2023-03-06 12:34:52,241:INFO:SubProcess create_model() called ==================================
2023-03-06 12:34:52,242:INFO:Initializing create_model()
2023-03-06 12:34:52,242:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636DD6040>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:34:52,242:INFO:Checking exceptions
2023-03-06 12:34:52,242:INFO:Importing libraries
2023-03-06 12:34:52,242:INFO:Copying training dataset
2023-03-06 12:34:52,248:INFO:Defining folds
2023-03-06 12:34:52,248:INFO:Declaring metric variables
2023-03-06 12:34:52,252:INFO:Importing untrained model
2023-03-06 12:34:52,257:INFO:K Neighbors Regressor Imported successfully
2023-03-06 12:34:52,265:INFO:Starting cross validation
2023-03-06 12:34:52,268:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:34:52,545:INFO:Calculating mean and std
2023-03-06 12:34:52,547:INFO:Creating metrics dataframe
2023-03-06 12:34:52,550:INFO:Uploading results into container
2023-03-06 12:34:52,551:INFO:Uploading model into container now
2023-03-06 12:34:52,551:INFO:_master_model_container: 11
2023-03-06 12:34:52,551:INFO:_display_container: 2
2023-03-06 12:34:52,552:INFO:KNeighborsRegressor(n_jobs=-1)
2023-03-06 12:34:52,552:INFO:create_model() successfully completed......................................
2023-03-06 12:34:52,639:INFO:SubProcess create_model() end ==================================
2023-03-06 12:34:52,639:INFO:Creating metrics dataframe
2023-03-06 12:34:52,649:INFO:Initializing Decision Tree Regressor
2023-03-06 12:34:52,649:INFO:Total runtime is 0.20975879430770877 minutes
2023-03-06 12:34:52,652:INFO:SubProcess create_model() called ==================================
2023-03-06 12:34:52,652:INFO:Initializing create_model()
2023-03-06 12:34:52,653:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636DD6040>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:34:52,653:INFO:Checking exceptions
2023-03-06 12:34:52,653:INFO:Importing libraries
2023-03-06 12:34:52,653:INFO:Copying training dataset
2023-03-06 12:34:52,659:INFO:Defining folds
2023-03-06 12:34:52,659:INFO:Declaring metric variables
2023-03-06 12:34:52,664:INFO:Importing untrained model
2023-03-06 12:34:52,668:INFO:Decision Tree Regressor Imported successfully
2023-03-06 12:34:52,678:INFO:Starting cross validation
2023-03-06 12:34:52,679:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:34:52,925:INFO:Calculating mean and std
2023-03-06 12:34:52,926:INFO:Creating metrics dataframe
2023-03-06 12:34:52,929:INFO:Uploading results into container
2023-03-06 12:34:52,930:INFO:Uploading model into container now
2023-03-06 12:34:52,930:INFO:_master_model_container: 12
2023-03-06 12:34:52,930:INFO:_display_container: 2
2023-03-06 12:34:52,930:INFO:DecisionTreeRegressor(random_state=1822)
2023-03-06 12:34:52,930:INFO:create_model() successfully completed......................................
2023-03-06 12:34:53,012:INFO:SubProcess create_model() end ==================================
2023-03-06 12:34:53,012:INFO:Creating metrics dataframe
2023-03-06 12:34:53,022:INFO:Initializing Random Forest Regressor
2023-03-06 12:34:53,022:INFO:Total runtime is 0.21597552696863814 minutes
2023-03-06 12:34:53,026:INFO:SubProcess create_model() called ==================================
2023-03-06 12:34:53,027:INFO:Initializing create_model()
2023-03-06 12:34:53,027:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636DD6040>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:34:53,027:INFO:Checking exceptions
2023-03-06 12:34:53,027:INFO:Importing libraries
2023-03-06 12:34:53,027:INFO:Copying training dataset
2023-03-06 12:34:53,033:INFO:Defining folds
2023-03-06 12:34:53,034:INFO:Declaring metric variables
2023-03-06 12:34:53,038:INFO:Importing untrained model
2023-03-06 12:34:53,043:INFO:Random Forest Regressor Imported successfully
2023-03-06 12:34:53,053:INFO:Starting cross validation
2023-03-06 12:34:53,056:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:34:54,728:INFO:Calculating mean and std
2023-03-06 12:34:54,729:INFO:Creating metrics dataframe
2023-03-06 12:34:54,732:INFO:Uploading results into container
2023-03-06 12:34:54,733:INFO:Uploading model into container now
2023-03-06 12:34:54,733:INFO:_master_model_container: 13
2023-03-06 12:34:54,733:INFO:_display_container: 2
2023-03-06 12:34:54,734:INFO:RandomForestRegressor(n_jobs=-1, random_state=1822)
2023-03-06 12:34:54,734:INFO:create_model() successfully completed......................................
2023-03-06 12:34:54,818:INFO:SubProcess create_model() end ==================================
2023-03-06 12:34:54,818:INFO:Creating metrics dataframe
2023-03-06 12:34:54,828:INFO:Initializing Extra Trees Regressor
2023-03-06 12:34:54,828:INFO:Total runtime is 0.24608528216679895 minutes
2023-03-06 12:34:54,831:INFO:SubProcess create_model() called ==================================
2023-03-06 12:34:54,831:INFO:Initializing create_model()
2023-03-06 12:34:54,831:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636DD6040>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:34:54,831:INFO:Checking exceptions
2023-03-06 12:34:54,832:INFO:Importing libraries
2023-03-06 12:34:54,832:INFO:Copying training dataset
2023-03-06 12:34:54,838:INFO:Defining folds
2023-03-06 12:34:54,838:INFO:Declaring metric variables
2023-03-06 12:34:54,842:INFO:Importing untrained model
2023-03-06 12:34:54,847:INFO:Extra Trees Regressor Imported successfully
2023-03-06 12:34:54,856:INFO:Starting cross validation
2023-03-06 12:34:54,858:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:34:56,534:INFO:Calculating mean and std
2023-03-06 12:34:56,535:INFO:Creating metrics dataframe
2023-03-06 12:34:56,538:INFO:Uploading results into container
2023-03-06 12:34:56,539:INFO:Uploading model into container now
2023-03-06 12:34:56,539:INFO:_master_model_container: 14
2023-03-06 12:34:56,539:INFO:_display_container: 2
2023-03-06 12:34:56,540:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=1822)
2023-03-06 12:34:56,540:INFO:create_model() successfully completed......................................
2023-03-06 12:34:56,623:INFO:SubProcess create_model() end ==================================
2023-03-06 12:34:56,623:INFO:Creating metrics dataframe
2023-03-06 12:34:56,633:INFO:Initializing AdaBoost Regressor
2023-03-06 12:34:56,633:INFO:Total runtime is 0.2761737267176311 minutes
2023-03-06 12:34:56,636:INFO:SubProcess create_model() called ==================================
2023-03-06 12:34:56,637:INFO:Initializing create_model()
2023-03-06 12:34:56,637:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636DD6040>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:34:56,637:INFO:Checking exceptions
2023-03-06 12:34:56,637:INFO:Importing libraries
2023-03-06 12:34:56,637:INFO:Copying training dataset
2023-03-06 12:34:56,643:INFO:Defining folds
2023-03-06 12:34:56,644:INFO:Declaring metric variables
2023-03-06 12:34:56,647:INFO:Importing untrained model
2023-03-06 12:34:56,652:INFO:AdaBoost Regressor Imported successfully
2023-03-06 12:34:56,660:INFO:Starting cross validation
2023-03-06 12:34:56,663:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:34:57,565:INFO:Calculating mean and std
2023-03-06 12:34:57,566:INFO:Creating metrics dataframe
2023-03-06 12:34:57,569:INFO:Uploading results into container
2023-03-06 12:34:57,569:INFO:Uploading model into container now
2023-03-06 12:34:57,569:INFO:_master_model_container: 15
2023-03-06 12:34:57,570:INFO:_display_container: 2
2023-03-06 12:34:57,570:INFO:AdaBoostRegressor(random_state=1822)
2023-03-06 12:34:57,570:INFO:create_model() successfully completed......................................
2023-03-06 12:34:57,655:INFO:SubProcess create_model() end ==================================
2023-03-06 12:34:57,655:INFO:Creating metrics dataframe
2023-03-06 12:34:57,665:INFO:Initializing Gradient Boosting Regressor
2023-03-06 12:34:57,665:INFO:Total runtime is 0.29336444536844897 minutes
2023-03-06 12:34:57,668:INFO:SubProcess create_model() called ==================================
2023-03-06 12:34:57,668:INFO:Initializing create_model()
2023-03-06 12:34:57,669:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636DD6040>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:34:57,669:INFO:Checking exceptions
2023-03-06 12:34:57,669:INFO:Importing libraries
2023-03-06 12:34:57,669:INFO:Copying training dataset
2023-03-06 12:34:57,675:INFO:Defining folds
2023-03-06 12:34:57,675:INFO:Declaring metric variables
2023-03-06 12:34:57,679:INFO:Importing untrained model
2023-03-06 12:34:57,683:INFO:Gradient Boosting Regressor Imported successfully
2023-03-06 12:34:57,691:INFO:Starting cross validation
2023-03-06 12:34:57,692:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:34:58,666:INFO:Calculating mean and std
2023-03-06 12:34:58,668:INFO:Creating metrics dataframe
2023-03-06 12:34:58,671:INFO:Uploading results into container
2023-03-06 12:34:58,671:INFO:Uploading model into container now
2023-03-06 12:34:58,672:INFO:_master_model_container: 16
2023-03-06 12:34:58,672:INFO:_display_container: 2
2023-03-06 12:34:58,672:INFO:GradientBoostingRegressor(random_state=1822)
2023-03-06 12:34:58,672:INFO:create_model() successfully completed......................................
2023-03-06 12:34:58,752:INFO:SubProcess create_model() end ==================================
2023-03-06 12:34:58,752:INFO:Creating metrics dataframe
2023-03-06 12:34:58,765:INFO:Initializing Extreme Gradient Boosting
2023-03-06 12:34:58,766:INFO:Total runtime is 0.311702044804891 minutes
2023-03-06 12:34:58,769:INFO:SubProcess create_model() called ==================================
2023-03-06 12:34:58,769:INFO:Initializing create_model()
2023-03-06 12:34:58,770:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636DD6040>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:34:58,770:INFO:Checking exceptions
2023-03-06 12:34:58,770:INFO:Importing libraries
2023-03-06 12:34:58,770:INFO:Copying training dataset
2023-03-06 12:34:58,776:INFO:Defining folds
2023-03-06 12:34:58,776:INFO:Declaring metric variables
2023-03-06 12:34:58,780:INFO:Importing untrained model
2023-03-06 12:34:58,786:INFO:Extreme Gradient Boosting Imported successfully
2023-03-06 12:34:58,793:INFO:Starting cross validation
2023-03-06 12:34:58,795:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:35:00,793:INFO:Calculating mean and std
2023-03-06 12:35:00,794:INFO:Creating metrics dataframe
2023-03-06 12:35:00,797:INFO:Uploading results into container
2023-03-06 12:35:00,798:INFO:Uploading model into container now
2023-03-06 12:35:00,798:INFO:_master_model_container: 17
2023-03-06 12:35:00,798:INFO:_display_container: 2
2023-03-06 12:35:00,799:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=1822, ...)
2023-03-06 12:35:00,799:INFO:create_model() successfully completed......................................
2023-03-06 12:35:00,882:INFO:SubProcess create_model() end ==================================
2023-03-06 12:35:00,883:INFO:Creating metrics dataframe
2023-03-06 12:35:00,894:INFO:Initializing Light Gradient Boosting Machine
2023-03-06 12:35:00,894:INFO:Total runtime is 0.3471873839696249 minutes
2023-03-06 12:35:00,897:INFO:SubProcess create_model() called ==================================
2023-03-06 12:35:00,898:INFO:Initializing create_model()
2023-03-06 12:35:00,898:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636DD6040>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:35:00,898:INFO:Checking exceptions
2023-03-06 12:35:00,898:INFO:Importing libraries
2023-03-06 12:35:00,898:INFO:Copying training dataset
2023-03-06 12:35:00,905:INFO:Defining folds
2023-03-06 12:35:00,905:INFO:Declaring metric variables
2023-03-06 12:35:00,910:INFO:Importing untrained model
2023-03-06 12:35:00,913:INFO:Light Gradient Boosting Machine Imported successfully
2023-03-06 12:35:00,921:INFO:Starting cross validation
2023-03-06 12:35:00,922:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:35:02,826:INFO:Calculating mean and std
2023-03-06 12:35:02,828:INFO:Creating metrics dataframe
2023-03-06 12:35:02,833:INFO:Uploading results into container
2023-03-06 12:35:02,834:INFO:Uploading model into container now
2023-03-06 12:35:02,835:INFO:_master_model_container: 18
2023-03-06 12:35:02,835:INFO:_display_container: 2
2023-03-06 12:35:02,836:INFO:LGBMRegressor(random_state=1822)
2023-03-06 12:35:02,836:INFO:create_model() successfully completed......................................
2023-03-06 12:35:02,938:INFO:SubProcess create_model() end ==================================
2023-03-06 12:35:02,939:INFO:Creating metrics dataframe
2023-03-06 12:35:02,956:INFO:Initializing Dummy Regressor
2023-03-06 12:35:02,956:INFO:Total runtime is 0.38154554367065435 minutes
2023-03-06 12:35:02,962:INFO:SubProcess create_model() called ==================================
2023-03-06 12:35:02,962:INFO:Initializing create_model()
2023-03-06 12:35:02,963:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636DD6040>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:35:02,963:INFO:Checking exceptions
2023-03-06 12:35:02,963:INFO:Importing libraries
2023-03-06 12:35:02,963:INFO:Copying training dataset
2023-03-06 12:35:02,971:INFO:Defining folds
2023-03-06 12:35:02,971:INFO:Declaring metric variables
2023-03-06 12:35:02,976:INFO:Importing untrained model
2023-03-06 12:35:02,982:INFO:Dummy Regressor Imported successfully
2023-03-06 12:35:02,991:INFO:Starting cross validation
2023-03-06 12:35:02,994:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:35:03,307:INFO:Calculating mean and std
2023-03-06 12:35:03,309:INFO:Creating metrics dataframe
2023-03-06 12:35:03,316:INFO:Uploading results into container
2023-03-06 12:35:03,317:INFO:Uploading model into container now
2023-03-06 12:35:03,317:INFO:_master_model_container: 19
2023-03-06 12:35:03,317:INFO:_display_container: 2
2023-03-06 12:35:03,318:INFO:DummyRegressor()
2023-03-06 12:35:03,318:INFO:create_model() successfully completed......................................
2023-03-06 12:35:03,433:INFO:SubProcess create_model() end ==================================
2023-03-06 12:35:03,434:INFO:Creating metrics dataframe
2023-03-06 12:35:03,466:INFO:Initializing create_model()
2023-03-06 12:35:03,466:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=GradientBoostingRegressor(random_state=1822), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:35:03,466:INFO:Checking exceptions
2023-03-06 12:35:03,468:INFO:Importing libraries
2023-03-06 12:35:03,468:INFO:Copying training dataset
2023-03-06 12:35:03,476:INFO:Defining folds
2023-03-06 12:35:03,476:INFO:Declaring metric variables
2023-03-06 12:35:03,476:INFO:Importing untrained model
2023-03-06 12:35:03,476:INFO:Declaring custom model
2023-03-06 12:35:03,477:INFO:Gradient Boosting Regressor Imported successfully
2023-03-06 12:35:03,479:INFO:Cross validation set to False
2023-03-06 12:35:03,479:INFO:Fitting Model
2023-03-06 12:35:03,615:INFO:GradientBoostingRegressor(random_state=1822)
2023-03-06 12:35:03,615:INFO:create_model() successfully completed......................................
2023-03-06 12:35:03,740:INFO:_master_model_container: 19
2023-03-06 12:35:03,740:INFO:_display_container: 2
2023-03-06 12:35:03,740:INFO:GradientBoostingRegressor(random_state=1822)
2023-03-06 12:35:03,740:INFO:compare_models() successfully completed......................................
2023-03-06 12:35:48,923:INFO:Initializing evaluate_model()
2023-03-06 12:35:48,923:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=GradientBoostingRegressor(random_state=1822), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-03-06 12:35:48,960:INFO:Initializing plot_model()
2023-03-06 12:35:48,960:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=1822), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, system=True)
2023-03-06 12:35:48,960:INFO:Checking exceptions
2023-03-06 12:35:48,963:INFO:Preloading libraries
2023-03-06 12:35:48,971:INFO:Copying training dataset
2023-03-06 12:35:48,972:INFO:Plot type: pipeline
2023-03-06 12:35:49,216:INFO:Visual Rendered Successfully
2023-03-06 12:35:49,303:INFO:plot_model() successfully completed......................................
2023-03-06 12:35:51,119:INFO:Initializing predict_model()
2023-03-06 12:35:51,119:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=GradientBoostingRegressor(random_state=1822), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x0000026636EE0E50>)
2023-03-06 12:35:51,119:INFO:Checking exceptions
2023-03-06 12:35:51,119:INFO:Preloading libraries
2023-03-06 12:35:51,121:INFO:Set up data.
2023-03-06 12:35:51,128:INFO:Set up index.
2023-03-06 12:36:00,704:INFO:Initializing predict_model()
2023-03-06 12:36:00,704:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=GradientBoostingRegressor(random_state=1822), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x0000026636EE0670>)
2023-03-06 12:36:00,704:INFO:Checking exceptions
2023-03-06 12:36:00,705:INFO:Preloading libraries
2023-03-06 12:36:00,707:INFO:Set up data.
2023-03-06 12:36:00,725:INFO:Set up index.
2023-03-06 12:36:15,620:INFO:Initializing compare_models()
2023-03-06 12:36:15,620:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, include=None, fold=None, round=4, cross_validation=True, sort=RMSE, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'RMSE', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-03-06 12:36:15,621:INFO:Checking exceptions
2023-03-06 12:36:15,623:INFO:Preparing display monitor
2023-03-06 12:36:15,665:INFO:Initializing Linear Regression
2023-03-06 12:36:15,665:INFO:Total runtime is 0.0 minutes
2023-03-06 12:36:15,669:INFO:SubProcess create_model() called ==================================
2023-03-06 12:36:15,669:INFO:Initializing create_model()
2023-03-06 12:36:15,669:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636689BE0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:36:15,669:INFO:Checking exceptions
2023-03-06 12:36:15,669:INFO:Importing libraries
2023-03-06 12:36:15,669:INFO:Copying training dataset
2023-03-06 12:36:15,677:INFO:Defining folds
2023-03-06 12:36:15,677:INFO:Declaring metric variables
2023-03-06 12:36:15,680:INFO:Importing untrained model
2023-03-06 12:36:15,685:INFO:Linear Regression Imported successfully
2023-03-06 12:36:15,695:INFO:Starting cross validation
2023-03-06 12:36:15,698:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:36:15,953:INFO:Calculating mean and std
2023-03-06 12:36:15,954:INFO:Creating metrics dataframe
2023-03-06 12:36:15,956:INFO:Uploading results into container
2023-03-06 12:36:15,957:INFO:Uploading model into container now
2023-03-06 12:36:15,957:INFO:_master_model_container: 20
2023-03-06 12:36:15,957:INFO:_display_container: 3
2023-03-06 12:36:15,957:INFO:LinearRegression(n_jobs=-1)
2023-03-06 12:36:15,957:INFO:create_model() successfully completed......................................
2023-03-06 12:36:16,041:INFO:SubProcess create_model() end ==================================
2023-03-06 12:36:16,042:INFO:Creating metrics dataframe
2023-03-06 12:36:16,049:INFO:Initializing Lasso Regression
2023-03-06 12:36:16,049:INFO:Total runtime is 0.006393428643544515 minutes
2023-03-06 12:36:16,052:INFO:SubProcess create_model() called ==================================
2023-03-06 12:36:16,052:INFO:Initializing create_model()
2023-03-06 12:36:16,052:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636689BE0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:36:16,052:INFO:Checking exceptions
2023-03-06 12:36:16,052:INFO:Importing libraries
2023-03-06 12:36:16,052:INFO:Copying training dataset
2023-03-06 12:36:16,058:INFO:Defining folds
2023-03-06 12:36:16,058:INFO:Declaring metric variables
2023-03-06 12:36:16,061:INFO:Importing untrained model
2023-03-06 12:36:16,065:INFO:Lasso Regression Imported successfully
2023-03-06 12:36:16,074:INFO:Starting cross validation
2023-03-06 12:36:16,077:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:36:16,222:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.548e+10, tolerance: 1.481e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-06 12:36:16,231:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.078e+10, tolerance: 1.482e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-06 12:36:16,231:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.282e+10, tolerance: 1.318e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-06 12:36:16,258:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.784e+10, tolerance: 1.446e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-06 12:36:16,261:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.102e+10, tolerance: 1.431e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-06 12:36:16,276:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.090e+09, tolerance: 1.498e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-06 12:36:16,288:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.573e+10, tolerance: 1.284e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-06 12:36:16,298:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.329e+10, tolerance: 1.424e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-06 12:36:16,372:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.389e+09, tolerance: 1.448e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-06 12:36:16,373:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.537e+10, tolerance: 1.381e+08
  model = cd_fast.enet_coordinate_descent(

2023-03-06 12:36:16,399:INFO:Calculating mean and std
2023-03-06 12:36:16,399:INFO:Creating metrics dataframe
2023-03-06 12:36:16,402:INFO:Uploading results into container
2023-03-06 12:36:16,402:INFO:Uploading model into container now
2023-03-06 12:36:16,403:INFO:_master_model_container: 21
2023-03-06 12:36:16,403:INFO:_display_container: 3
2023-03-06 12:36:16,403:INFO:Lasso(random_state=1822)
2023-03-06 12:36:16,403:INFO:create_model() successfully completed......................................
2023-03-06 12:36:16,485:INFO:SubProcess create_model() end ==================================
2023-03-06 12:36:16,485:INFO:Creating metrics dataframe
2023-03-06 12:36:16,493:INFO:Initializing Ridge Regression
2023-03-06 12:36:16,493:INFO:Total runtime is 0.013790341218312581 minutes
2023-03-06 12:36:16,496:INFO:SubProcess create_model() called ==================================
2023-03-06 12:36:16,496:INFO:Initializing create_model()
2023-03-06 12:36:16,496:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636689BE0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:36:16,496:INFO:Checking exceptions
2023-03-06 12:36:16,496:INFO:Importing libraries
2023-03-06 12:36:16,497:INFO:Copying training dataset
2023-03-06 12:36:16,502:INFO:Defining folds
2023-03-06 12:36:16,502:INFO:Declaring metric variables
2023-03-06 12:36:16,505:INFO:Importing untrained model
2023-03-06 12:36:16,509:INFO:Ridge Regression Imported successfully
2023-03-06 12:36:16,517:INFO:Starting cross validation
2023-03-06 12:36:16,519:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:36:16,764:INFO:Calculating mean and std
2023-03-06 12:36:16,765:INFO:Creating metrics dataframe
2023-03-06 12:36:16,768:INFO:Uploading results into container
2023-03-06 12:36:16,769:INFO:Uploading model into container now
2023-03-06 12:36:16,769:INFO:_master_model_container: 22
2023-03-06 12:36:16,769:INFO:_display_container: 3
2023-03-06 12:36:16,769:INFO:Ridge(random_state=1822)
2023-03-06 12:36:16,769:INFO:create_model() successfully completed......................................
2023-03-06 12:36:16,859:INFO:SubProcess create_model() end ==================================
2023-03-06 12:36:16,859:INFO:Creating metrics dataframe
2023-03-06 12:36:16,867:INFO:Initializing Elastic Net
2023-03-06 12:36:16,867:INFO:Total runtime is 0.020022519429524738 minutes
2023-03-06 12:36:16,872:INFO:SubProcess create_model() called ==================================
2023-03-06 12:36:16,873:INFO:Initializing create_model()
2023-03-06 12:36:16,873:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636689BE0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:36:16,873:INFO:Checking exceptions
2023-03-06 12:36:16,873:INFO:Importing libraries
2023-03-06 12:36:16,873:INFO:Copying training dataset
2023-03-06 12:36:16,880:INFO:Defining folds
2023-03-06 12:36:16,880:INFO:Declaring metric variables
2023-03-06 12:36:16,882:INFO:Importing untrained model
2023-03-06 12:36:16,886:INFO:Elastic Net Imported successfully
2023-03-06 12:36:16,896:INFO:Starting cross validation
2023-03-06 12:36:16,897:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:36:17,155:INFO:Calculating mean and std
2023-03-06 12:36:17,156:INFO:Creating metrics dataframe
2023-03-06 12:36:17,160:INFO:Uploading results into container
2023-03-06 12:36:17,160:INFO:Uploading model into container now
2023-03-06 12:36:17,161:INFO:_master_model_container: 23
2023-03-06 12:36:17,161:INFO:_display_container: 3
2023-03-06 12:36:17,162:INFO:ElasticNet(random_state=1822)
2023-03-06 12:36:17,162:INFO:create_model() successfully completed......................................
2023-03-06 12:36:17,246:INFO:SubProcess create_model() end ==================================
2023-03-06 12:36:17,246:INFO:Creating metrics dataframe
2023-03-06 12:36:17,253:INFO:Initializing Least Angle Regression
2023-03-06 12:36:17,253:INFO:Total runtime is 0.026468471686045325 minutes
2023-03-06 12:36:17,256:INFO:SubProcess create_model() called ==================================
2023-03-06 12:36:17,256:INFO:Initializing create_model()
2023-03-06 12:36:17,256:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636689BE0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:36:17,256:INFO:Checking exceptions
2023-03-06 12:36:17,257:INFO:Importing libraries
2023-03-06 12:36:17,257:INFO:Copying training dataset
2023-03-06 12:36:17,263:INFO:Defining folds
2023-03-06 12:36:17,263:INFO:Declaring metric variables
2023-03-06 12:36:17,267:INFO:Importing untrained model
2023-03-06 12:36:17,271:INFO:Least Angle Regression Imported successfully
2023-03-06 12:36:17,280:INFO:Starting cross validation
2023-03-06 12:36:17,282:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:36:17,369:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:36:17,382:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:36:17,390:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.796e+02, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,391:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.675e+02, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,392:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.646e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,394:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.321e+02, with an active set of 33 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,395:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.171e+02, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,396:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.171e+02, with an active set of 39 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,397:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:36:17,397:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=9.608e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,397:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.034e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,398:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.034e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,398:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=9.608e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,399:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=9.835e+01, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,400:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=8.607e+01, with an active set of 46 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,400:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=9.835e+01, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,403:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=7.725e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,403:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=7.725e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,404:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=7.352e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,404:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=6.058e+01, with an active set of 56 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,405:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.970e+02, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,405:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=5.568e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,405:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=6.447e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,405:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=6.447e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,406:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=5.496e+01, with an active set of 59 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,406:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=6.306e+01, with an active set of 59 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,406:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=6.306e+01, with an active set of 59 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,406:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.851e+01, with an active set of 60 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,407:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.851e+01, with an active set of 60 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,407:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:36:17,407:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=6.145e+01, with an active set of 60 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,408:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.627e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,408:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=4.475e+01, with an active set of 61 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,408:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=5.078e+01, with an active set of 62 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,408:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=4.475e+01, with an active set of 61 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,409:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=4.373e+01, with an active set of 62 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,409:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=4.339e+01, with an active set of 62 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,410:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:36:17,410:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=4.467e+01, with an active set of 65 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,410:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=4.355e+01, with an active set of 65 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,410:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=3.707e+01, with an active set of 63 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,411:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=3.651e+01, with an active set of 63 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,411:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=3.603e+01, with an active set of 63 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,411:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=4.151e+01, with an active set of 66 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,411:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=4.151e+01, with an active set of 66 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,412:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=3.343e+01, with an active set of 65 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,413:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=3.955e+01, with an active set of 68 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,413:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=3.853e+01, with an active set of 68 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,413:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.004e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,413:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=3.121e+01, with an active set of 67 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,413:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=3.061e+01, with an active set of 67 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,414:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=9.819e+01, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,414:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=3.521e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,415:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=9.819e+01, with an active set of 45 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,415:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=2.438e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,416:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.785e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,416:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=3.109e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,416:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=2.081e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,416:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=2.968e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,416:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=1.979e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,417:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=2.865e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,417:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=1.979e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,417:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.529e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,417:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=2.738e+01, with an active set of 74 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,417:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=8.729e+01, with an active set of 50 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,417:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=1.889e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,418:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=2.576e+01, with an active set of 74 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,418:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.529e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,418:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=1.830e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,418:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=2.221e+01, with an active set of 74 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,418:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=1.906e+01, with an active set of 74 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,418:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=1.895e+01, with an active set of 74 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,419:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.859e+02, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,419:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=1.724e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,419:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=8.079e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,419:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=1.724e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,419:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=8.079e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,419:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=1.643e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,420:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=1.702e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,420:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=7.719e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,420:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=1.402e+01, with an active set of 75 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,420:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.686e+02, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,421:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=1.257e+02, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,421:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=6.963e+01, with an active set of 57 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,422:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=6.630e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,422:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.587e+02, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,422:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=1.587e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,422:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=1.587e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,422:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=1.322e+01, with an active set of 79 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,423:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=1.322e+01, with an active set of 79 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,423:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=5.804e+01, with an active set of 61 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,424:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=1.290e+01, with an active set of 79 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,424:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.215e+02, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,424:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.033e+02, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,424:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.215e+02, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,424:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 89 iterations, i.e. alpha=1.528e+01, with an active set of 82 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,424:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=1.208e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,424:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=1.157e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,425:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=5.416e+01, with an active set of 64 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,425:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.364e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,425:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=1.487e+01, with an active set of 83 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,425:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.364e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,425:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=9.362e+00, with an active set of 81 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,426:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.364e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,426:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.102e+02, with an active set of 40 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,426:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.364e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,426:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.102e+02, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,426:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 92 iterations, i.e. alpha=1.311e+01, with an active set of 84 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,426:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 92 iterations, i.e. alpha=1.311e+01, with an active set of 84 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,427:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=4.949e+01, with an active set of 68 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,427:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=4.820e+01, with an active set of 69 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,427:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=1.122e+01, with an active set of 86 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,428:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.018e+02, with an active set of 44 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,428:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 92 iterations, i.e. alpha=6.334e+00, with an active set of 86 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,428:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=4.648e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,429:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=9.942e+01, with an active set of 46 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,429:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=5.829e+00, with an active set of 87 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,429:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=7.885e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,429:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=7.885e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,429:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=4.513e+01, with an active set of 72 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,429:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=5.037e+00, with an active set of 87 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,430:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=9.702e+01, with an active set of 48 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,430:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=4.480e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,431:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=4.648e+00, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,431:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=4.648e+00, with an active set of 88 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,432:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=4.648e+00, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,432:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=4.529e+00, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,432:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=4.148e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,433:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=3.609e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,433:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=8.541e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,433:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=3.499e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,433:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=3.499e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,433:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=6.022e+01, with an active set of 64 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,433:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 98 iterations, i.e. alpha=4.552e+00, with an active set of 89 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,434:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=7.547e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,434:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=5.683e+01, with an active set of 64 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,435:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=5.151e+01, with an active set of 66 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,435:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=5.151e+01, with an active set of 66 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,435:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=3.307e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-06 12:36:17,436:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=1.155e+02, with an active set of 90 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,436:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=9.491e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,436:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=7.908e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,437:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=6.299e+01, with an active set of 61 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,437:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.598e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,437:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=7.507e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,437:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.572e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,437:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=4.778e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,437:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=7.488e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,437:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=4.778e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-06 12:36:17,437:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=5.929e+01, with an active set of 62 regressors, and the smallest cholesky pivot element being 8.752e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,437:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=7.471e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,438:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=7.387e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,438:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=5.833e+01, with an active set of 63 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-06 12:36:17,438:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.477e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,438:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=7.301e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,438:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.472e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,439:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.392e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,439:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=5.756e+01, with an active set of 64 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,439:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=7.277e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,439:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.330e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,439:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=7.220e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,439:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=7.206e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,440:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=5.620e+01, with an active set of 66 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-06 12:36:17,440:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=7.187e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,439:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=4.088e+01, with an active set of 74 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,440:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.310e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,440:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=7.174e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,440:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=5.604e+01, with an active set of 66 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,440:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=4.088e+01, with an active set of 74 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,440:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.308e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,440:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=7.147e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,441:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=4.088e+01, with an active set of 74 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,441:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.300e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,441:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=7.124e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,441:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=5.486e+01, with an active set of 67 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,441:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.299e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,441:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=7.038e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,441:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=3.788e+01, with an active set of 74 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,441:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.295e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,441:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=6.893e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,441:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=5.141e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,441:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:36:17,442:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.284e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,442:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=6.776e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,442:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=5.141e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,442:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.264e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,442:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=6.075e+05, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,442:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.262e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,442:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=2.963e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,442:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.240e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,443:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=5.242e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 8.752e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,443:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=2.864e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,443:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=4.141e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,443:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=2.864e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-06 12:36:17,443:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=5.017e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,444:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=2.714e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-06 12:36:17,444:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=4.992e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,444:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=2.693e+00, with an active set of 91 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,444:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=4.951e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,444:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=8.879e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,444:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=4.929e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,444:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.545e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,445:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=1.982e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,445:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=4.411e+01, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,445:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.374e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,445:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=4.403e+01, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,445:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.357e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,445:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=4.033e+01, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,445:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.340e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,446:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=1.868e+01, with an active set of 79 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,446:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=4.606e+01, with an active set of 74 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,446:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=4.005e+01, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,446:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.340e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,446:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=1.868e+01, with an active set of 79 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,446:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.323e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,446:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=3.580e+01, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,447:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.323e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,447:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=4.417e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,447:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=1.717e+01, with an active set of 81 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,447:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=4.245e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,447:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.306e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,447:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=1.544e+01, with an active set of 81 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,447:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.306e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,448:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=1.497e+01, with an active set of 81 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,448:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=4.186e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-06 12:36:17,448:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=3.384e+01, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,448:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=1.480e+01, with an active set of 81 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,448:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.289e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,448:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=3.330e+01, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,448:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.289e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,448:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=2.768e+01, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,449:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.272e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,449:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=2.764e+01, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,449:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.272e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,449:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=2.698e+01, with an active set of 96 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,449:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.255e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,449:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=2.420e+01, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,450:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.238e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,450:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.922e+01, with an active set of 96 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,450:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.238e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,450:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 91 iterations, i.e. alpha=1.183e+01, with an active set of 85 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,450:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 91 iterations, i.e. alpha=1.183e+01, with an active set of 85 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,451:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.434e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,451:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 91 iterations, i.e. alpha=1.109e+02, with an active set of 80 regressors, and the smallest cholesky pivot element being 8.752e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,451:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.134e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,451:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.076e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,451:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:36:17,451:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.565e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,452:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=7.693e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,452:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=7.003e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,452:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=6.873e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,453:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=6.817e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,453:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=6.813e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,453:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 99 iterations, i.e. alpha=7.045e+03, with an active set of 81 regressors, and the smallest cholesky pivot element being 9.246e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,453:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 100 iterations, i.e. alpha=2.768e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,454:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 99 iterations, i.e. alpha=3.467e+03, with an active set of 81 regressors, and the smallest cholesky pivot element being 8.752e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,454:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=6.765e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,454:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=6.655e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,454:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=6.512e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,455:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=3.391e+01, with an active set of 91 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,455:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=6.344e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,455:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=6.297e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,455:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=5.414e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,456:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=3.053e+03, with an active set of 85 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,456:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=5.194e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,456:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=5.158e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,456:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=3.052e+03, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,456:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=4.573e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,457:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=4.110e+02, with an active set of 93 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,457:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=4.544e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,457:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.269e+02, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,457:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=4.447e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,457:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.181e+02, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,457:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.257e+02, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,457:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=3.404e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,458:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.237e+02, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,458:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.231e+02, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,458:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=1.779e+04, with an active set of 87 regressors, and the smallest cholesky pivot element being 9.246e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,458:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.215e+02, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,458:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=7.222e+03, with an active set of 87 regressors, and the smallest cholesky pivot element being 8.752e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,458:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.200e+02, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,459:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=4.121e+03, with an active set of 87 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,459:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.179e+02, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,459:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.091e+02, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,459:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.176e+02, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,459:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.176e+02, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,460:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.167e+02, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,460:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=3.315e+03, with an active set of 89 regressors, and the smallest cholesky pivot element being 9.186e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,460:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.162e+02, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,460:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.152e+02, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,460:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.123e+02, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,461:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.110e+02, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,461:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=3.154e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,461:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.043e+02, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,461:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.036e+02, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,461:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=9.354e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,462:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=9.896e+01, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,462:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.665e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,462:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=9.255e+01, with an active set of 93 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,462:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.610e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,462:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=5.506e+01, with an active set of 93 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,462:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.591e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,462:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.566e+02, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,463:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.590e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,463:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.585e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,463:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.577e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,463:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=8.881e+01, with an active set of 56 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,464:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.573e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,464:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=8.881e+01, with an active set of 56 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,464:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.567e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,464:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.552e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,465:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.549e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,465:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.540e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,465:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.538e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,465:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.281e+02, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,465:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.515e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,466:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.499e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,466:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.459e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,466:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.340e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,467:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.183e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,467:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.162e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,467:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.161e+03, with an active set of 92 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,467:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=7.606e+01, with an active set of 63 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,468:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 115 iterations, i.e. alpha=1.084e+03, with an active set of 93 regressors, and the smallest cholesky pivot element being 9.186e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,470:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.063e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,470:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.033e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 9.186e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,470:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=1.047e+02, with an active set of 51 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,470:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=4.202e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,471:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=1.047e+02, with an active set of 51 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,471:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=3.497e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,471:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=6.700e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,471:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.104e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,471:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=1.047e+02, with an active set of 51 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,471:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.100e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,472:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.097e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,472:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.093e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,472:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.090e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,473:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.080e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,473:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.077e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,473:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.076e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,473:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.073e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,474:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=7.130e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,474:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.072e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,474:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.063e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,475:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.062e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,475:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.058e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,475:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.049e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,475:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=7.229e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,475:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.038e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,476:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.028e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,476:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=1.022e+02, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,476:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=9.542e+01, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,477:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=9.071e+01, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,478:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=1.278e+02, with an active set of 85 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,480:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=8.408e+01, with an active set of 87 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,480:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=8.066e+01, with an active set of 87 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,481:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=7.386e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,481:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=7.202e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,481:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=6.238e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,482:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=5.740e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,482:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=5.617e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,483:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=3.835e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,483:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=3.785e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,484:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=3.218e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,484:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=3.181e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,485:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=3.046e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,485:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=3.008e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,485:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=6.920e+08, with an active set of 75 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,485:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=2.788e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,486:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=2.530e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,486:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=2.449e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,486:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=1.333e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,487:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=1.246e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,487:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=1.186e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,487:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=6.803e+08, with an active set of 78 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,487:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=1.121e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,487:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=6.803e+08, with an active set of 78 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,487:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=1.057e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,488:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=6.802e+08, with an active set of 78 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,488:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=1.048e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,488:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=1.040e+01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,488:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=9.189e+00, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,489:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=6.558e+08, with an active set of 79 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,489:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=8.065e+00, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,489:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=5.099e+00, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,489:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=3.129e+00, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,489:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=6.536e+08, with an active set of 80 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,490:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=2.988e+00, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,490:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=2.725e+00, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,490:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=2.585e+00, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,490:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=4.500e-01, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,496:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 128 iterations, i.e. alpha=2.938e+10, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,497:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=2.370e+10, with an active set of 95 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,498:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=1.604e+10, with an active set of 95 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,498:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=1.009e+10, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,498:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=9.713e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,499:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=9.684e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,499:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=9.519e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,499:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=9.492e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,499:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=8.807e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,500:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=8.756e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,500:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=8.691e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,500:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=8.285e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,501:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=8.091e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,501:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=7.984e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,501:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=7.944e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,502:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=7.869e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,502:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=7.813e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,502:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=7.778e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,503:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=7.716e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,503:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=7.687e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,503:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=7.557e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,503:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=7.482e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,504:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=6.438e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,504:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=6.090e+09, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,565:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\utils\extmath.py:153: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2023-03-06 12:36:17,570:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 264, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\metrics\_regression.py", line 191, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\metrics\_regression.py", line 96, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 800, in check_array
    _assert_all_finite(array, allow_nan=force_all_finite == "allow-nan")
  File "C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 114, in _assert_all_finite
    raise ValueError(
ValueError: Input contains NaN, infinity or a value too large for dtype('float64').

  warnings.warn(

2023-03-06 12:36:17,576:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:36:17,580:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.756e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,580:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:36:17,580:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.668e+02, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,581:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.621e+02, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,582:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.204e+02, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,582:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.204e+02, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,584:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.718e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,584:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.718e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,584:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.774e+02, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,585:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=8.538e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,585:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=8.538e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,585:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.605e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,585:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=8.538e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,585:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.605e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,586:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=6.898e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,586:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=6.716e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,586:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.246e+02, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,587:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.034e+02, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,587:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=5.685e+01, with an active set of 62 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,587:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=4.400e+01, with an active set of 64 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,588:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=4.400e+01, with an active set of 64 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,588:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=8.069e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,588:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=3.994e+01, with an active set of 68 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,588:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=8.023e+01, with an active set of 48 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,589:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=3.994e+01, with an active set of 68 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,589:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=8.023e+01, with an active set of 48 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,589:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=4.052e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,589:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=7.701e+01, with an active set of 49 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,589:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=4.052e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,589:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=3.980e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,590:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=7.136e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,590:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=3.209e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,590:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=3.140e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,591:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=2.941e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,591:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=2.733e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,591:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=2.651e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,591:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.642e+01, with an active set of 62 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,592:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.642e+01, with an active set of 62 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,592:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.642e+01, with an active set of 62 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,592:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=2.164e+01, with an active set of 79 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,593:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=1.950e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,593:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=1.950e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,593:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=4.012e+01, with an active set of 65 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,593:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=3.834e+01, with an active set of 65 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,594:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=3.239e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,594:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=3.239e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,594:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=3.239e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,594:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=3.213e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,595:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 100 iterations, i.e. alpha=1.839e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,595:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=2.951e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,595:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 100 iterations, i.e. alpha=1.816e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,596:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=1.697e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,596:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=2.706e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,596:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=1.686e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,596:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=2.706e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,596:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=1.625e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,596:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=1.580e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,596:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=1.479e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,596:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=2.115e+01, with an active set of 78 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,596:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=1.439e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,596:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=1.005e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,597:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=1.005e+01, with an active set of 89 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,597:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=1.802e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,597:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=1.802e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,597:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=1.802e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,597:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=9.621e+00, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,597:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=9.496e+00, with an active set of 90 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,597:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=7.165e+00, with an active set of 90 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,598:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=7.165e+00, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,598:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=6.286e+00, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,598:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=6.197e+00, with an active set of 91 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,598:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=5.321e+00, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,598:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=5.321e+00, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,599:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=2.897e+00, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,599:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=2.781e+00, with an active set of 91 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,599:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=2.338e+00, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,599:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=1.754e+00, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,599:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=1.754e+00, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,599:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=9.981e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,600:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=9.981e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,600:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=9.981e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,600:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.780e+02, with an active set of 89 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,600:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=6.778e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,600:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=6.778e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,600:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=6.282e-01, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,600:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=3.770e+02, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,601:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=6.229e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,601:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=4.924e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,602:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=4.549e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,602:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.876e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,602:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.865e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,602:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.823e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,602:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.656e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,602:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.650e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,602:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.583e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,603:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.563e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,603:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.505e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,603:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.496e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,603:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.456e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,603:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.446e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,603:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.445e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,604:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.444e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,604:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.382e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,604:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.346e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,604:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.326e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,604:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.297e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,604:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.111e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,604:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.076e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,604:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=2.896e+03, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,635:INFO:Calculating mean and std
2023-03-06 12:36:17,637:INFO:Creating metrics dataframe
2023-03-06 12:36:17,641:INFO:Uploading results into container
2023-03-06 12:36:17,641:INFO:Uploading model into container now
2023-03-06 12:36:17,642:INFO:_master_model_container: 24
2023-03-06 12:36:17,642:INFO:_display_container: 3
2023-03-06 12:36:17,642:INFO:Lars(random_state=1822)
2023-03-06 12:36:17,642:INFO:create_model() successfully completed......................................
2023-03-06 12:36:17,733:INFO:SubProcess create_model() end ==================================
2023-03-06 12:36:17,733:INFO:Creating metrics dataframe
2023-03-06 12:36:17,742:INFO:Initializing Lasso Least Angle Regression
2023-03-06 12:36:17,742:INFO:Total runtime is 0.03461317221323649 minutes
2023-03-06 12:36:17,746:INFO:SubProcess create_model() called ==================================
2023-03-06 12:36:17,746:INFO:Initializing create_model()
2023-03-06 12:36:17,746:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636689BE0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:36:17,746:INFO:Checking exceptions
2023-03-06 12:36:17,747:INFO:Importing libraries
2023-03-06 12:36:17,747:INFO:Copying training dataset
2023-03-06 12:36:17,752:INFO:Defining folds
2023-03-06 12:36:17,752:INFO:Declaring metric variables
2023-03-06 12:36:17,757:INFO:Importing untrained model
2023-03-06 12:36:17,762:INFO:Lasso Least Angle Regression Imported successfully
2023-03-06 12:36:17,769:INFO:Starting cross validation
2023-03-06 12:36:17,770:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:36:17,858:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 12:36:17,870:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.321e+02, with an active set of 33 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,874:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=9.608e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,875:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=9.608e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,876:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 12:36:17,876:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=8.607e+01, with an active set of 46 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,880:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 12:36:17,884:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=5.332e+01, with an active set of 57 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,884:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=5.332e+01, with an active set of 57 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,885:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.796e+02, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,885:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=5.086e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,885:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=5.085e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,885:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.675e+02, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,886:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=5.009e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,886:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.646e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,886:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.635e+01, with an active set of 59 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,887:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.635e+01, with an active set of 59 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,887:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.635e+01, with an active set of 59 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,889:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 64 iterations, alpha=4.708e+01, previous alpha=4.515e+01, with an active set of 59 regressors.
  warnings.warn(

2023-03-06 12:36:17,890:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.171e+02, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,891:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.171e+02, with an active set of 39 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,892:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.034e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,893:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.034e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,894:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=9.835e+01, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,894:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=9.835e+01, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,897:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=7.725e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,897:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=7.725e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,898:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=7.352e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,899:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=6.447e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,900:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=6.447e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,900:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.970e+02, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,900:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=6.306e+01, with an active set of 59 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,901:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=6.306e+01, with an active set of 59 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,901:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=6.145e+01, with an active set of 60 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,902:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.646e+02, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,902:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=5.078e+01, with an active set of 62 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,903:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.627e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,904:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 64 iterations, alpha=6.249e+01, previous alpha=4.689e+01, with an active set of 63 regressors.
  warnings.warn(

2023-03-06 12:36:17,908:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.004e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,908:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 12:36:17,909:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=9.819e+01, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,909:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 12:36:17,910:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=9.819e+01, with an active set of 45 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,911:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.785e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,912:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=8.729e+01, with an active set of 50 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,914:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=8.079e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,914:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=8.079e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,915:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=7.719e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,916:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=6.963e+01, with an active set of 57 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,917:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=6.630e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,918:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.859e+02, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,918:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=5.804e+01, with an active set of 61 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,920:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=5.416e+01, with an active set of 64 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,920:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.529e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,920:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.686e+02, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,920:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.529e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,921:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.587e+02, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,921:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 67 iterations, alpha=5.379e+01, previous alpha=5.084e+01, with an active set of 66 regressors.
  warnings.warn(

2023-03-06 12:36:17,922:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.215e+02, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,923:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.215e+02, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,923:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=1.257e+02, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,925:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.102e+02, with an active set of 40 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,925:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.102e+02, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,926:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.033e+02, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,927:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.018e+02, with an active set of 44 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,927:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.364e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,927:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.364e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,928:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.364e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,928:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=9.942e+01, with an active set of 46 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,928:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.364e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,929:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=9.702e+01, with an active set of 48 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,930:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 12:36:17,932:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=9.023e+01, previous alpha=8.269e+01, with an active set of 53 regressors.
  warnings.warn(

2023-03-06 12:36:17,932:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=7.637e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,932:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=7.637e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,936:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 66 iterations, alpha=6.242e+01, previous alpha=6.235e+01, with an active set of 59 regressors.
  warnings.warn(

2023-03-06 12:36:17,939:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.741e+02, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,940:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.670e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,942:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.522e+02, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,942:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.522e+02, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,945:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.166e+02, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,945:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 12:36:17,946:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.149e+02, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,947:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.145e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,949:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=9.647e+01, with an active set of 49 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,949:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=9.647e+01, with an active set of 49 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,952:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=7.979e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,952:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=7.979e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,952:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=7.979e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,953:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=7.979e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,954:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=7.136e+01, with an active set of 56 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,955:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.565e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,955:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 59 iterations, alpha=7.735e+01, previous alpha=6.879e+01, with an active set of 58 regressors.
  warnings.warn(

2023-03-06 12:36:17,958:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 12:36:17,960:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.122e+02, with an active set of 40 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,962:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=9.691e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,965:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=8.842e+01, with an active set of 43 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,966:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=8.824e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,966:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=8.824e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,966:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=8.824e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,967:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=8.824e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,969:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.615e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,969:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=7.826e+01, with an active set of 50 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,969:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=7.826e+01, with an active set of 50 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,970:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.479e+02, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,972:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.218e+02, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,973:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 66 iterations, alpha=6.251e+01, previous alpha=6.164e+01, with an active set of 57 regressors.
  warnings.warn(

2023-03-06 12:36:17,973:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.105e+02, with an active set of 40 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,974:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.105e+02, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,975:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.032e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,975:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.032e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,978:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=8.096e+01, with an active set of 49 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,978:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=8.096e+01, with an active set of 49 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,979:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=8.096e+01, with an active set of 49 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,979:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=8.051e+01, with an active set of 49 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,983:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=5.525e+01, with an active set of 57 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,983:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=5.053e+01, with an active set of 59 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,983:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=4.738e+01, with an active set of 60 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,984:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=4.728e+01, with an active set of 60 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,985:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=3.749e+01, with an active set of 62 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,985:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=3.749e+01, with an active set of 62 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,986:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=3.474e+01, with an active set of 64 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,988:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=3.042e+01, with an active set of 67 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,989:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=2.687e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:17,990:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 75 iterations, alpha=2.901e+01, previous alpha=2.687e+01, with an active set of 70 regressors.
  warnings.warn(

2023-03-06 12:36:18,035:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 12:36:18,039:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.774e+02, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:18,040:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.605e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

odel = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 12:36:18,041:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.605e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:18,042:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.246e+02, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:18,043:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.034e+02, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:18,044:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=8.069e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:18,044:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=8.023e+01, with an active set of 48 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:18,044:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=8.023e+01, with an active set of 48 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:18,044:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=8.023e+01, with an active set of 48 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:18,045:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.756e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:18,045:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=7.701e+01, with an active set of 49 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:18,045:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.668e+02, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:18,045:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=7.136e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:18,045:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.621e+02, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:18,046:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=5.958e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:18,046:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.204e+02, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:18,047:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.204e+02, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:18,047:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 61 iterations, alpha=5.126e+01, previous alpha=4.932e+01, with an active set of 60 regressors.
  warnings.warn(

2023-03-06 12:36:18,048:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.718e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:18,048:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.718e+01, with an active set of 47 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:18,049:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=8.538e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:18,049:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=8.538e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:18,049:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=8.538e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 12:36:18,050:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:682: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=8.696e+01, previous alpha=8.294e+01, with an active set of 53 regressors.
  warnings.warn(

2023-03-06 12:36:18,079:INFO:Calculating mean and std
2023-03-06 12:36:18,081:INFO:Creating metrics dataframe
2023-03-06 12:36:18,084:INFO:Uploading results into container
2023-03-06 12:36:18,085:INFO:Uploading model into container now
2023-03-06 12:36:18,085:INFO:_master_model_container: 25
2023-03-06 12:36:18,085:INFO:_display_container: 3
2023-03-06 12:36:18,086:INFO:LassoLars(random_state=1822)
2023-03-06 12:36:18,086:INFO:create_model() successfully completed......................................
2023-03-06 12:36:18,178:INFO:SubProcess create_model() end ==================================
2023-03-06 12:36:18,178:INFO:Creating metrics dataframe
2023-03-06 12:36:18,187:INFO:Initializing Orthogonal Matching Pursuit
2023-03-06 12:36:18,187:INFO:Total runtime is 0.042029519875844315 minutes
2023-03-06 12:36:18,190:INFO:SubProcess create_model() called ==================================
2023-03-06 12:36:18,190:INFO:Initializing create_model()
2023-03-06 12:36:18,190:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636689BE0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:36:18,190:INFO:Checking exceptions
2023-03-06 12:36:18,191:INFO:Importing libraries
2023-03-06 12:36:18,191:INFO:Copying training dataset
2023-03-06 12:36:18,197:INFO:Defining folds
2023-03-06 12:36:18,197:INFO:Declaring metric variables
2023-03-06 12:36:18,201:INFO:Importing untrained model
2023-03-06 12:36:18,206:INFO:Orthogonal Matching Pursuit Imported successfully
2023-03-06 12:36:18,214:INFO:Starting cross validation
2023-03-06 12:36:18,215:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:36:18,294:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:36:18,304:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:36:18,317:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:36:18,329:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:36:18,337:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:36:18,351:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:36:18,367:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:36:18,378:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:36:18,427:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:36:18,430:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 12:36:18,459:INFO:Calculating mean and std
2023-03-06 12:36:18,460:INFO:Creating metrics dataframe
2023-03-06 12:36:18,463:INFO:Uploading results into container
2023-03-06 12:36:18,464:INFO:Uploading model into container now
2023-03-06 12:36:18,464:INFO:_master_model_container: 26
2023-03-06 12:36:18,464:INFO:_display_container: 3
2023-03-06 12:36:18,465:INFO:OrthogonalMatchingPursuit()
2023-03-06 12:36:18,465:INFO:create_model() successfully completed......................................
2023-03-06 12:36:18,547:INFO:SubProcess create_model() end ==================================
2023-03-06 12:36:18,547:INFO:Creating metrics dataframe
2023-03-06 12:36:18,556:INFO:Initializing Bayesian Ridge
2023-03-06 12:36:18,556:INFO:Total runtime is 0.0481750210126241 minutes
2023-03-06 12:36:18,561:INFO:SubProcess create_model() called ==================================
2023-03-06 12:36:18,561:INFO:Initializing create_model()
2023-03-06 12:36:18,562:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636689BE0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:36:18,562:INFO:Checking exceptions
2023-03-06 12:36:18,562:INFO:Importing libraries
2023-03-06 12:36:18,562:INFO:Copying training dataset
2023-03-06 12:36:18,569:INFO:Defining folds
2023-03-06 12:36:18,569:INFO:Declaring metric variables
2023-03-06 12:36:18,573:INFO:Importing untrained model
2023-03-06 12:36:18,578:INFO:Bayesian Ridge Imported successfully
2023-03-06 12:36:18,585:INFO:Starting cross validation
2023-03-06 12:36:18,588:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:36:18,873:INFO:Calculating mean and std
2023-03-06 12:36:18,874:INFO:Creating metrics dataframe
2023-03-06 12:36:18,877:INFO:Uploading results into container
2023-03-06 12:36:18,878:INFO:Uploading model into container now
2023-03-06 12:36:18,878:INFO:_master_model_container: 27
2023-03-06 12:36:18,879:INFO:_display_container: 3
2023-03-06 12:36:18,879:INFO:BayesianRidge()
2023-03-06 12:36:18,879:INFO:create_model() successfully completed......................................
2023-03-06 12:36:18,972:INFO:SubProcess create_model() end ==================================
2023-03-06 12:36:18,972:INFO:Creating metrics dataframe
2023-03-06 12:36:18,980:INFO:Initializing Passive Aggressive Regressor
2023-03-06 12:36:18,980:INFO:Total runtime is 0.05524005095163981 minutes
2023-03-06 12:36:18,983:INFO:SubProcess create_model() called ==================================
2023-03-06 12:36:18,984:INFO:Initializing create_model()
2023-03-06 12:36:18,984:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636689BE0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:36:18,984:INFO:Checking exceptions
2023-03-06 12:36:18,984:INFO:Importing libraries
2023-03-06 12:36:18,984:INFO:Copying training dataset
2023-03-06 12:36:18,990:INFO:Defining folds
2023-03-06 12:36:18,991:INFO:Declaring metric variables
2023-03-06 12:36:18,994:INFO:Importing untrained model
2023-03-06 12:36:18,999:INFO:Passive Aggressive Regressor Imported successfully
2023-03-06 12:36:19,009:INFO:Starting cross validation
2023-03-06 12:36:19,011:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:36:19,260:INFO:Calculating mean and std
2023-03-06 12:36:19,261:INFO:Creating metrics dataframe
2023-03-06 12:36:19,264:INFO:Uploading results into container
2023-03-06 12:36:19,264:INFO:Uploading model into container now
2023-03-06 12:36:19,265:INFO:_master_model_container: 28
2023-03-06 12:36:19,265:INFO:_display_container: 3
2023-03-06 12:36:19,265:INFO:PassiveAggressiveRegressor(random_state=1822)
2023-03-06 12:36:19,265:INFO:create_model() successfully completed......................................
2023-03-06 12:36:19,351:INFO:SubProcess create_model() end ==================================
2023-03-06 12:36:19,351:INFO:Creating metrics dataframe
2023-03-06 12:36:19,361:INFO:Initializing Huber Regressor
2023-03-06 12:36:19,361:INFO:Total runtime is 0.06158913373947143 minutes
2023-03-06 12:36:19,365:INFO:SubProcess create_model() called ==================================
2023-03-06 12:36:19,365:INFO:Initializing create_model()
2023-03-06 12:36:19,365:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636689BE0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:36:19,365:INFO:Checking exceptions
2023-03-06 12:36:19,365:INFO:Importing libraries
2023-03-06 12:36:19,366:INFO:Copying training dataset
2023-03-06 12:36:19,371:INFO:Defining folds
2023-03-06 12:36:19,372:INFO:Declaring metric variables
2023-03-06 12:36:19,376:INFO:Importing untrained model
2023-03-06 12:36:19,381:INFO:Huber Regressor Imported successfully
2023-03-06 12:36:19,388:INFO:Starting cross validation
2023-03-06 12:36:19,391:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:36:19,634:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-06 12:36:19,646:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-06 12:36:19,672:INFO:Calculating mean and std
2023-03-06 12:36:19,673:INFO:Creating metrics dataframe
2023-03-06 12:36:19,677:INFO:Uploading results into container
2023-03-06 12:36:19,678:INFO:Uploading model into container now
2023-03-06 12:36:19,678:INFO:_master_model_container: 29
2023-03-06 12:36:19,678:INFO:_display_container: 3
2023-03-06 12:36:19,679:INFO:HuberRegressor()
2023-03-06 12:36:19,679:INFO:create_model() successfully completed......................................
2023-03-06 12:36:19,762:INFO:SubProcess create_model() end ==================================
2023-03-06 12:36:19,762:INFO:Creating metrics dataframe
2023-03-06 12:36:19,772:INFO:Initializing K Neighbors Regressor
2023-03-06 12:36:19,772:INFO:Total runtime is 0.06843747695287068 minutes
2023-03-06 12:36:19,776:INFO:SubProcess create_model() called ==================================
2023-03-06 12:36:19,776:INFO:Initializing create_model()
2023-03-06 12:36:19,776:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636689BE0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:36:19,777:INFO:Checking exceptions
2023-03-06 12:36:19,777:INFO:Importing libraries
2023-03-06 12:36:19,777:INFO:Copying training dataset
2023-03-06 12:36:19,783:INFO:Defining folds
2023-03-06 12:36:19,783:INFO:Declaring metric variables
2023-03-06 12:36:19,786:INFO:Importing untrained model
2023-03-06 12:36:19,790:INFO:K Neighbors Regressor Imported successfully
2023-03-06 12:36:19,798:INFO:Starting cross validation
2023-03-06 12:36:19,799:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:36:20,132:INFO:Calculating mean and std
2023-03-06 12:36:20,133:INFO:Creating metrics dataframe
2023-03-06 12:36:20,138:INFO:Uploading results into container
2023-03-06 12:36:20,138:INFO:Uploading model into container now
2023-03-06 12:36:20,139:INFO:_master_model_container: 30
2023-03-06 12:36:20,139:INFO:_display_container: 3
2023-03-06 12:36:20,139:INFO:KNeighborsRegressor(n_jobs=-1)
2023-03-06 12:36:20,139:INFO:create_model() successfully completed......................................
2023-03-06 12:36:20,237:INFO:SubProcess create_model() end ==================================
2023-03-06 12:36:20,238:INFO:Creating metrics dataframe
2023-03-06 12:36:20,248:INFO:Initializing Decision Tree Regressor
2023-03-06 12:36:20,248:INFO:Total runtime is 0.07637500365575155 minutes
2023-03-06 12:36:20,252:INFO:SubProcess create_model() called ==================================
2023-03-06 12:36:20,252:INFO:Initializing create_model()
2023-03-06 12:36:20,252:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636689BE0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:36:20,253:INFO:Checking exceptions
2023-03-06 12:36:20,253:INFO:Importing libraries
2023-03-06 12:36:20,253:INFO:Copying training dataset
2023-03-06 12:36:20,258:INFO:Defining folds
2023-03-06 12:36:20,258:INFO:Declaring metric variables
2023-03-06 12:36:20,262:INFO:Importing untrained model
2023-03-06 12:36:20,266:INFO:Decision Tree Regressor Imported successfully
2023-03-06 12:36:20,274:INFO:Starting cross validation
2023-03-06 12:36:20,275:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:36:20,528:INFO:Calculating mean and std
2023-03-06 12:36:20,530:INFO:Creating metrics dataframe
2023-03-06 12:36:20,533:INFO:Uploading results into container
2023-03-06 12:36:20,534:INFO:Uploading model into container now
2023-03-06 12:36:20,534:INFO:_master_model_container: 31
2023-03-06 12:36:20,534:INFO:_display_container: 3
2023-03-06 12:36:20,535:INFO:DecisionTreeRegressor(random_state=1822)
2023-03-06 12:36:20,535:INFO:create_model() successfully completed......................................
2023-03-06 12:36:20,629:INFO:SubProcess create_model() end ==================================
2023-03-06 12:36:20,629:INFO:Creating metrics dataframe
2023-03-06 12:36:20,638:INFO:Initializing Random Forest Regressor
2023-03-06 12:36:20,638:INFO:Total runtime is 0.08287483056386312 minutes
2023-03-06 12:36:20,641:INFO:SubProcess create_model() called ==================================
2023-03-06 12:36:20,642:INFO:Initializing create_model()
2023-03-06 12:36:20,642:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636689BE0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:36:20,642:INFO:Checking exceptions
2023-03-06 12:36:20,643:INFO:Importing libraries
2023-03-06 12:36:20,643:INFO:Copying training dataset
2023-03-06 12:36:20,649:INFO:Defining folds
2023-03-06 12:36:20,649:INFO:Declaring metric variables
2023-03-06 12:36:20,653:INFO:Importing untrained model
2023-03-06 12:36:20,657:INFO:Random Forest Regressor Imported successfully
2023-03-06 12:36:20,664:INFO:Starting cross validation
2023-03-06 12:36:20,666:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:36:21,209:INFO:Calculating mean and std
2023-03-06 12:36:21,210:INFO:Creating metrics dataframe
2023-03-06 12:36:21,213:INFO:Uploading results into container
2023-03-06 12:36:21,213:INFO:Uploading model into container now
2023-03-06 12:36:21,214:INFO:_master_model_container: 32
2023-03-06 12:36:21,214:INFO:_display_container: 3
2023-03-06 12:36:21,214:INFO:RandomForestRegressor(n_jobs=-1, random_state=1822)
2023-03-06 12:36:21,214:INFO:create_model() successfully completed......................................
2023-03-06 12:36:21,316:INFO:SubProcess create_model() end ==================================
2023-03-06 12:36:21,316:INFO:Creating metrics dataframe
2023-03-06 12:36:21,328:INFO:Initializing Extra Trees Regressor
2023-03-06 12:36:21,328:INFO:Total runtime is 0.09438176949818929 minutes
2023-03-06 12:36:21,331:INFO:SubProcess create_model() called ==================================
2023-03-06 12:36:21,332:INFO:Initializing create_model()
2023-03-06 12:36:21,332:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636689BE0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:36:21,332:INFO:Checking exceptions
2023-03-06 12:36:21,332:INFO:Importing libraries
2023-03-06 12:36:21,332:INFO:Copying training dataset
2023-03-06 12:36:21,339:INFO:Defining folds
2023-03-06 12:36:21,339:INFO:Declaring metric variables
2023-03-06 12:36:21,343:INFO:Importing untrained model
2023-03-06 12:36:21,348:INFO:Extra Trees Regressor Imported successfully
2023-03-06 12:36:21,358:INFO:Starting cross validation
2023-03-06 12:36:21,361:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:36:21,891:INFO:Calculating mean and std
2023-03-06 12:36:21,892:INFO:Creating metrics dataframe
2023-03-06 12:36:21,895:INFO:Uploading results into container
2023-03-06 12:36:21,895:INFO:Uploading model into container now
2023-03-06 12:36:21,895:INFO:_master_model_container: 33
2023-03-06 12:36:21,896:INFO:_display_container: 3
2023-03-06 12:36:21,896:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=1822)
2023-03-06 12:36:21,896:INFO:create_model() successfully completed......................................
2023-03-06 12:36:21,988:INFO:SubProcess create_model() end ==================================
2023-03-06 12:36:21,988:INFO:Creating metrics dataframe
2023-03-06 12:36:21,998:INFO:Initializing AdaBoost Regressor
2023-03-06 12:36:21,998:INFO:Total runtime is 0.10555081764856974 minutes
2023-03-06 12:36:22,001:INFO:SubProcess create_model() called ==================================
2023-03-06 12:36:22,002:INFO:Initializing create_model()
2023-03-06 12:36:22,002:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636689BE0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:36:22,002:INFO:Checking exceptions
2023-03-06 12:36:22,002:INFO:Importing libraries
2023-03-06 12:36:22,002:INFO:Copying training dataset
2023-03-06 12:36:22,009:INFO:Defining folds
2023-03-06 12:36:22,009:INFO:Declaring metric variables
2023-03-06 12:36:22,013:INFO:Importing untrained model
2023-03-06 12:36:22,018:INFO:AdaBoost Regressor Imported successfully
2023-03-06 12:36:22,028:INFO:Starting cross validation
2023-03-06 12:36:22,029:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:36:22,476:INFO:Calculating mean and std
2023-03-06 12:36:22,477:INFO:Creating metrics dataframe
2023-03-06 12:36:22,481:INFO:Uploading results into container
2023-03-06 12:36:22,481:INFO:Uploading model into container now
2023-03-06 12:36:22,482:INFO:_master_model_container: 34
2023-03-06 12:36:22,482:INFO:_display_container: 3
2023-03-06 12:36:22,483:INFO:AdaBoostRegressor(random_state=1822)
2023-03-06 12:36:22,483:INFO:create_model() successfully completed......................................
2023-03-06 12:36:22,572:INFO:SubProcess create_model() end ==================================
2023-03-06 12:36:22,572:INFO:Creating metrics dataframe
2023-03-06 12:36:22,582:INFO:Initializing Gradient Boosting Regressor
2023-03-06 12:36:22,582:INFO:Total runtime is 0.11526957750320435 minutes
2023-03-06 12:36:22,586:INFO:SubProcess create_model() called ==================================
2023-03-06 12:36:22,586:INFO:Initializing create_model()
2023-03-06 12:36:22,586:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636689BE0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:36:22,587:INFO:Checking exceptions
2023-03-06 12:36:22,587:INFO:Importing libraries
2023-03-06 12:36:22,587:INFO:Copying training dataset
2023-03-06 12:36:22,592:INFO:Defining folds
2023-03-06 12:36:22,592:INFO:Declaring metric variables
2023-03-06 12:36:22,596:INFO:Importing untrained model
2023-03-06 12:36:22,600:INFO:Gradient Boosting Regressor Imported successfully
2023-03-06 12:36:22,610:INFO:Starting cross validation
2023-03-06 12:36:22,611:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:36:23,064:INFO:Calculating mean and std
2023-03-06 12:36:23,065:INFO:Creating metrics dataframe
2023-03-06 12:36:23,069:INFO:Uploading results into container
2023-03-06 12:36:23,070:INFO:Uploading model into container now
2023-03-06 12:36:23,070:INFO:_master_model_container: 35
2023-03-06 12:36:23,070:INFO:_display_container: 3
2023-03-06 12:36:23,071:INFO:GradientBoostingRegressor(random_state=1822)
2023-03-06 12:36:23,071:INFO:create_model() successfully completed......................................
2023-03-06 12:36:23,161:INFO:SubProcess create_model() end ==================================
2023-03-06 12:36:23,161:INFO:Creating metrics dataframe
2023-03-06 12:36:23,172:INFO:Initializing Extreme Gradient Boosting
2023-03-06 12:36:23,172:INFO:Total runtime is 0.1251073122024536 minutes
2023-03-06 12:36:23,175:INFO:SubProcess create_model() called ==================================
2023-03-06 12:36:23,175:INFO:Initializing create_model()
2023-03-06 12:36:23,175:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636689BE0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:36:23,175:INFO:Checking exceptions
2023-03-06 12:36:23,175:INFO:Importing libraries
2023-03-06 12:36:23,175:INFO:Copying training dataset
2023-03-06 12:36:23,181:INFO:Defining folds
2023-03-06 12:36:23,182:INFO:Declaring metric variables
2023-03-06 12:36:23,185:INFO:Importing untrained model
2023-03-06 12:36:23,190:INFO:Extreme Gradient Boosting Imported successfully
2023-03-06 12:36:23,198:INFO:Starting cross validation
2023-03-06 12:36:23,199:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:36:23,475:INFO:Calculating mean and std
2023-03-06 12:36:23,476:INFO:Creating metrics dataframe
2023-03-06 12:36:23,480:INFO:Uploading results into container
2023-03-06 12:36:23,480:INFO:Uploading model into container now
2023-03-06 12:36:23,481:INFO:_master_model_container: 36
2023-03-06 12:36:23,481:INFO:_display_container: 3
2023-03-06 12:36:23,482:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=1822, ...)
2023-03-06 12:36:23,482:INFO:create_model() successfully completed......................................
2023-03-06 12:36:23,572:INFO:SubProcess create_model() end ==================================
2023-03-06 12:36:23,572:INFO:Creating metrics dataframe
2023-03-06 12:36:23,583:INFO:Initializing Light Gradient Boosting Machine
2023-03-06 12:36:23,583:INFO:Total runtime is 0.1319555600484212 minutes
2023-03-06 12:36:23,587:INFO:SubProcess create_model() called ==================================
2023-03-06 12:36:23,588:INFO:Initializing create_model()
2023-03-06 12:36:23,588:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636689BE0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:36:23,588:INFO:Checking exceptions
2023-03-06 12:36:23,588:INFO:Importing libraries
2023-03-06 12:36:23,588:INFO:Copying training dataset
2023-03-06 12:36:23,594:INFO:Defining folds
2023-03-06 12:36:23,594:INFO:Declaring metric variables
2023-03-06 12:36:23,598:INFO:Importing untrained model
2023-03-06 12:36:23,603:INFO:Light Gradient Boosting Machine Imported successfully
2023-03-06 12:36:23,612:INFO:Starting cross validation
2023-03-06 12:36:23,613:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:36:23,910:INFO:Calculating mean and std
2023-03-06 12:36:23,911:INFO:Creating metrics dataframe
2023-03-06 12:36:23,914:INFO:Uploading results into container
2023-03-06 12:36:23,914:INFO:Uploading model into container now
2023-03-06 12:36:23,914:INFO:_master_model_container: 37
2023-03-06 12:36:23,915:INFO:_display_container: 3
2023-03-06 12:36:23,915:INFO:LGBMRegressor(random_state=1822)
2023-03-06 12:36:23,915:INFO:create_model() successfully completed......................................
2023-03-06 12:36:23,998:INFO:SubProcess create_model() end ==================================
2023-03-06 12:36:23,998:INFO:Creating metrics dataframe
2023-03-06 12:36:24,009:INFO:Initializing Dummy Regressor
2023-03-06 12:36:24,009:INFO:Total runtime is 0.13905307054519653 minutes
2023-03-06 12:36:24,013:INFO:SubProcess create_model() called ==================================
2023-03-06 12:36:24,014:INFO:Initializing create_model()
2023-03-06 12:36:24,014:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026636689BE0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:36:24,014:INFO:Checking exceptions
2023-03-06 12:36:24,014:INFO:Importing libraries
2023-03-06 12:36:24,014:INFO:Copying training dataset
2023-03-06 12:36:24,022:INFO:Defining folds
2023-03-06 12:36:24,023:INFO:Declaring metric variables
2023-03-06 12:36:24,026:INFO:Importing untrained model
2023-03-06 12:36:24,029:INFO:Dummy Regressor Imported successfully
2023-03-06 12:36:24,041:INFO:Starting cross validation
2023-03-06 12:36:24,043:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 12:36:24,282:INFO:Calculating mean and std
2023-03-06 12:36:24,284:INFO:Creating metrics dataframe
2023-03-06 12:36:24,286:INFO:Uploading results into container
2023-03-06 12:36:24,287:INFO:Uploading model into container now
2023-03-06 12:36:24,287:INFO:_master_model_container: 38
2023-03-06 12:36:24,287:INFO:_display_container: 3
2023-03-06 12:36:24,287:INFO:DummyRegressor()
2023-03-06 12:36:24,287:INFO:create_model() successfully completed......................................
2023-03-06 12:36:24,370:INFO:SubProcess create_model() end ==================================
2023-03-06 12:36:24,370:INFO:Creating metrics dataframe
2023-03-06 12:36:24,393:INFO:Initializing create_model()
2023-03-06 12:36:24,393:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=GradientBoostingRegressor(random_state=1822), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-06 12:36:24,393:INFO:Checking exceptions
2023-03-06 12:36:24,395:INFO:Importing libraries
2023-03-06 12:36:24,395:INFO:Copying training dataset
2023-03-06 12:36:24,400:INFO:Defining folds
2023-03-06 12:36:24,401:INFO:Declaring metric variables
2023-03-06 12:36:24,401:INFO:Importing untrained model
2023-03-06 12:36:24,401:INFO:Declaring custom model
2023-03-06 12:36:24,402:INFO:Gradient Boosting Regressor Imported successfully
2023-03-06 12:36:24,403:INFO:Cross validation set to False
2023-03-06 12:36:24,403:INFO:Fitting Model
2023-03-06 12:36:24,506:INFO:GradientBoostingRegressor(random_state=1822)
2023-03-06 12:36:24,506:INFO:create_model() successfully completed......................................
2023-03-06 12:36:24,622:INFO:_master_model_container: 38
2023-03-06 12:36:24,623:INFO:_display_container: 3
2023-03-06 12:36:24,623:INFO:GradientBoostingRegressor(random_state=1822)
2023-03-06 12:36:24,623:INFO:compare_models() successfully completed......................................
2023-03-06 12:36:24,636:INFO:Initializing evaluate_model()
2023-03-06 12:36:24,636:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=GradientBoostingRegressor(random_state=1822), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-03-06 12:36:24,660:INFO:Initializing plot_model()
2023-03-06 12:36:24,661:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=1822), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, system=True)
2023-03-06 12:36:24,661:INFO:Checking exceptions
2023-03-06 12:36:24,664:INFO:Preloading libraries
2023-03-06 12:36:24,673:INFO:Copying training dataset
2023-03-06 12:36:24,674:INFO:Plot type: pipeline
2023-03-06 12:36:24,738:INFO:Visual Rendered Successfully
2023-03-06 12:36:24,828:INFO:plot_model() successfully completed......................................
2023-03-06 12:36:24,837:INFO:Initializing predict_model()
2023-03-06 12:36:24,837:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000266316F09D0>, estimator=GradientBoostingRegressor(random_state=1822), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x0000026636759D30>)
2023-03-06 12:36:24,838:INFO:Checking exceptions
2023-03-06 12:36:24,838:INFO:Preloading libraries
2023-03-06 12:36:24,840:INFO:Set up data.
2023-03-06 12:36:24,862:INFO:Set up index.
2023-03-06 18:44:32,934:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-06 18:44:32,934:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-06 18:44:32,935:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-06 18:44:32,935:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-06 18:44:34,401:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-03-06 18:44:35,111:INFO:PyCaret RegressionExperiment
2023-03-06 18:44:35,112:INFO:Logging name: reg-default-name
2023-03-06 18:44:35,112:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-06 18:44:35,112:INFO:version 3.0.0.rc9
2023-03-06 18:44:35,112:INFO:Initializing setup()
2023-03-06 18:44:35,112:INFO:self.USI: 074f
2023-03-06 18:44:35,112:INFO:self._variable_keys: {'_available_plots', 'fold_groups_param', 'fold_generator', 'pipeline', 'USI', 'logging_param', 'X', '_ml_usecase', 'idx', 'exp_id', 'fold_shuffle_param', 'X_train', 'exp_name_log', 'y_test', 'transform_target_param', 'y_train', 'seed', 'data', 'log_plots_param', 'gpu_n_jobs_param', 'target_param', 'n_jobs_param', 'X_test', 'memory', 'y', 'html_param', 'gpu_param'}
2023-03-06 18:44:35,113:INFO:Checking environment
2023-03-06 18:44:35,113:INFO:python_version: 3.9.13
2023-03-06 18:44:35,113:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-03-06 18:44:35,113:INFO:machine: AMD64
2023-03-06 18:44:35,113:INFO:platform: Windows-10-10.0.19044-SP0
2023-03-06 18:44:35,113:INFO:Memory: svmem(total=17009516544, available=5877596160, percent=65.4, used=11131920384, free=5877596160)
2023-03-06 18:44:35,113:INFO:Physical Core: 4
2023-03-06 18:44:35,114:INFO:Logical Core: 8
2023-03-06 18:44:35,114:INFO:Checking libraries
2023-03-06 18:44:35,114:INFO:System:
2023-03-06 18:44:35,114:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-03-06 18:44:35,114:INFO:executable: C:\Users\Alvaro\anaconda3\python.exe
2023-03-06 18:44:35,114:INFO:   machine: Windows-10-10.0.19044-SP0
2023-03-06 18:44:35,114:INFO:PyCaret required dependencies:
2023-03-06 18:44:35,115:INFO:                 pip: 22.2.2
2023-03-06 18:44:35,115:INFO:          setuptools: 63.4.1
2023-03-06 18:44:35,115:INFO:             pycaret: 3.0.0rc9
2023-03-06 18:44:35,115:INFO:             IPython: 7.31.1
2023-03-06 18:44:35,115:INFO:          ipywidgets: 7.6.5
2023-03-06 18:44:35,115:INFO:                tqdm: 4.64.1
2023-03-06 18:44:35,115:INFO:               numpy: 1.21.5
2023-03-06 18:44:35,116:INFO:              pandas: 1.4.4
2023-03-06 18:44:35,116:INFO:              jinja2: 2.11.3
2023-03-06 18:44:35,116:INFO:               scipy: 1.9.1
2023-03-06 18:44:35,116:INFO:              joblib: 1.2.0
2023-03-06 18:44:35,116:INFO:             sklearn: 1.0.2
2023-03-06 18:44:35,116:INFO:                pyod: 1.0.7
2023-03-06 18:44:35,116:INFO:            imblearn: 0.10.1
2023-03-06 18:44:35,116:INFO:   category_encoders: 2.6.0
2023-03-06 18:44:35,117:INFO:            lightgbm: 3.3.5
2023-03-06 18:44:35,117:INFO:               numba: 0.55.1
2023-03-06 18:44:35,117:INFO:            requests: 2.28.1
2023-03-06 18:44:35,117:INFO:          matplotlib: 3.5.2
2023-03-06 18:44:35,117:INFO:          scikitplot: 0.3.7
2023-03-06 18:44:35,117:INFO:         yellowbrick: 1.5
2023-03-06 18:44:35,117:INFO:              plotly: 5.9.0
2023-03-06 18:44:35,117:INFO:             kaleido: 0.2.1
2023-03-06 18:44:35,118:INFO:         statsmodels: 0.13.2
2023-03-06 18:44:35,118:INFO:              sktime: 0.16.1
2023-03-06 18:44:35,118:INFO:               tbats: 1.1.2
2023-03-06 18:44:35,118:INFO:            pmdarima: 2.0.2
2023-03-06 18:44:35,118:INFO:              psutil: 5.9.0
2023-03-06 18:44:35,118:INFO:PyCaret optional dependencies:
2023-03-06 18:44:35,174:INFO:                shap: Not installed
2023-03-06 18:44:35,175:INFO:           interpret: Not installed
2023-03-06 18:44:35,175:INFO:                umap: Not installed
2023-03-06 18:44:35,175:INFO:    pandas_profiling: Not installed
2023-03-06 18:44:35,175:INFO:  explainerdashboard: Not installed
2023-03-06 18:44:35,175:INFO:             autoviz: Not installed
2023-03-06 18:44:35,175:INFO:           fairlearn: Not installed
2023-03-06 18:44:35,175:INFO:             xgboost: 1.7.4
2023-03-06 18:44:35,175:INFO:            catboost: Not installed
2023-03-06 18:44:35,176:INFO:              kmodes: Not installed
2023-03-06 18:44:35,176:INFO:             mlxtend: Not installed
2023-03-06 18:44:35,176:INFO:       statsforecast: Not installed
2023-03-06 18:44:35,176:INFO:        tune_sklearn: Not installed
2023-03-06 18:44:35,176:INFO:                 ray: Not installed
2023-03-06 18:44:35,176:INFO:            hyperopt: Not installed
2023-03-06 18:44:35,176:INFO:              optuna: Not installed
2023-03-06 18:44:35,176:INFO:               skopt: Not installed
2023-03-06 18:44:35,176:INFO:              mlflow: Not installed
2023-03-06 18:44:35,177:INFO:              gradio: Not installed
2023-03-06 18:44:35,177:INFO:             fastapi: Not installed
2023-03-06 18:44:35,177:INFO:             uvicorn: Not installed
2023-03-06 18:44:35,177:INFO:              m2cgen: Not installed
2023-03-06 18:44:35,177:INFO:           evidently: Not installed
2023-03-06 18:44:35,177:INFO:               fugue: Not installed
2023-03-06 18:44:35,177:INFO:           streamlit: Not installed
2023-03-06 18:44:35,177:INFO:             prophet: Not installed
2023-03-06 18:44:35,178:INFO:None
2023-03-06 18:44:35,178:INFO:Set up data.
2023-03-06 18:44:35,194:INFO:Set up train/test split.
2023-03-06 18:44:35,208:INFO:Set up index.
2023-03-06 18:44:35,208:INFO:Set up folding strategy.
2023-03-06 18:44:35,209:INFO:Assigning column types.
2023-03-06 18:44:35,216:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-06 18:44:35,217:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-06 18:44:35,228:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-06 18:44:35,241:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-06 18:44:35,391:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:44:35,510:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 18:44:35,695:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:44:35,795:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:44:35,796:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-06 18:44:35,808:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-06 18:44:35,820:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-06 18:44:35,969:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:44:36,093:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 18:44:36,094:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:44:36,102:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:44:36,102:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-03-06 18:44:36,114:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-06 18:44:36,126:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-06 18:44:36,271:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:44:36,387:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 18:44:36,388:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:44:36,395:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:44:36,408:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-06 18:44:36,420:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-06 18:44:36,567:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:44:36,681:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 18:44:36,682:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:44:36,688:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:44:36,692:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-03-06 18:44:36,717:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-06 18:44:36,870:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:44:36,986:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 18:44:36,988:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:44:36,994:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:44:37,021:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-06 18:44:37,169:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:44:37,283:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 18:44:37,284:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:44:37,291:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:44:37,292:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-03-06 18:44:37,465:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:44:37,579:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 18:44:37,581:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:44:37,587:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:44:37,767:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:44:37,881:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 18:44:37,883:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:44:37,889:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:44:37,890:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-06 18:44:38,064:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:44:38,180:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:44:38,187:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:44:38,361:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:44:38,482:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:44:38,488:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:44:38,489:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-03-06 18:44:38,789:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:44:38,796:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:44:39,095:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:44:39,103:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:44:39,107:INFO:Preparing preprocessing pipeline...
2023-03-06 18:44:39,109:INFO:Set up simple imputation.
2023-03-06 18:44:39,124:INFO:Set up encoding of categorical features.
2023-03-06 18:44:39,377:INFO:Finished creating preprocessing pipeline.
2023-03-06 18:44:39,401:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Alvaro\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['work_year', 'remote_ratio'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['experience_level',
                                             'employment_type', 'job_title',
                                             'employee_residence',
                                             'company_location',
                                             'company_size'],
                                    t...
                                    transformer=OneHotEncoder(cols=['experience_level',
                                                                    'employment_type',
                                                                    'company_size'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['job_title', 'employee_residence',
                                             'company_location'],
                                    transformer=LeaveOneOutEncoder(cols=['job_title',
                                                                         'employee_residence',
                                                                         'company_location'],
                                                                   handle_missing='return_nan',
                                                                   random_state=2359)))])
2023-03-06 18:44:39,402:INFO:Creating final display dataframe.
2023-03-06 18:44:40,329:INFO:Setup _display_container:                     Description             Value
0                    Session id              2359
1                        Target     salary_in_usd
2                   Target type        Regression
3           Original data shape          (500, 9)
4        Transformed data shape         (500, 17)
5   Transformed train set shape         (350, 17)
6    Transformed test set shape         (150, 17)
7              Numeric features                 2
8          Categorical features                 6
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator             KFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  reg-default-name
21                          USI              074f
2023-03-06 18:44:40,657:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:44:40,664:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:44:40,949:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:44:40,956:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:44:40,958:INFO:setup() successfully completed in 5.88s...............
2023-03-06 18:44:40,976:INFO:Initializing compare_models()
2023-03-06 18:44:40,976:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055911A6A0>, include=None, fold=None, round=4, cross_validation=True, sort=RMSE, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000002055911A6A0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'RMSE', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-03-06 18:44:40,976:INFO:Checking exceptions
2023-03-06 18:44:40,982:INFO:Preparing display monitor
2023-03-06 18:44:41,082:INFO:Initializing Linear Regression
2023-03-06 18:44:41,082:INFO:Total runtime is 0.0 minutes
2023-03-06 18:44:41,093:INFO:SubProcess create_model() called ==================================
2023-03-06 18:44:41,094:INFO:Initializing create_model()
2023-03-06 18:44:41,095:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055911A6A0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055911A130>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:44:41,095:INFO:Checking exceptions
2023-03-06 18:44:41,095:INFO:Importing libraries
2023-03-06 18:44:41,095:INFO:Copying training dataset
2023-03-06 18:44:41,107:INFO:Defining folds
2023-03-06 18:44:41,108:INFO:Declaring metric variables
2023-03-06 18:44:41,114:INFO:Importing untrained model
2023-03-06 18:44:41,122:INFO:Linear Regression Imported successfully
2023-03-06 18:44:41,142:INFO:Starting cross validation
2023-03-06 18:44:41,158:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:44:54,440:INFO:Calculating mean and std
2023-03-06 18:44:54,443:INFO:Creating metrics dataframe
2023-03-06 18:44:54,451:INFO:Uploading results into container
2023-03-06 18:44:54,453:INFO:Uploading model into container now
2023-03-06 18:44:54,454:INFO:_master_model_container: 1
2023-03-06 18:44:54,454:INFO:_display_container: 2
2023-03-06 18:44:54,455:INFO:LinearRegression(n_jobs=-1)
2023-03-06 18:44:54,455:INFO:create_model() successfully completed......................................
2023-03-06 18:44:54,649:INFO:SubProcess create_model() end ==================================
2023-03-06 18:44:54,649:INFO:Creating metrics dataframe
2023-03-06 18:44:54,670:INFO:Initializing Lasso Regression
2023-03-06 18:44:54,671:INFO:Total runtime is 0.22648497025171915 minutes
2023-03-06 18:44:54,678:INFO:SubProcess create_model() called ==================================
2023-03-06 18:44:54,679:INFO:Initializing create_model()
2023-03-06 18:44:54,679:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055911A6A0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055911A130>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:44:54,679:INFO:Checking exceptions
2023-03-06 18:44:54,680:INFO:Importing libraries
2023-03-06 18:44:54,680:INFO:Copying training dataset
2023-03-06 18:44:54,694:INFO:Defining folds
2023-03-06 18:44:54,695:INFO:Declaring metric variables
2023-03-06 18:44:54,707:INFO:Importing untrained model
2023-03-06 18:44:54,719:INFO:Lasso Regression Imported successfully
2023-03-06 18:44:54,744:INFO:Starting cross validation
2023-03-06 18:44:54,750:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:44:56,691:INFO:Calculating mean and std
2023-03-06 18:44:56,695:INFO:Creating metrics dataframe
2023-03-06 18:44:56,703:INFO:Uploading results into container
2023-03-06 18:44:56,704:INFO:Uploading model into container now
2023-03-06 18:44:56,705:INFO:_master_model_container: 2
2023-03-06 18:44:56,705:INFO:_display_container: 2
2023-03-06 18:44:56,706:INFO:Lasso(random_state=2359)
2023-03-06 18:44:56,706:INFO:create_model() successfully completed......................................
2023-03-06 18:44:56,913:INFO:SubProcess create_model() end ==================================
2023-03-06 18:44:56,914:INFO:Creating metrics dataframe
2023-03-06 18:44:56,939:INFO:Initializing Ridge Regression
2023-03-06 18:44:56,939:INFO:Total runtime is 0.2642921209335327 minutes
2023-03-06 18:44:56,949:INFO:SubProcess create_model() called ==================================
2023-03-06 18:44:56,949:INFO:Initializing create_model()
2023-03-06 18:44:56,950:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055911A6A0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055911A130>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:44:56,951:INFO:Checking exceptions
2023-03-06 18:44:56,951:INFO:Importing libraries
2023-03-06 18:44:56,951:INFO:Copying training dataset
2023-03-06 18:44:56,965:INFO:Defining folds
2023-03-06 18:44:56,966:INFO:Declaring metric variables
2023-03-06 18:44:56,976:INFO:Importing untrained model
2023-03-06 18:44:56,986:INFO:Ridge Regression Imported successfully
2023-03-06 18:44:57,000:INFO:Starting cross validation
2023-03-06 18:44:57,005:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:44:58,878:INFO:Calculating mean and std
2023-03-06 18:44:58,882:INFO:Creating metrics dataframe
2023-03-06 18:44:58,890:INFO:Uploading results into container
2023-03-06 18:44:58,891:INFO:Uploading model into container now
2023-03-06 18:44:58,892:INFO:_master_model_container: 3
2023-03-06 18:44:58,893:INFO:_display_container: 2
2023-03-06 18:44:58,893:INFO:Ridge(random_state=2359)
2023-03-06 18:44:58,894:INFO:create_model() successfully completed......................................
2023-03-06 18:44:59,103:INFO:SubProcess create_model() end ==================================
2023-03-06 18:44:59,104:INFO:Creating metrics dataframe
2023-03-06 18:44:59,126:INFO:Initializing Elastic Net
2023-03-06 18:44:59,127:INFO:Total runtime is 0.30075898170471194 minutes
2023-03-06 18:44:59,137:INFO:SubProcess create_model() called ==================================
2023-03-06 18:44:59,138:INFO:Initializing create_model()
2023-03-06 18:44:59,138:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055911A6A0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055911A130>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:44:59,139:INFO:Checking exceptions
2023-03-06 18:44:59,139:INFO:Importing libraries
2023-03-06 18:44:59,139:INFO:Copying training dataset
2023-03-06 18:44:59,153:INFO:Defining folds
2023-03-06 18:44:59,153:INFO:Declaring metric variables
2023-03-06 18:44:59,162:INFO:Importing untrained model
2023-03-06 18:44:59,177:INFO:Elastic Net Imported successfully
2023-03-06 18:44:59,196:INFO:Starting cross validation
2023-03-06 18:44:59,201:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:45:01,154:INFO:Calculating mean and std
2023-03-06 18:45:01,158:INFO:Creating metrics dataframe
2023-03-06 18:45:01,165:INFO:Uploading results into container
2023-03-06 18:45:01,167:INFO:Uploading model into container now
2023-03-06 18:45:01,168:INFO:_master_model_container: 4
2023-03-06 18:45:01,168:INFO:_display_container: 2
2023-03-06 18:45:01,169:INFO:ElasticNet(random_state=2359)
2023-03-06 18:45:01,169:INFO:create_model() successfully completed......................................
2023-03-06 18:45:01,355:INFO:SubProcess create_model() end ==================================
2023-03-06 18:45:01,355:INFO:Creating metrics dataframe
2023-03-06 18:45:01,383:INFO:Initializing Least Angle Regression
2023-03-06 18:45:01,384:INFO:Total runtime is 0.3383599678675334 minutes
2023-03-06 18:45:01,393:INFO:SubProcess create_model() called ==================================
2023-03-06 18:45:01,393:INFO:Initializing create_model()
2023-03-06 18:45:01,394:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055911A6A0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055911A130>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:45:01,395:INFO:Checking exceptions
2023-03-06 18:45:01,395:INFO:Importing libraries
2023-03-06 18:45:01,395:INFO:Copying training dataset
2023-03-06 18:45:01,408:INFO:Defining folds
2023-03-06 18:45:01,408:INFO:Declaring metric variables
2023-03-06 18:45:01,423:INFO:Importing untrained model
2023-03-06 18:45:01,433:INFO:Least Angle Regression Imported successfully
2023-03-06 18:45:01,449:INFO:Starting cross validation
2023-03-06 18:45:01,455:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:45:01,943:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:45:01,989:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:45:02,016:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:45:02,047:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:45:02,077:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:45:02,209:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:45:02,210:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:45:02,234:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:45:02,832:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:45:02,868:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:45:03,323:INFO:Calculating mean and std
2023-03-06 18:45:03,326:INFO:Creating metrics dataframe
2023-03-06 18:45:03,334:INFO:Uploading results into container
2023-03-06 18:45:03,335:INFO:Uploading model into container now
2023-03-06 18:45:03,336:INFO:_master_model_container: 5
2023-03-06 18:45:03,337:INFO:_display_container: 2
2023-03-06 18:45:03,337:INFO:Lars(random_state=2359)
2023-03-06 18:45:03,338:INFO:create_model() successfully completed......................................
2023-03-06 18:45:03,528:INFO:SubProcess create_model() end ==================================
2023-03-06 18:45:03,529:INFO:Creating metrics dataframe
2023-03-06 18:45:03,555:INFO:Initializing Lasso Least Angle Regression
2023-03-06 18:45:03,555:INFO:Total runtime is 0.37455136775970466 minutes
2023-03-06 18:45:03,564:INFO:SubProcess create_model() called ==================================
2023-03-06 18:45:03,565:INFO:Initializing create_model()
2023-03-06 18:45:03,565:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055911A6A0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055911A130>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:45:03,565:INFO:Checking exceptions
2023-03-06 18:45:03,565:INFO:Importing libraries
2023-03-06 18:45:03,566:INFO:Copying training dataset
2023-03-06 18:45:03,578:INFO:Defining folds
2023-03-06 18:45:03,579:INFO:Declaring metric variables
2023-03-06 18:45:03,587:INFO:Importing untrained model
2023-03-06 18:45:03,598:INFO:Lasso Least Angle Regression Imported successfully
2023-03-06 18:45:03,617:INFO:Starting cross validation
2023-03-06 18:45:03,620:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:45:04,098:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:45:04,125:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:45:04,169:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:45:04,208:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:45:04,225:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:45:04,343:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:45:04,361:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:45:04,387:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:45:04,966:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:45:04,987:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:45:05,410:INFO:Calculating mean and std
2023-03-06 18:45:05,412:INFO:Creating metrics dataframe
2023-03-06 18:45:05,420:INFO:Uploading results into container
2023-03-06 18:45:05,422:INFO:Uploading model into container now
2023-03-06 18:45:05,423:INFO:_master_model_container: 6
2023-03-06 18:45:05,423:INFO:_display_container: 2
2023-03-06 18:45:05,425:INFO:LassoLars(random_state=2359)
2023-03-06 18:45:05,425:INFO:create_model() successfully completed......................................
2023-03-06 18:45:05,617:INFO:SubProcess create_model() end ==================================
2023-03-06 18:45:05,618:INFO:Creating metrics dataframe
2023-03-06 18:45:05,642:INFO:Initializing Orthogonal Matching Pursuit
2023-03-06 18:45:05,642:INFO:Total runtime is 0.4093327283859254 minutes
2023-03-06 18:45:05,650:INFO:SubProcess create_model() called ==================================
2023-03-06 18:45:05,651:INFO:Initializing create_model()
2023-03-06 18:45:05,651:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055911A6A0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055911A130>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:45:05,651:INFO:Checking exceptions
2023-03-06 18:45:05,652:INFO:Importing libraries
2023-03-06 18:45:05,652:INFO:Copying training dataset
2023-03-06 18:45:05,663:INFO:Defining folds
2023-03-06 18:45:05,663:INFO:Declaring metric variables
2023-03-06 18:45:05,671:INFO:Importing untrained model
2023-03-06 18:45:05,682:INFO:Orthogonal Matching Pursuit Imported successfully
2023-03-06 18:45:05,699:INFO:Starting cross validation
2023-03-06 18:45:05,702:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:45:06,183:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:45:06,228:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:45:06,229:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:45:06,274:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:45:06,302:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:45:06,420:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:45:06,425:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:45:06,451:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:45:07,103:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:45:07,123:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:45:07,524:INFO:Calculating mean and std
2023-03-06 18:45:07,527:INFO:Creating metrics dataframe
2023-03-06 18:45:07,535:INFO:Uploading results into container
2023-03-06 18:45:07,536:INFO:Uploading model into container now
2023-03-06 18:45:07,537:INFO:_master_model_container: 7
2023-03-06 18:45:07,537:INFO:_display_container: 2
2023-03-06 18:45:07,538:INFO:OrthogonalMatchingPursuit()
2023-03-06 18:45:07,538:INFO:create_model() successfully completed......................................
2023-03-06 18:45:07,726:INFO:SubProcess create_model() end ==================================
2023-03-06 18:45:07,726:INFO:Creating metrics dataframe
2023-03-06 18:45:07,766:INFO:Initializing Bayesian Ridge
2023-03-06 18:45:07,766:INFO:Total runtime is 0.4447420835494996 minutes
2023-03-06 18:45:07,776:INFO:SubProcess create_model() called ==================================
2023-03-06 18:45:07,777:INFO:Initializing create_model()
2023-03-06 18:45:07,777:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055911A6A0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055911A130>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:45:07,778:INFO:Checking exceptions
2023-03-06 18:45:07,778:INFO:Importing libraries
2023-03-06 18:45:07,779:INFO:Copying training dataset
2023-03-06 18:45:07,796:INFO:Defining folds
2023-03-06 18:45:07,797:INFO:Declaring metric variables
2023-03-06 18:45:07,811:INFO:Importing untrained model
2023-03-06 18:45:07,822:INFO:Bayesian Ridge Imported successfully
2023-03-06 18:45:07,842:INFO:Starting cross validation
2023-03-06 18:45:07,849:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:45:09,783:INFO:Calculating mean and std
2023-03-06 18:45:09,787:INFO:Creating metrics dataframe
2023-03-06 18:45:09,795:INFO:Uploading results into container
2023-03-06 18:45:09,796:INFO:Uploading model into container now
2023-03-06 18:45:09,797:INFO:_master_model_container: 8
2023-03-06 18:45:09,797:INFO:_display_container: 2
2023-03-06 18:45:09,798:INFO:BayesianRidge()
2023-03-06 18:45:09,799:INFO:create_model() successfully completed......................................
2023-03-06 18:45:09,985:INFO:SubProcess create_model() end ==================================
2023-03-06 18:45:09,985:INFO:Creating metrics dataframe
2023-03-06 18:45:10,017:INFO:Initializing Passive Aggressive Regressor
2023-03-06 18:45:10,017:INFO:Total runtime is 0.4822532375653586 minutes
2023-03-06 18:45:10,027:INFO:SubProcess create_model() called ==================================
2023-03-06 18:45:10,027:INFO:Initializing create_model()
2023-03-06 18:45:10,028:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055911A6A0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055911A130>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:45:10,028:INFO:Checking exceptions
2023-03-06 18:45:10,029:INFO:Importing libraries
2023-03-06 18:45:10,029:INFO:Copying training dataset
2023-03-06 18:45:10,042:INFO:Defining folds
2023-03-06 18:45:10,043:INFO:Declaring metric variables
2023-03-06 18:45:10,059:INFO:Importing untrained model
2023-03-06 18:45:10,078:INFO:Passive Aggressive Regressor Imported successfully
2023-03-06 18:45:10,101:INFO:Starting cross validation
2023-03-06 18:45:10,107:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:45:11,924:INFO:Calculating mean and std
2023-03-06 18:45:11,928:INFO:Creating metrics dataframe
2023-03-06 18:45:11,937:INFO:Uploading results into container
2023-03-06 18:45:11,939:INFO:Uploading model into container now
2023-03-06 18:45:11,940:INFO:_master_model_container: 9
2023-03-06 18:45:11,940:INFO:_display_container: 2
2023-03-06 18:45:11,941:INFO:PassiveAggressiveRegressor(random_state=2359)
2023-03-06 18:45:11,941:INFO:create_model() successfully completed......................................
2023-03-06 18:45:12,135:INFO:SubProcess create_model() end ==================================
2023-03-06 18:45:12,136:INFO:Creating metrics dataframe
2023-03-06 18:45:12,164:INFO:Initializing Huber Regressor
2023-03-06 18:45:12,164:INFO:Total runtime is 0.5180263002713522 minutes
2023-03-06 18:45:12,174:INFO:SubProcess create_model() called ==================================
2023-03-06 18:45:12,174:INFO:Initializing create_model()
2023-03-06 18:45:12,175:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055911A6A0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055911A130>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:45:12,176:INFO:Checking exceptions
2023-03-06 18:45:12,176:INFO:Importing libraries
2023-03-06 18:45:12,176:INFO:Copying training dataset
2023-03-06 18:45:12,188:INFO:Defining folds
2023-03-06 18:45:12,189:INFO:Declaring metric variables
2023-03-06 18:45:12,200:INFO:Importing untrained model
2023-03-06 18:45:12,210:INFO:Huber Regressor Imported successfully
2023-03-06 18:45:12,229:INFO:Starting cross validation
2023-03-06 18:45:12,233:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:45:14,242:INFO:Calculating mean and std
2023-03-06 18:45:14,245:INFO:Creating metrics dataframe
2023-03-06 18:45:14,256:INFO:Uploading results into container
2023-03-06 18:45:14,257:INFO:Uploading model into container now
2023-03-06 18:45:14,258:INFO:_master_model_container: 10
2023-03-06 18:45:14,259:INFO:_display_container: 2
2023-03-06 18:45:14,260:INFO:HuberRegressor()
2023-03-06 18:45:14,261:INFO:create_model() successfully completed......................................
2023-03-06 18:45:14,469:INFO:SubProcess create_model() end ==================================
2023-03-06 18:45:14,469:INFO:Creating metrics dataframe
2023-03-06 18:45:14,496:INFO:Initializing K Neighbors Regressor
2023-03-06 18:45:14,497:INFO:Total runtime is 0.5569245656331381 minutes
2023-03-06 18:45:14,506:INFO:SubProcess create_model() called ==================================
2023-03-06 18:45:14,508:INFO:Initializing create_model()
2023-03-06 18:45:14,508:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055911A6A0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055911A130>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:45:14,508:INFO:Checking exceptions
2023-03-06 18:45:14,508:INFO:Importing libraries
2023-03-06 18:45:14,509:INFO:Copying training dataset
2023-03-06 18:45:14,522:INFO:Defining folds
2023-03-06 18:45:14,522:INFO:Declaring metric variables
2023-03-06 18:45:14,531:INFO:Importing untrained model
2023-03-06 18:45:14,544:INFO:K Neighbors Regressor Imported successfully
2023-03-06 18:45:14,564:INFO:Starting cross validation
2023-03-06 18:45:14,568:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:45:16,477:INFO:Calculating mean and std
2023-03-06 18:45:16,483:INFO:Creating metrics dataframe
2023-03-06 18:45:16,494:INFO:Uploading results into container
2023-03-06 18:45:16,495:INFO:Uploading model into container now
2023-03-06 18:45:16,496:INFO:_master_model_container: 11
2023-03-06 18:45:16,497:INFO:_display_container: 2
2023-03-06 18:45:16,497:INFO:KNeighborsRegressor(n_jobs=-1)
2023-03-06 18:45:16,498:INFO:create_model() successfully completed......................................
2023-03-06 18:45:16,726:INFO:SubProcess create_model() end ==================================
2023-03-06 18:45:16,727:INFO:Creating metrics dataframe
2023-03-06 18:45:16,769:INFO:Initializing Decision Tree Regressor
2023-03-06 18:45:16,770:INFO:Total runtime is 0.5947953502337139 minutes
2023-03-06 18:45:16,781:INFO:SubProcess create_model() called ==================================
2023-03-06 18:45:16,782:INFO:Initializing create_model()
2023-03-06 18:45:16,783:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055911A6A0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055911A130>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:45:16,784:INFO:Checking exceptions
2023-03-06 18:45:16,784:INFO:Importing libraries
2023-03-06 18:45:16,784:INFO:Copying training dataset
2023-03-06 18:45:16,803:INFO:Defining folds
2023-03-06 18:45:16,804:INFO:Declaring metric variables
2023-03-06 18:45:16,813:INFO:Importing untrained model
2023-03-06 18:45:16,824:INFO:Decision Tree Regressor Imported successfully
2023-03-06 18:45:16,844:INFO:Starting cross validation
2023-03-06 18:45:16,847:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:45:18,697:INFO:Calculating mean and std
2023-03-06 18:45:18,700:INFO:Creating metrics dataframe
2023-03-06 18:45:18,710:INFO:Uploading results into container
2023-03-06 18:45:18,712:INFO:Uploading model into container now
2023-03-06 18:45:18,713:INFO:_master_model_container: 12
2023-03-06 18:45:18,713:INFO:_display_container: 2
2023-03-06 18:45:18,714:INFO:DecisionTreeRegressor(random_state=2359)
2023-03-06 18:45:18,714:INFO:create_model() successfully completed......................................
2023-03-06 18:45:18,911:INFO:SubProcess create_model() end ==================================
2023-03-06 18:45:18,911:INFO:Creating metrics dataframe
2023-03-06 18:45:18,939:INFO:Initializing Random Forest Regressor
2023-03-06 18:45:18,940:INFO:Total runtime is 0.6309658765792847 minutes
2023-03-06 18:45:18,948:INFO:SubProcess create_model() called ==================================
2023-03-06 18:45:18,949:INFO:Initializing create_model()
2023-03-06 18:45:18,950:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055911A6A0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055911A130>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:45:18,950:INFO:Checking exceptions
2023-03-06 18:45:18,950:INFO:Importing libraries
2023-03-06 18:45:18,951:INFO:Copying training dataset
2023-03-06 18:45:18,962:INFO:Defining folds
2023-03-06 18:45:18,962:INFO:Declaring metric variables
2023-03-06 18:45:18,969:INFO:Importing untrained model
2023-03-06 18:45:18,978:INFO:Random Forest Regressor Imported successfully
2023-03-06 18:45:18,995:INFO:Starting cross validation
2023-03-06 18:45:19,001:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:45:23,426:INFO:Calculating mean and std
2023-03-06 18:45:23,429:INFO:Creating metrics dataframe
2023-03-06 18:45:23,437:INFO:Uploading results into container
2023-03-06 18:45:23,439:INFO:Uploading model into container now
2023-03-06 18:45:23,440:INFO:_master_model_container: 13
2023-03-06 18:45:23,440:INFO:_display_container: 2
2023-03-06 18:45:23,442:INFO:RandomForestRegressor(n_jobs=-1, random_state=2359)
2023-03-06 18:45:23,442:INFO:create_model() successfully completed......................................
2023-03-06 18:45:23,692:INFO:SubProcess create_model() end ==================================
2023-03-06 18:45:23,692:INFO:Creating metrics dataframe
2023-03-06 18:45:23,723:INFO:Initializing Extra Trees Regressor
2023-03-06 18:45:23,724:INFO:Total runtime is 0.7107044100761414 minutes
2023-03-06 18:45:23,731:INFO:SubProcess create_model() called ==================================
2023-03-06 18:45:23,732:INFO:Initializing create_model()
2023-03-06 18:45:23,732:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055911A6A0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055911A130>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:45:23,733:INFO:Checking exceptions
2023-03-06 18:45:23,733:INFO:Importing libraries
2023-03-06 18:45:23,734:INFO:Copying training dataset
2023-03-06 18:45:23,745:INFO:Defining folds
2023-03-06 18:45:23,746:INFO:Declaring metric variables
2023-03-06 18:45:23,757:INFO:Importing untrained model
2023-03-06 18:45:23,768:INFO:Extra Trees Regressor Imported successfully
2023-03-06 18:45:23,788:INFO:Starting cross validation
2023-03-06 18:45:23,793:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:45:27,852:INFO:Calculating mean and std
2023-03-06 18:45:27,855:INFO:Creating metrics dataframe
2023-03-06 18:45:27,864:INFO:Uploading results into container
2023-03-06 18:45:27,866:INFO:Uploading model into container now
2023-03-06 18:45:27,867:INFO:_master_model_container: 14
2023-03-06 18:45:27,868:INFO:_display_container: 2
2023-03-06 18:45:27,869:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=2359)
2023-03-06 18:45:27,870:INFO:create_model() successfully completed......................................
2023-03-06 18:45:28,061:INFO:SubProcess create_model() end ==================================
2023-03-06 18:45:28,061:INFO:Creating metrics dataframe
2023-03-06 18:45:28,105:INFO:Initializing AdaBoost Regressor
2023-03-06 18:45:28,105:INFO:Total runtime is 0.7837154467900594 minutes
2023-03-06 18:45:28,112:INFO:SubProcess create_model() called ==================================
2023-03-06 18:45:28,113:INFO:Initializing create_model()
2023-03-06 18:45:28,113:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055911A6A0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055911A130>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:45:28,113:INFO:Checking exceptions
2023-03-06 18:45:28,113:INFO:Importing libraries
2023-03-06 18:45:28,114:INFO:Copying training dataset
2023-03-06 18:45:28,126:INFO:Defining folds
2023-03-06 18:45:28,126:INFO:Declaring metric variables
2023-03-06 18:45:28,138:INFO:Importing untrained model
2023-03-06 18:45:28,146:INFO:AdaBoost Regressor Imported successfully
2023-03-06 18:45:28,167:INFO:Starting cross validation
2023-03-06 18:45:28,172:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:45:31,051:INFO:Calculating mean and std
2023-03-06 18:45:31,055:INFO:Creating metrics dataframe
2023-03-06 18:45:31,063:INFO:Uploading results into container
2023-03-06 18:45:31,064:INFO:Uploading model into container now
2023-03-06 18:45:31,065:INFO:_master_model_container: 15
2023-03-06 18:45:31,065:INFO:_display_container: 2
2023-03-06 18:45:31,066:INFO:AdaBoostRegressor(random_state=2359)
2023-03-06 18:45:31,067:INFO:create_model() successfully completed......................................
2023-03-06 18:45:31,275:INFO:SubProcess create_model() end ==================================
2023-03-06 18:45:31,275:INFO:Creating metrics dataframe
2023-03-06 18:45:31,312:INFO:Initializing Gradient Boosting Regressor
2023-03-06 18:45:31,312:INFO:Total runtime is 0.8371744354565939 minutes
2023-03-06 18:45:31,321:INFO:SubProcess create_model() called ==================================
2023-03-06 18:45:31,322:INFO:Initializing create_model()
2023-03-06 18:45:31,322:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055911A6A0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055911A130>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:45:31,322:INFO:Checking exceptions
2023-03-06 18:45:31,323:INFO:Importing libraries
2023-03-06 18:45:31,323:INFO:Copying training dataset
2023-03-06 18:45:31,339:INFO:Defining folds
2023-03-06 18:45:31,339:INFO:Declaring metric variables
2023-03-06 18:45:31,350:INFO:Importing untrained model
2023-03-06 18:45:31,363:INFO:Gradient Boosting Regressor Imported successfully
2023-03-06 18:45:31,387:INFO:Starting cross validation
2023-03-06 18:45:31,393:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:45:34,848:INFO:Calculating mean and std
2023-03-06 18:45:34,851:INFO:Creating metrics dataframe
2023-03-06 18:45:34,859:INFO:Uploading results into container
2023-03-06 18:45:34,861:INFO:Uploading model into container now
2023-03-06 18:45:34,863:INFO:_master_model_container: 16
2023-03-06 18:45:34,864:INFO:_display_container: 2
2023-03-06 18:45:34,864:INFO:GradientBoostingRegressor(random_state=2359)
2023-03-06 18:45:34,865:INFO:create_model() successfully completed......................................
2023-03-06 18:45:35,060:INFO:SubProcess create_model() end ==================================
2023-03-06 18:45:35,060:INFO:Creating metrics dataframe
2023-03-06 18:45:35,091:INFO:Initializing Extreme Gradient Boosting
2023-03-06 18:45:35,092:INFO:Total runtime is 0.9001654187838237 minutes
2023-03-06 18:45:35,103:INFO:SubProcess create_model() called ==================================
2023-03-06 18:45:35,103:INFO:Initializing create_model()
2023-03-06 18:45:35,104:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055911A6A0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055911A130>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:45:35,104:INFO:Checking exceptions
2023-03-06 18:45:35,104:INFO:Importing libraries
2023-03-06 18:45:35,104:INFO:Copying training dataset
2023-03-06 18:45:35,115:INFO:Defining folds
2023-03-06 18:45:35,116:INFO:Declaring metric variables
2023-03-06 18:45:35,124:INFO:Importing untrained model
2023-03-06 18:45:35,133:INFO:Extreme Gradient Boosting Imported successfully
2023-03-06 18:45:35,153:INFO:Starting cross validation
2023-03-06 18:45:35,156:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:45:38,525:INFO:Calculating mean and std
2023-03-06 18:45:38,530:INFO:Creating metrics dataframe
2023-03-06 18:45:38,537:INFO:Uploading results into container
2023-03-06 18:45:38,538:INFO:Uploading model into container now
2023-03-06 18:45:38,539:INFO:_master_model_container: 17
2023-03-06 18:45:38,540:INFO:_display_container: 2
2023-03-06 18:45:38,542:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=2359, ...)
2023-03-06 18:45:38,542:INFO:create_model() successfully completed......................................
2023-03-06 18:45:38,733:INFO:SubProcess create_model() end ==================================
2023-03-06 18:45:38,733:INFO:Creating metrics dataframe
2023-03-06 18:45:38,768:INFO:Initializing Light Gradient Boosting Machine
2023-03-06 18:45:38,768:INFO:Total runtime is 0.9614299813906353 minutes
2023-03-06 18:45:38,782:INFO:SubProcess create_model() called ==================================
2023-03-06 18:45:38,783:INFO:Initializing create_model()
2023-03-06 18:45:38,783:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055911A6A0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055911A130>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:45:38,784:INFO:Checking exceptions
2023-03-06 18:45:38,784:INFO:Importing libraries
2023-03-06 18:45:38,784:INFO:Copying training dataset
2023-03-06 18:45:38,803:INFO:Defining folds
2023-03-06 18:45:38,804:INFO:Declaring metric variables
2023-03-06 18:45:38,816:INFO:Importing untrained model
2023-03-06 18:45:38,833:INFO:Light Gradient Boosting Machine Imported successfully
2023-03-06 18:45:38,861:INFO:Starting cross validation
2023-03-06 18:45:38,869:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:45:43,790:INFO:Calculating mean and std
2023-03-06 18:45:43,793:INFO:Creating metrics dataframe
2023-03-06 18:45:43,803:INFO:Uploading results into container
2023-03-06 18:45:43,804:INFO:Uploading model into container now
2023-03-06 18:45:43,805:INFO:_master_model_container: 18
2023-03-06 18:45:43,806:INFO:_display_container: 2
2023-03-06 18:45:43,807:INFO:LGBMRegressor(random_state=2359)
2023-03-06 18:45:43,807:INFO:create_model() successfully completed......................................
2023-03-06 18:45:44,044:INFO:SubProcess create_model() end ==================================
2023-03-06 18:45:44,044:INFO:Creating metrics dataframe
2023-03-06 18:45:44,077:INFO:Initializing Dummy Regressor
2023-03-06 18:45:44,078:INFO:Total runtime is 1.0499290982882183 minutes
2023-03-06 18:45:44,085:INFO:SubProcess create_model() called ==================================
2023-03-06 18:45:44,086:INFO:Initializing create_model()
2023-03-06 18:45:44,086:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055911A6A0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055911A130>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:45:44,087:INFO:Checking exceptions
2023-03-06 18:45:44,087:INFO:Importing libraries
2023-03-06 18:45:44,088:INFO:Copying training dataset
2023-03-06 18:45:44,108:INFO:Defining folds
2023-03-06 18:45:44,108:INFO:Declaring metric variables
2023-03-06 18:45:44,120:INFO:Importing untrained model
2023-03-06 18:45:44,131:INFO:Dummy Regressor Imported successfully
2023-03-06 18:45:44,154:INFO:Starting cross validation
2023-03-06 18:45:44,159:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:45:46,237:INFO:Calculating mean and std
2023-03-06 18:45:46,242:INFO:Creating metrics dataframe
2023-03-06 18:45:46,256:INFO:Uploading results into container
2023-03-06 18:45:46,258:INFO:Uploading model into container now
2023-03-06 18:45:46,260:INFO:_master_model_container: 19
2023-03-06 18:45:46,261:INFO:_display_container: 2
2023-03-06 18:45:46,262:INFO:DummyRegressor()
2023-03-06 18:45:46,263:INFO:create_model() successfully completed......................................
2023-03-06 18:45:46,488:INFO:SubProcess create_model() end ==================================
2023-03-06 18:45:46,488:INFO:Creating metrics dataframe
2023-03-06 18:45:46,552:INFO:Initializing create_model()
2023-03-06 18:45:46,552:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055911A6A0>, estimator=HuberRegressor(), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:45:46,553:INFO:Checking exceptions
2023-03-06 18:45:46,557:INFO:Importing libraries
2023-03-06 18:45:46,557:INFO:Copying training dataset
2023-03-06 18:45:46,567:INFO:Defining folds
2023-03-06 18:45:46,567:INFO:Declaring metric variables
2023-03-06 18:45:46,568:INFO:Importing untrained model
2023-03-06 18:45:46,568:INFO:Declaring custom model
2023-03-06 18:45:46,569:INFO:Huber Regressor Imported successfully
2023-03-06 18:45:46,572:INFO:Cross validation set to False
2023-03-06 18:45:46,572:INFO:Fitting Model
2023-03-06 18:45:47,131:INFO:HuberRegressor()
2023-03-06 18:45:47,132:INFO:create_model() successfully completed......................................
2023-03-06 18:45:47,467:INFO:_master_model_container: 19
2023-03-06 18:45:47,468:INFO:_display_container: 2
2023-03-06 18:45:47,469:INFO:HuberRegressor()
2023-03-06 18:45:47,469:INFO:compare_models() successfully completed......................................
2023-03-06 18:45:47,487:INFO:Initializing evaluate_model()
2023-03-06 18:45:47,487:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055911A6A0>, estimator=HuberRegressor(), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-03-06 18:45:47,558:INFO:Initializing plot_model()
2023-03-06 18:45:47,559:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=HuberRegressor(), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055911A6A0>, system=True)
2023-03-06 18:45:47,560:INFO:Checking exceptions
2023-03-06 18:45:47,567:INFO:Preloading libraries
2023-03-06 18:45:47,568:INFO:Copying training dataset
2023-03-06 18:45:47,568:INFO:Plot type: pipeline
2023-03-06 18:45:47,969:INFO:Visual Rendered Successfully
2023-03-06 18:45:48,162:INFO:plot_model() successfully completed......................................
2023-03-06 18:47:11,513:INFO:PyCaret RegressionExperiment
2023-03-06 18:47:11,514:INFO:Logging name: reg-default-name
2023-03-06 18:47:11,514:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-06 18:47:11,514:INFO:version 3.0.0.rc9
2023-03-06 18:47:11,514:INFO:Initializing setup()
2023-03-06 18:47:11,514:INFO:self.USI: 364e
2023-03-06 18:47:11,514:INFO:self._variable_keys: {'_available_plots', 'fold_groups_param', 'fold_generator', 'pipeline', 'USI', 'logging_param', 'X', '_ml_usecase', 'idx', 'exp_id', 'fold_shuffle_param', 'X_train', 'exp_name_log', 'y_test', 'transform_target_param', 'y_train', 'seed', 'data', 'log_plots_param', 'gpu_n_jobs_param', 'target_param', 'n_jobs_param', 'X_test', 'memory', 'y', 'html_param', 'gpu_param'}
2023-03-06 18:47:11,514:INFO:Checking environment
2023-03-06 18:47:11,515:INFO:python_version: 3.9.13
2023-03-06 18:47:11,515:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-03-06 18:47:11,515:INFO:machine: AMD64
2023-03-06 18:47:11,515:INFO:platform: Windows-10-10.0.19044-SP0
2023-03-06 18:47:11,515:INFO:Memory: svmem(total=17009516544, available=4776136704, percent=71.9, used=12233379840, free=4776136704)
2023-03-06 18:47:11,515:INFO:Physical Core: 4
2023-03-06 18:47:11,515:INFO:Logical Core: 8
2023-03-06 18:47:11,515:INFO:Checking libraries
2023-03-06 18:47:11,515:INFO:System:
2023-03-06 18:47:11,516:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-03-06 18:47:11,516:INFO:executable: C:\Users\Alvaro\anaconda3\python.exe
2023-03-06 18:47:11,516:INFO:   machine: Windows-10-10.0.19044-SP0
2023-03-06 18:47:11,516:INFO:PyCaret required dependencies:
2023-03-06 18:47:11,516:INFO:                 pip: 22.2.2
2023-03-06 18:47:11,516:INFO:          setuptools: 63.4.1
2023-03-06 18:47:11,516:INFO:             pycaret: 3.0.0rc9
2023-03-06 18:47:11,517:INFO:             IPython: 7.31.1
2023-03-06 18:47:11,517:INFO:          ipywidgets: 7.6.5
2023-03-06 18:47:11,517:INFO:                tqdm: 4.64.1
2023-03-06 18:47:11,517:INFO:               numpy: 1.21.5
2023-03-06 18:47:11,517:INFO:              pandas: 1.4.4
2023-03-06 18:47:11,517:INFO:              jinja2: 2.11.3
2023-03-06 18:47:11,518:INFO:               scipy: 1.9.1
2023-03-06 18:47:11,518:INFO:              joblib: 1.2.0
2023-03-06 18:47:11,518:INFO:             sklearn: 1.0.2
2023-03-06 18:47:11,518:INFO:                pyod: 1.0.7
2023-03-06 18:47:11,518:INFO:            imblearn: 0.10.1
2023-03-06 18:47:11,518:INFO:   category_encoders: 2.6.0
2023-03-06 18:47:11,518:INFO:            lightgbm: 3.3.5
2023-03-06 18:47:11,519:INFO:               numba: 0.55.1
2023-03-06 18:47:11,519:INFO:            requests: 2.28.1
2023-03-06 18:47:11,519:INFO:          matplotlib: 3.5.2
2023-03-06 18:47:11,519:INFO:          scikitplot: 0.3.7
2023-03-06 18:47:11,519:INFO:         yellowbrick: 1.5
2023-03-06 18:47:11,519:INFO:              plotly: 5.9.0
2023-03-06 18:47:11,519:INFO:             kaleido: 0.2.1
2023-03-06 18:47:11,519:INFO:         statsmodels: 0.13.2
2023-03-06 18:47:11,519:INFO:              sktime: 0.16.1
2023-03-06 18:47:11,519:INFO:               tbats: 1.1.2
2023-03-06 18:47:11,519:INFO:            pmdarima: 2.0.2
2023-03-06 18:47:11,520:INFO:              psutil: 5.9.0
2023-03-06 18:47:11,520:INFO:PyCaret optional dependencies:
2023-03-06 18:47:11,520:INFO:                shap: Not installed
2023-03-06 18:47:11,520:INFO:           interpret: Not installed
2023-03-06 18:47:11,520:INFO:                umap: Not installed
2023-03-06 18:47:11,520:INFO:    pandas_profiling: Not installed
2023-03-06 18:47:11,520:INFO:  explainerdashboard: Not installed
2023-03-06 18:47:11,520:INFO:             autoviz: Not installed
2023-03-06 18:47:11,520:INFO:           fairlearn: Not installed
2023-03-06 18:47:11,520:INFO:             xgboost: 1.7.4
2023-03-06 18:47:11,521:INFO:            catboost: Not installed
2023-03-06 18:47:11,521:INFO:              kmodes: Not installed
2023-03-06 18:47:11,521:INFO:             mlxtend: Not installed
2023-03-06 18:47:11,521:INFO:       statsforecast: Not installed
2023-03-06 18:47:11,521:INFO:        tune_sklearn: Not installed
2023-03-06 18:47:11,521:INFO:                 ray: Not installed
2023-03-06 18:47:11,521:INFO:            hyperopt: Not installed
2023-03-06 18:47:11,521:INFO:              optuna: Not installed
2023-03-06 18:47:11,521:INFO:               skopt: Not installed
2023-03-06 18:47:11,521:INFO:              mlflow: Not installed
2023-03-06 18:47:11,521:INFO:              gradio: Not installed
2023-03-06 18:47:11,522:INFO:             fastapi: Not installed
2023-03-06 18:47:11,522:INFO:             uvicorn: Not installed
2023-03-06 18:47:11,522:INFO:              m2cgen: Not installed
2023-03-06 18:47:11,522:INFO:           evidently: Not installed
2023-03-06 18:47:11,522:INFO:               fugue: Not installed
2023-03-06 18:47:11,522:INFO:           streamlit: Not installed
2023-03-06 18:47:11,522:INFO:             prophet: Not installed
2023-03-06 18:47:11,522:INFO:None
2023-03-06 18:47:11,522:INFO:Set up data.
2023-03-06 18:47:11,605:INFO:Set up train/test split.
2023-03-06 18:47:11,614:INFO:Set up index.
2023-03-06 18:47:11,615:INFO:Set up folding strategy.
2023-03-06 18:47:11,615:INFO:Assigning column types.
2023-03-06 18:47:11,623:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-06 18:47:11,623:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-06 18:47:11,635:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-06 18:47:11,647:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-06 18:47:11,794:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:47:11,910:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 18:47:11,911:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:47:11,918:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:47:11,919:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-06 18:47:11,931:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-06 18:47:11,942:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-06 18:47:12,088:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:47:12,201:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 18:47:12,203:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:47:12,209:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:47:12,210:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-03-06 18:47:12,222:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-06 18:47:12,234:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-06 18:47:12,378:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:47:12,493:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 18:47:12,494:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:47:12,501:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:47:12,514:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-06 18:47:12,526:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-06 18:47:12,673:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:47:12,794:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 18:47:12,795:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:47:12,801:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:47:12,802:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-03-06 18:47:12,826:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-06 18:47:12,975:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:47:13,107:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 18:47:13,109:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:47:13,115:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:47:13,139:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-06 18:47:13,285:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:47:13,400:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 18:47:13,404:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:47:13,411:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:47:13,412:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-03-06 18:47:13,583:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:47:13,698:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 18:47:13,700:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:47:13,706:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:47:13,877:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:47:13,991:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 18:47:13,992:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:47:13,999:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:47:13,999:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-06 18:47:14,171:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:47:14,286:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:47:14,293:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:47:14,463:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:47:14,579:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:47:14,586:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:47:14,587:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-03-06 18:47:14,872:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:47:14,878:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:47:15,162:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:47:15,169:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:47:15,172:INFO:Preparing preprocessing pipeline...
2023-03-06 18:47:15,174:INFO:Set up column name cleaning.
2023-03-06 18:47:15,174:INFO:Set up simple imputation.
2023-03-06 18:47:15,247:INFO:Finished creating preprocessing pipeline.
2023-03-06 18:47:15,262:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Alvaro\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('numerical_imputer',
                 TransformerWrapper(include=['work_year', 'remote_ratio',
                                             'experience_level_EX',
                                             'experience_level_MI',
                                             'experience_level_SE',
                                             'employment_type_FL',
                                             'employment_type_FT',
                                             'emplo...
                                             'job_title_Data Architect',
                                             'job_title_Data Engineer',
                                             'job_title_Data Engineering '
                                             'Manager',
                                             'job_title_Data Science '
                                             'Consultant',
                                             'job_title_Data Science Engineer',
                                             'job_title_Data Science Manager',
                                             'job_title_Data Scientist', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent')))])
2023-03-06 18:47:15,263:INFO:Creating final display dataframe.
2023-03-06 18:47:15,730:INFO:Setup _display_container:                     Description             Value
0                    Session id              8452
1                        Target     salary_in_usd
2                   Target type        Regression
3           Original data shape        (500, 148)
4        Transformed data shape        (500, 148)
5   Transformed train set shape        (350, 148)
6    Transformed test set shape        (150, 148)
7              Numeric features               147
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator             KFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  reg-default-name
18                          USI              364e
2023-03-06 18:47:16,036:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:47:16,043:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:47:16,327:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:47:16,333:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:47:16,335:INFO:setup() successfully completed in 4.83s...............
2023-03-06 18:47:16,351:INFO:Initializing compare_models()
2023-03-06 18:47:16,352:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000205593D4790>, include=None, fold=None, round=4, cross_validation=True, sort=RMSE, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x00000205593D4790>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'RMSE', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-03-06 18:47:16,352:INFO:Checking exceptions
2023-03-06 18:47:16,357:INFO:Preparing display monitor
2023-03-06 18:47:16,468:INFO:Initializing Linear Regression
2023-03-06 18:47:16,468:INFO:Total runtime is 0.0 minutes
2023-03-06 18:47:16,481:INFO:SubProcess create_model() called ==================================
2023-03-06 18:47:16,483:INFO:Initializing create_model()
2023-03-06 18:47:16,483:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000205593D4790>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055D40A160>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:47:16,484:INFO:Checking exceptions
2023-03-06 18:47:16,484:INFO:Importing libraries
2023-03-06 18:47:16,484:INFO:Copying training dataset
2023-03-06 18:47:16,500:INFO:Defining folds
2023-03-06 18:47:16,500:INFO:Declaring metric variables
2023-03-06 18:47:16,509:INFO:Importing untrained model
2023-03-06 18:47:16,520:INFO:Linear Regression Imported successfully
2023-03-06 18:47:16,541:INFO:Starting cross validation
2023-03-06 18:47:16,546:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:47:17,137:INFO:Calculating mean and std
2023-03-06 18:47:17,138:INFO:Creating metrics dataframe
2023-03-06 18:47:17,145:INFO:Uploading results into container
2023-03-06 18:47:17,146:INFO:Uploading model into container now
2023-03-06 18:47:17,147:INFO:_master_model_container: 1
2023-03-06 18:47:17,147:INFO:_display_container: 2
2023-03-06 18:47:17,148:INFO:LinearRegression(n_jobs=-1)
2023-03-06 18:47:17,148:INFO:create_model() successfully completed......................................
2023-03-06 18:47:17,342:INFO:SubProcess create_model() end ==================================
2023-03-06 18:47:17,342:INFO:Creating metrics dataframe
2023-03-06 18:47:17,362:INFO:Initializing Lasso Regression
2023-03-06 18:47:17,363:INFO:Total runtime is 0.014894998073577881 minutes
2023-03-06 18:47:17,371:INFO:SubProcess create_model() called ==================================
2023-03-06 18:47:17,372:INFO:Initializing create_model()
2023-03-06 18:47:17,373:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000205593D4790>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055D40A160>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:47:17,373:INFO:Checking exceptions
2023-03-06 18:47:17,373:INFO:Importing libraries
2023-03-06 18:47:17,373:INFO:Copying training dataset
2023-03-06 18:47:17,386:INFO:Defining folds
2023-03-06 18:47:17,386:INFO:Declaring metric variables
2023-03-06 18:47:17,396:INFO:Importing untrained model
2023-03-06 18:47:17,404:INFO:Lasso Regression Imported successfully
2023-03-06 18:47:17,419:INFO:Starting cross validation
2023-03-06 18:47:17,422:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:47:18,007:INFO:Calculating mean and std
2023-03-06 18:47:18,010:INFO:Creating metrics dataframe
2023-03-06 18:47:18,018:INFO:Uploading results into container
2023-03-06 18:47:18,019:INFO:Uploading model into container now
2023-03-06 18:47:18,020:INFO:_master_model_container: 2
2023-03-06 18:47:18,020:INFO:_display_container: 2
2023-03-06 18:47:18,021:INFO:Lasso(random_state=8452)
2023-03-06 18:47:18,021:INFO:create_model() successfully completed......................................
2023-03-06 18:47:18,276:INFO:SubProcess create_model() end ==================================
2023-03-06 18:47:18,276:INFO:Creating metrics dataframe
2023-03-06 18:47:18,309:INFO:Initializing Ridge Regression
2023-03-06 18:47:18,309:INFO:Total runtime is 0.030688393115997317 minutes
2023-03-06 18:47:18,323:INFO:SubProcess create_model() called ==================================
2023-03-06 18:47:18,324:INFO:Initializing create_model()
2023-03-06 18:47:18,324:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000205593D4790>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055D40A160>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:47:18,324:INFO:Checking exceptions
2023-03-06 18:47:18,326:INFO:Importing libraries
2023-03-06 18:47:18,326:INFO:Copying training dataset
2023-03-06 18:47:18,347:INFO:Defining folds
2023-03-06 18:47:18,347:INFO:Declaring metric variables
2023-03-06 18:47:18,364:INFO:Importing untrained model
2023-03-06 18:47:18,386:INFO:Ridge Regression Imported successfully
2023-03-06 18:47:18,425:INFO:Starting cross validation
2023-03-06 18:47:18,431:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:47:19,097:INFO:Calculating mean and std
2023-03-06 18:47:19,101:INFO:Creating metrics dataframe
2023-03-06 18:47:19,108:INFO:Uploading results into container
2023-03-06 18:47:19,110:INFO:Uploading model into container now
2023-03-06 18:47:19,111:INFO:_master_model_container: 3
2023-03-06 18:47:19,111:INFO:_display_container: 2
2023-03-06 18:47:19,112:INFO:Ridge(random_state=8452)
2023-03-06 18:47:19,112:INFO:create_model() successfully completed......................................
2023-03-06 18:47:19,302:INFO:SubProcess create_model() end ==================================
2023-03-06 18:47:19,302:INFO:Creating metrics dataframe
2023-03-06 18:47:19,335:INFO:Initializing Elastic Net
2023-03-06 18:47:19,336:INFO:Total runtime is 0.04779280821482341 minutes
2023-03-06 18:47:19,347:INFO:SubProcess create_model() called ==================================
2023-03-06 18:47:19,348:INFO:Initializing create_model()
2023-03-06 18:47:19,350:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000205593D4790>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055D40A160>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:47:19,350:INFO:Checking exceptions
2023-03-06 18:47:19,350:INFO:Importing libraries
2023-03-06 18:47:19,350:INFO:Copying training dataset
2023-03-06 18:47:19,366:INFO:Defining folds
2023-03-06 18:47:19,366:INFO:Declaring metric variables
2023-03-06 18:47:19,381:INFO:Importing untrained model
2023-03-06 18:47:19,394:INFO:Elastic Net Imported successfully
2023-03-06 18:47:19,418:INFO:Starting cross validation
2023-03-06 18:47:19,422:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:47:19,984:INFO:Calculating mean and std
2023-03-06 18:47:19,987:INFO:Creating metrics dataframe
2023-03-06 18:47:19,994:INFO:Uploading results into container
2023-03-06 18:47:19,996:INFO:Uploading model into container now
2023-03-06 18:47:19,997:INFO:_master_model_container: 4
2023-03-06 18:47:19,997:INFO:_display_container: 2
2023-03-06 18:47:19,998:INFO:ElasticNet(random_state=8452)
2023-03-06 18:47:19,999:INFO:create_model() successfully completed......................................
2023-03-06 18:47:20,261:INFO:SubProcess create_model() end ==================================
2023-03-06 18:47:20,262:INFO:Creating metrics dataframe
2023-03-06 18:47:20,292:INFO:Initializing Least Angle Regression
2023-03-06 18:47:20,293:INFO:Total runtime is 0.06374705632527669 minutes
2023-03-06 18:47:20,303:INFO:SubProcess create_model() called ==================================
2023-03-06 18:47:20,304:INFO:Initializing create_model()
2023-03-06 18:47:20,305:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000205593D4790>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055D40A160>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:47:20,305:INFO:Checking exceptions
2023-03-06 18:47:20,306:INFO:Importing libraries
2023-03-06 18:47:20,307:INFO:Copying training dataset
2023-03-06 18:47:20,324:INFO:Defining folds
2023-03-06 18:47:20,325:INFO:Declaring metric variables
2023-03-06 18:47:20,339:INFO:Importing untrained model
2023-03-06 18:47:20,358:INFO:Least Angle Regression Imported successfully
2023-03-06 18:47:20,385:INFO:Starting cross validation
2023-03-06 18:47:20,392:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:47:20,606:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:47:20,637:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:47:20,653:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=1.590e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,662:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=1.250e-03, with an active set of 61 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,669:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=2.064e-03, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,675:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:47:20,678:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=1.444e-03, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,683:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=1.283e-03, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,684:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=7.664e-04, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,685:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=1.275e-03, with an active set of 53 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,687:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=1.264e-03, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,692:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=1.147e-03, with an active set of 60 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,693:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=1.146e-03, with an active set of 61 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,696:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=1.098e-03, with an active set of 63 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,696:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 92 iterations, i.e. alpha=6.945e-04, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,697:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 92 iterations, i.e. alpha=6.933e-04, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,699:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:47:20,708:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.777e-03, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,711:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=1.737e-03, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,712:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=1.731e-03, with an active set of 98 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,720:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=2.968e-03, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,726:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.659e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,727:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=2.201e-03, with an active set of 110 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,727:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=2.088e-03, with an active set of 110 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,728:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=1.651e-03, with an active set of 110 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,729:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=1.586e-03, with an active set of 110 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,730:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=1.574e-03, with an active set of 110 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,731:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=1.517e-03, with an active set of 111 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,732:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=9.244e-04, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,733:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=1.370e-03, with an active set of 67 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,733:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 133 iterations, i.e. alpha=1.447e-03, with an active set of 112 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,734:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 133 iterations, i.e. alpha=1.433e-03, with an active set of 112 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,736:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:47:20,737:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=5.780e-04, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,737:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=5.048e-04, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,738:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=5.001e-04, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,739:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=4.784e-04, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,740:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=4.703e-04, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,741:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=2.965e-04, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,741:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.745e-03, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,741:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=2.508e-04, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,742:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=2.114e-04, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,743:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 136 iterations, i.e. alpha=4.765e-05, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,745:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 136 iterations, i.e. alpha=4.495e-05, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,745:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 136 iterations, i.e. alpha=4.135e-05, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,746:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 136 iterations, i.e. alpha=3.866e-05, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,747:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 136 iterations, i.e. alpha=3.057e-05, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,747:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=1.502e-03, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,748:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 136 iterations, i.e. alpha=2.068e-05, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,748:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 136 iterations, i.e. alpha=8.990e-06, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,750:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 144 iterations, i.e. alpha=1.147e+00, with an active set of 110 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,751:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=1.417e-03, with an active set of 58 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,754:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 146 iterations, i.e. alpha=1.053e+00, with an active set of 112 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,754:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=1.330e-03, with an active set of 60 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,755:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=1.321e-03, with an active set of 61 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,756:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 148 iterations, i.e. alpha=1.033e+00, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,757:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 148 iterations, i.e. alpha=1.022e+00, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,757:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=2.859e-03, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,757:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 148 iterations, i.e. alpha=1.008e+00, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,758:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 148 iterations, i.e. alpha=9.853e-01, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,759:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 148 iterations, i.e. alpha=9.678e-01, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,759:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=1.228e-03, with an active set of 64 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,760:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 148 iterations, i.e. alpha=9.605e-01, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,760:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 148 iterations, i.e. alpha=9.414e-01, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,761:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 148 iterations, i.e. alpha=9.402e-01, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,762:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 148 iterations, i.e. alpha=8.966e-01, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,762:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 148 iterations, i.e. alpha=8.963e-01, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,763:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 148 iterations, i.e. alpha=3.875e-02, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,764:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 148 iterations, i.e. alpha=1.913e-02, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,767:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=1.116e-03, with an active set of 72 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,767:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:47:20,776:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.596e-03, with an active set of 48 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,778:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=1.493e-03, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,780:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 146 iterations, i.e. alpha=5.918e+04, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,782:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=1.363e-03, with an active set of 57 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,787:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:47:20,787:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 157 iterations, i.e. alpha=3.874e+04, with an active set of 120 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,788:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 157 iterations, i.e. alpha=2.799e+04, with an active set of 120 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,789:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 157 iterations, i.e. alpha=6.319e+03, with an active set of 120 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,789:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 157 iterations, i.e. alpha=5.936e+03, with an active set of 120 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,791:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=8.851e-04, with an active set of 70 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,796:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=1.023e-03, with an active set of 74 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,796:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 128 iterations, i.e. alpha=6.566e+00, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,797:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:47:20,799:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=1.579e-03, with an active set of 48 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,802:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=1.351e-03, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,804:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 143 iterations, i.e. alpha=1.827e+01, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,804:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=1.237e-03, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,808:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=1.085e-03, with an active set of 59 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,808:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=7.270e-03, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,808:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 149 iterations, i.e. alpha=1.585e+01, with an active set of 109 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,812:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=6.931e-03, with an active set of 92 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,815:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=3.060e-03, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,818:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 164 iterations, i.e. alpha=1.685e+01, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,818:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 164 iterations, i.e. alpha=1.628e+01, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,820:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 164 iterations, i.e. alpha=1.612e+01, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,820:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 164 iterations, i.e. alpha=1.487e+01, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,820:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=6.755e-04, with an active set of 73 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,820:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 164 iterations, i.e. alpha=1.408e+01, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,820:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.664e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,822:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 164 iterations, i.e. alpha=1.389e+01, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,822:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 164 iterations, i.e. alpha=1.030e+01, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,823:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 164 iterations, i.e. alpha=7.779e+00, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,824:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=1.576e-03, with an active set of 56 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,824:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 166 iterations, i.e. alpha=6.687e+00, with an active set of 119 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,824:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=2.355e-03, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,825:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 166 iterations, i.e. alpha=3.194e+00, with an active set of 119 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,826:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 166 iterations, i.e. alpha=2.941e+00, with an active set of 119 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,826:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 166 iterations, i.e. alpha=2.828e+00, with an active set of 119 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,827:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 166 iterations, i.e. alpha=2.812e+00, with an active set of 119 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,827:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 166 iterations, i.e. alpha=2.700e+00, with an active set of 119 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,828:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 166 iterations, i.e. alpha=2.648e+00, with an active set of 119 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,828:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 166 iterations, i.e. alpha=2.643e+00, with an active set of 119 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,829:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 166 iterations, i.e. alpha=2.572e+00, with an active set of 119 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,829:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 92 iterations, i.e. alpha=4.912e-04, with an active set of 87 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,830:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 92 iterations, i.e. alpha=4.876e-04, with an active set of 87 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,835:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 152 iterations, i.e. alpha=3.264e+03, with an active set of 110 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,836:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=2.027e-03, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,837:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 154 iterations, i.e. alpha=2.950e+03, with an active set of 112 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,838:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 154 iterations, i.e. alpha=2.873e+03, with an active set of 112 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,839:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 155 iterations, i.e. alpha=2.857e+03, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,840:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 155 iterations, i.e. alpha=2.594e+03, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,842:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 156 iterations, i.e. alpha=2.470e+03, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,843:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 157 iterations, i.e. alpha=2.296e+03, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,844:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 157 iterations, i.e. alpha=2.290e+03, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,844:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 157 iterations, i.e. alpha=2.361e+02, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,845:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 157 iterations, i.e. alpha=1.520e+02, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,845:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 157 iterations, i.e. alpha=8.871e+01, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,846:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 157 iterations, i.e. alpha=8.787e+01, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,847:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 157 iterations, i.e. alpha=8.177e+01, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,847:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 157 iterations, i.e. alpha=1.916e+01, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,853:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=4.784e+01, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,856:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=4.771e+01, with an active set of 87 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,859:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 143 iterations, i.e. alpha=3.057e-01, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,860:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 143 iterations, i.e. alpha=3.034e-01, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,863:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=2.666e+03, with an active set of 81 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,864:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 151 iterations, i.e. alpha=2.885e-01, with an active set of 119 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,867:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 155 iterations, i.e. alpha=3.199e-01, with an active set of 121 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,868:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 155 iterations, i.e. alpha=2.788e-01, with an active set of 121 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,868:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 155 iterations, i.e. alpha=2.056e-01, with an active set of 121 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,870:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 155 iterations, i.e. alpha=1.254e-01, with an active set of 121 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,874:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=2.500e+03, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,880:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 156 iterations, i.e. alpha=5.764e+04, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,881:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 156 iterations, i.e. alpha=5.421e+04, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,882:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 157 iterations, i.e. alpha=3.951e+04, with an active set of 116 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,883:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 157 iterations, i.e. alpha=3.590e+04, with an active set of 116 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,884:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 158 iterations, i.e. alpha=3.142e+04, with an active set of 117 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,888:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 162 iterations, i.e. alpha=2.458e+04, with an active set of 121 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,890:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 164 iterations, i.e. alpha=1.221e+04, with an active set of 122 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,890:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 164 iterations, i.e. alpha=8.753e+03, with an active set of 122 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,891:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 164 iterations, i.e. alpha=6.228e+03, with an active set of 122 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,893:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 165 iterations, i.e. alpha=2.230e+03, with an active set of 123 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,894:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 165 iterations, i.e. alpha=1.133e+03, with an active set of 123 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,897:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 154 iterations, i.e. alpha=1.385e+09, with an active set of 116 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,900:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 154 iterations, i.e. alpha=1.146e+09, with an active set of 116 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,900:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 154 iterations, i.e. alpha=9.671e+08, with an active set of 116 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,901:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 154 iterations, i.e. alpha=8.005e+08, with an active set of 116 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,901:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 154 iterations, i.e. alpha=6.691e+08, with an active set of 116 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,902:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 154 iterations, i.e. alpha=6.581e+08, with an active set of 116 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,903:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 155 iterations, i.e. alpha=3.830e+08, with an active set of 117 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,904:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 155 iterations, i.e. alpha=3.003e+08, with an active set of 117 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,904:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 155 iterations, i.e. alpha=2.655e+08, with an active set of 117 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,905:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 155 iterations, i.e. alpha=1.349e+08, with an active set of 117 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,906:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 156 iterations, i.e. alpha=8.405e+07, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,906:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 156 iterations, i.e. alpha=3.556e+07, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,907:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 156 iterations, i.e. alpha=2.816e+07, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,907:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 156 iterations, i.e. alpha=2.618e+07, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,908:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 156 iterations, i.e. alpha=1.877e+07, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:20,909:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 156 iterations, i.e. alpha=1.136e+07, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,502:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:47:21,517:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.432e-03, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,522:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:47:21,532:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=9.734e-04, with an active set of 70 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,535:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=2.599e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,537:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.468e-03, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,543:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 100 iterations, i.e. alpha=1.226e-02, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,546:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=1.286e-02, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,549:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=1.374e-03, with an active set of 60 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,552:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=1.184e-03, with an active set of 66 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,556:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=1.029e-03, with an active set of 74 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,561:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=7.471e-04, with an active set of 82 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,562:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=7.368e-04, with an active set of 83 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,564:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 149 iterations, i.e. alpha=2.110e-01, with an active set of 109 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,567:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 156 iterations, i.e. alpha=3.634e-01, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,569:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 158 iterations, i.e. alpha=3.675e-01, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,569:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 159 iterations, i.e. alpha=3.334e-01, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,570:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 159 iterations, i.e. alpha=3.323e-01, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,570:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 159 iterations, i.e. alpha=3.248e-01, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,571:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 159 iterations, i.e. alpha=3.053e-01, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,571:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 159 iterations, i.e. alpha=2.483e-01, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,572:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 159 iterations, i.e. alpha=2.088e-01, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,573:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=2.235e-03, with an active set of 98 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-06 18:47:21,574:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 162 iterations, i.e. alpha=1.148e-01, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,574:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 162 iterations, i.e. alpha=8.949e-02, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,575:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 162 iterations, i.e. alpha=8.178e-02, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,575:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 162 iterations, i.e. alpha=5.982e-02, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,576:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 162 iterations, i.e. alpha=5.846e-02, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,576:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 162 iterations, i.e. alpha=5.702e-02, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,576:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 162 iterations, i.e. alpha=5.296e-02, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,577:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 162 iterations, i.e. alpha=5.151e-02, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,586:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 150 iterations, i.e. alpha=4.829e-01, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,587:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 150 iterations, i.e. alpha=4.786e-01, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,587:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 150 iterations, i.e. alpha=4.751e-01, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,588:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 150 iterations, i.e. alpha=4.562e-01, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,588:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 150 iterations, i.e. alpha=4.315e-01, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,589:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 151 iterations, i.e. alpha=3.979e-01, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,590:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 152 iterations, i.e. alpha=3.603e-01, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,591:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 152 iterations, i.e. alpha=2.805e-01, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,591:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 153 iterations, i.e. alpha=5.502e-02, with an active set of 116 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,592:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 153 iterations, i.e. alpha=5.470e-02, with an active set of 116 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,593:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 154 iterations, i.e. alpha=5.212e-02, with an active set of 117 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,593:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 154 iterations, i.e. alpha=5.187e-02, with an active set of 117 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,594:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 154 iterations, i.e. alpha=4.867e-02, with an active set of 117 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,594:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 154 iterations, i.e. alpha=2.632e-02, with an active set of 117 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,594:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 154 iterations, i.e. alpha=2.370e-02, with an active set of 117 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,596:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 156 iterations, i.e. alpha=9.660e-03, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,596:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 156 iterations, i.e. alpha=9.501e-03, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,597:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 156 iterations, i.e. alpha=8.624e-03, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,597:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 156 iterations, i.e. alpha=7.823e-03, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,597:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 156 iterations, i.e. alpha=7.546e-03, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,598:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 156 iterations, i.e. alpha=6.881e-03, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,598:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 156 iterations, i.e. alpha=6.846e-03, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:47:21,683:INFO:Calculating mean and std
2023-03-06 18:47:21,687:INFO:Creating metrics dataframe
2023-03-06 18:47:21,694:INFO:Uploading results into container
2023-03-06 18:47:21,696:INFO:Uploading model into container now
2023-03-06 18:47:21,696:INFO:_master_model_container: 5
2023-03-06 18:47:21,697:INFO:_display_container: 2
2023-03-06 18:47:21,698:INFO:Lars(random_state=8452)
2023-03-06 18:47:21,699:INFO:create_model() successfully completed......................................
2023-03-06 18:47:21,883:INFO:SubProcess create_model() end ==================================
2023-03-06 18:47:21,884:INFO:Creating metrics dataframe
2023-03-06 18:47:21,912:INFO:Initializing Lasso Least Angle Regression
2023-03-06 18:47:21,912:INFO:Total runtime is 0.09073226849238078 minutes
2023-03-06 18:47:21,919:INFO:SubProcess create_model() called ==================================
2023-03-06 18:47:21,920:INFO:Initializing create_model()
2023-03-06 18:47:21,921:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000205593D4790>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055D40A160>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:47:21,921:INFO:Checking exceptions
2023-03-06 18:47:21,921:INFO:Importing libraries
2023-03-06 18:47:21,921:INFO:Copying training dataset
2023-03-06 18:47:21,932:INFO:Defining folds
2023-03-06 18:47:21,932:INFO:Declaring metric variables
2023-03-06 18:47:21,942:INFO:Importing untrained model
2023-03-06 18:47:21,952:INFO:Lasso Least Angle Regression Imported successfully
2023-03-06 18:47:21,967:INFO:Starting cross validation
2023-03-06 18:47:21,971:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:47:22,165:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:47:22,206:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:47:22,232:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:47:22,261:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:47:22,296:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:47:22,323:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:47:22,335:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:47:22,368:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:47:22,418:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:47:22,460:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:47:22,547:INFO:Calculating mean and std
2023-03-06 18:47:22,551:INFO:Creating metrics dataframe
2023-03-06 18:47:22,561:INFO:Uploading results into container
2023-03-06 18:47:22,563:INFO:Uploading model into container now
2023-03-06 18:47:22,564:INFO:_master_model_container: 6
2023-03-06 18:47:22,564:INFO:_display_container: 2
2023-03-06 18:47:22,565:INFO:LassoLars(random_state=8452)
2023-03-06 18:47:22,566:INFO:create_model() successfully completed......................................
2023-03-06 18:47:22,906:INFO:SubProcess create_model() end ==================================
2023-03-06 18:47:22,907:INFO:Creating metrics dataframe
2023-03-06 18:47:22,964:INFO:Initializing Orthogonal Matching Pursuit
2023-03-06 18:47:22,965:INFO:Total runtime is 0.10829015970230102 minutes
2023-03-06 18:47:22,987:INFO:SubProcess create_model() called ==================================
2023-03-06 18:47:22,988:INFO:Initializing create_model()
2023-03-06 18:47:22,989:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000205593D4790>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055D40A160>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:47:22,990:INFO:Checking exceptions
2023-03-06 18:47:22,991:INFO:Importing libraries
2023-03-06 18:47:22,991:INFO:Copying training dataset
2023-03-06 18:47:23,027:INFO:Defining folds
2023-03-06 18:47:23,028:INFO:Declaring metric variables
2023-03-06 18:47:23,053:INFO:Importing untrained model
2023-03-06 18:47:23,078:INFO:Orthogonal Matching Pursuit Imported successfully
2023-03-06 18:47:23,125:INFO:Starting cross validation
2023-03-06 18:47:23,136:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:47:23,414:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:47:23,415:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:47:23,487:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:47:23,522:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:47:23,542:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:47:23,567:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:47:23,588:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:47:23,617:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:47:23,672:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:47:23,694:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:47:23,846:INFO:Calculating mean and std
2023-03-06 18:47:23,853:INFO:Creating metrics dataframe
2023-03-06 18:47:23,872:INFO:Uploading results into container
2023-03-06 18:47:23,875:INFO:Uploading model into container now
2023-03-06 18:47:23,877:INFO:_master_model_container: 7
2023-03-06 18:47:23,877:INFO:_display_container: 2
2023-03-06 18:47:23,878:INFO:OrthogonalMatchingPursuit()
2023-03-06 18:47:23,879:INFO:create_model() successfully completed......................................
2023-03-06 18:47:24,165:INFO:SubProcess create_model() end ==================================
2023-03-06 18:47:24,166:INFO:Creating metrics dataframe
2023-03-06 18:47:24,200:INFO:Initializing Bayesian Ridge
2023-03-06 18:47:24,201:INFO:Total runtime is 0.12888755003611246 minutes
2023-03-06 18:47:24,213:INFO:SubProcess create_model() called ==================================
2023-03-06 18:47:24,214:INFO:Initializing create_model()
2023-03-06 18:47:24,214:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000205593D4790>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055D40A160>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:47:24,214:INFO:Checking exceptions
2023-03-06 18:47:24,215:INFO:Importing libraries
2023-03-06 18:47:24,215:INFO:Copying training dataset
2023-03-06 18:47:24,233:INFO:Defining folds
2023-03-06 18:47:24,233:INFO:Declaring metric variables
2023-03-06 18:47:24,248:INFO:Importing untrained model
2023-03-06 18:47:24,260:INFO:Bayesian Ridge Imported successfully
2023-03-06 18:47:24,281:INFO:Starting cross validation
2023-03-06 18:47:24,287:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:47:24,998:INFO:Calculating mean and std
2023-03-06 18:47:25,005:INFO:Creating metrics dataframe
2023-03-06 18:47:25,016:INFO:Uploading results into container
2023-03-06 18:47:25,018:INFO:Uploading model into container now
2023-03-06 18:47:25,019:INFO:_master_model_container: 8
2023-03-06 18:47:25,019:INFO:_display_container: 2
2023-03-06 18:47:25,020:INFO:BayesianRidge()
2023-03-06 18:47:25,021:INFO:create_model() successfully completed......................................
2023-03-06 18:47:25,234:INFO:SubProcess create_model() end ==================================
2023-03-06 18:47:25,235:INFO:Creating metrics dataframe
2023-03-06 18:47:25,265:INFO:Initializing Passive Aggressive Regressor
2023-03-06 18:47:25,265:INFO:Total runtime is 0.14661453167597452 minutes
2023-03-06 18:47:25,276:INFO:SubProcess create_model() called ==================================
2023-03-06 18:47:25,277:INFO:Initializing create_model()
2023-03-06 18:47:25,277:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000205593D4790>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055D40A160>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:47:25,278:INFO:Checking exceptions
2023-03-06 18:47:25,279:INFO:Importing libraries
2023-03-06 18:47:25,279:INFO:Copying training dataset
2023-03-06 18:47:25,295:INFO:Defining folds
2023-03-06 18:47:25,295:INFO:Declaring metric variables
2023-03-06 18:47:25,308:INFO:Importing untrained model
2023-03-06 18:47:25,328:INFO:Passive Aggressive Regressor Imported successfully
2023-03-06 18:47:25,353:INFO:Starting cross validation
2023-03-06 18:47:25,361:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:47:25,967:INFO:Calculating mean and std
2023-03-06 18:47:25,971:INFO:Creating metrics dataframe
2023-03-06 18:47:25,979:INFO:Uploading results into container
2023-03-06 18:47:25,980:INFO:Uploading model into container now
2023-03-06 18:47:25,981:INFO:_master_model_container: 9
2023-03-06 18:47:25,981:INFO:_display_container: 2
2023-03-06 18:47:25,982:INFO:PassiveAggressiveRegressor(random_state=8452)
2023-03-06 18:47:25,982:INFO:create_model() successfully completed......................................
2023-03-06 18:47:26,224:INFO:SubProcess create_model() end ==================================
2023-03-06 18:47:26,225:INFO:Creating metrics dataframe
2023-03-06 18:47:26,260:INFO:Initializing Huber Regressor
2023-03-06 18:47:26,261:INFO:Total runtime is 0.16321262915929158 minutes
2023-03-06 18:47:26,273:INFO:SubProcess create_model() called ==================================
2023-03-06 18:47:26,274:INFO:Initializing create_model()
2023-03-06 18:47:26,275:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000205593D4790>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055D40A160>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:47:26,276:INFO:Checking exceptions
2023-03-06 18:47:26,276:INFO:Importing libraries
2023-03-06 18:47:26,277:INFO:Copying training dataset
2023-03-06 18:47:26,293:INFO:Defining folds
2023-03-06 18:47:26,294:INFO:Declaring metric variables
2023-03-06 18:47:26,311:INFO:Importing untrained model
2023-03-06 18:47:26,324:INFO:Huber Regressor Imported successfully
2023-03-06 18:47:26,349:INFO:Starting cross validation
2023-03-06 18:47:26,355:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:47:26,808:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-06 18:47:26,836:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-06 18:47:26,865:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-06 18:47:26,969:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-06 18:47:26,977:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-06 18:47:27,664:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-06 18:47:28,206:INFO:Calculating mean and std
2023-03-06 18:47:28,209:INFO:Creating metrics dataframe
2023-03-06 18:47:28,218:INFO:Uploading results into container
2023-03-06 18:47:28,218:INFO:Uploading model into container now
2023-03-06 18:47:28,220:INFO:_master_model_container: 10
2023-03-06 18:47:28,220:INFO:_display_container: 2
2023-03-06 18:47:28,221:INFO:HuberRegressor()
2023-03-06 18:47:28,221:INFO:create_model() successfully completed......................................
2023-03-06 18:47:28,415:INFO:SubProcess create_model() end ==================================
2023-03-06 18:47:28,416:INFO:Creating metrics dataframe
2023-03-06 18:47:28,444:INFO:Initializing K Neighbors Regressor
2023-03-06 18:47:28,444:INFO:Total runtime is 0.1995931108792623 minutes
2023-03-06 18:47:28,452:INFO:SubProcess create_model() called ==================================
2023-03-06 18:47:28,453:INFO:Initializing create_model()
2023-03-06 18:47:28,453:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000205593D4790>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055D40A160>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:47:28,454:INFO:Checking exceptions
2023-03-06 18:47:28,455:INFO:Importing libraries
2023-03-06 18:47:28,455:INFO:Copying training dataset
2023-03-06 18:47:28,468:INFO:Defining folds
2023-03-06 18:47:28,468:INFO:Declaring metric variables
2023-03-06 18:47:28,480:INFO:Importing untrained model
2023-03-06 18:47:28,492:INFO:K Neighbors Regressor Imported successfully
2023-03-06 18:47:28,512:INFO:Starting cross validation
2023-03-06 18:47:28,517:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:47:29,136:INFO:Calculating mean and std
2023-03-06 18:47:29,141:INFO:Creating metrics dataframe
2023-03-06 18:47:29,153:INFO:Uploading results into container
2023-03-06 18:47:29,155:INFO:Uploading model into container now
2023-03-06 18:47:29,157:INFO:_master_model_container: 11
2023-03-06 18:47:29,157:INFO:_display_container: 2
2023-03-06 18:47:29,158:INFO:KNeighborsRegressor(n_jobs=-1)
2023-03-06 18:47:29,159:INFO:create_model() successfully completed......................................
2023-03-06 18:47:29,423:INFO:SubProcess create_model() end ==================================
2023-03-06 18:47:29,423:INFO:Creating metrics dataframe
2023-03-06 18:47:29,450:INFO:Initializing Decision Tree Regressor
2023-03-06 18:47:29,450:INFO:Total runtime is 0.21637060244878134 minutes
2023-03-06 18:47:29,459:INFO:SubProcess create_model() called ==================================
2023-03-06 18:47:29,460:INFO:Initializing create_model()
2023-03-06 18:47:29,461:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000205593D4790>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055D40A160>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:47:29,461:INFO:Checking exceptions
2023-03-06 18:47:29,461:INFO:Importing libraries
2023-03-06 18:47:29,461:INFO:Copying training dataset
2023-03-06 18:47:29,472:INFO:Defining folds
2023-03-06 18:47:29,473:INFO:Declaring metric variables
2023-03-06 18:47:29,484:INFO:Importing untrained model
2023-03-06 18:47:29,494:INFO:Decision Tree Regressor Imported successfully
2023-03-06 18:47:29,515:INFO:Starting cross validation
2023-03-06 18:47:29,520:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:47:30,147:INFO:Calculating mean and std
2023-03-06 18:47:30,151:INFO:Creating metrics dataframe
2023-03-06 18:47:30,160:INFO:Uploading results into container
2023-03-06 18:47:30,161:INFO:Uploading model into container now
2023-03-06 18:47:30,162:INFO:_master_model_container: 12
2023-03-06 18:47:30,162:INFO:_display_container: 2
2023-03-06 18:47:30,163:INFO:DecisionTreeRegressor(random_state=8452)
2023-03-06 18:47:30,163:INFO:create_model() successfully completed......................................
2023-03-06 18:47:30,392:INFO:SubProcess create_model() end ==================================
2023-03-06 18:47:30,392:INFO:Creating metrics dataframe
2023-03-06 18:47:30,420:INFO:Initializing Random Forest Regressor
2023-03-06 18:47:30,420:INFO:Total runtime is 0.23252739906311037 minutes
2023-03-06 18:47:30,430:INFO:SubProcess create_model() called ==================================
2023-03-06 18:47:30,431:INFO:Initializing create_model()
2023-03-06 18:47:30,431:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000205593D4790>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055D40A160>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:47:30,431:INFO:Checking exceptions
2023-03-06 18:47:30,432:INFO:Importing libraries
2023-03-06 18:47:30,432:INFO:Copying training dataset
2023-03-06 18:47:30,447:INFO:Defining folds
2023-03-06 18:47:30,447:INFO:Declaring metric variables
2023-03-06 18:47:30,458:INFO:Importing untrained model
2023-03-06 18:47:30,473:INFO:Random Forest Regressor Imported successfully
2023-03-06 18:47:30,498:INFO:Starting cross validation
2023-03-06 18:47:30,503:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:47:32,317:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:47:32,327:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:47:32,344:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:47:32,350:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:47:32,401:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:47:32,436:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:47:32,494:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:47:32,500:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:47:34,029:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:47:34,034:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:47:34,373:INFO:Calculating mean and std
2023-03-06 18:47:34,377:INFO:Creating metrics dataframe
2023-03-06 18:47:34,385:INFO:Uploading results into container
2023-03-06 18:47:34,386:INFO:Uploading model into container now
2023-03-06 18:47:34,387:INFO:_master_model_container: 13
2023-03-06 18:47:34,387:INFO:_display_container: 2
2023-03-06 18:47:34,388:INFO:RandomForestRegressor(n_jobs=-1, random_state=8452)
2023-03-06 18:47:34,388:INFO:create_model() successfully completed......................................
2023-03-06 18:47:34,584:INFO:SubProcess create_model() end ==================================
2023-03-06 18:47:34,584:INFO:Creating metrics dataframe
2023-03-06 18:47:34,615:INFO:Initializing Extra Trees Regressor
2023-03-06 18:47:34,615:INFO:Total runtime is 0.3024439851442973 minutes
2023-03-06 18:47:34,625:INFO:SubProcess create_model() called ==================================
2023-03-06 18:47:34,626:INFO:Initializing create_model()
2023-03-06 18:47:34,626:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000205593D4790>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055D40A160>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:47:34,626:INFO:Checking exceptions
2023-03-06 18:47:34,626:INFO:Importing libraries
2023-03-06 18:47:34,627:INFO:Copying training dataset
2023-03-06 18:47:34,640:INFO:Defining folds
2023-03-06 18:47:34,641:INFO:Declaring metric variables
2023-03-06 18:47:34,650:INFO:Importing untrained model
2023-03-06 18:47:34,660:INFO:Extra Trees Regressor Imported successfully
2023-03-06 18:47:34,676:INFO:Starting cross validation
2023-03-06 18:47:34,680:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:47:36,557:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:47:36,565:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:47:36,589:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:47:36,617:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:47:36,625:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:47:36,652:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:47:36,709:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:47:38,261:INFO:Calculating mean and std
2023-03-06 18:47:38,264:INFO:Creating metrics dataframe
2023-03-06 18:47:38,272:INFO:Uploading results into container
2023-03-06 18:47:38,273:INFO:Uploading model into container now
2023-03-06 18:47:38,274:INFO:_master_model_container: 14
2023-03-06 18:47:38,274:INFO:_display_container: 2
2023-03-06 18:47:38,275:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=8452)
2023-03-06 18:47:38,276:INFO:create_model() successfully completed......................................
2023-03-06 18:47:38,469:INFO:SubProcess create_model() end ==================================
2023-03-06 18:47:38,469:INFO:Creating metrics dataframe
2023-03-06 18:47:38,500:INFO:Initializing AdaBoost Regressor
2023-03-06 18:47:38,501:INFO:Total runtime is 0.3672056953112285 minutes
2023-03-06 18:47:38,511:INFO:SubProcess create_model() called ==================================
2023-03-06 18:47:38,511:INFO:Initializing create_model()
2023-03-06 18:47:38,512:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000205593D4790>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055D40A160>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:47:38,512:INFO:Checking exceptions
2023-03-06 18:47:38,513:INFO:Importing libraries
2023-03-06 18:47:38,513:INFO:Copying training dataset
2023-03-06 18:47:38,528:INFO:Defining folds
2023-03-06 18:47:38,528:INFO:Declaring metric variables
2023-03-06 18:47:38,539:INFO:Importing untrained model
2023-03-06 18:47:38,550:INFO:AdaBoost Regressor Imported successfully
2023-03-06 18:47:38,571:INFO:Starting cross validation
2023-03-06 18:47:38,577:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:47:40,020:INFO:Calculating mean and std
2023-03-06 18:47:40,024:INFO:Creating metrics dataframe
2023-03-06 18:47:40,032:INFO:Uploading results into container
2023-03-06 18:47:40,033:INFO:Uploading model into container now
2023-03-06 18:47:40,034:INFO:_master_model_container: 15
2023-03-06 18:47:40,035:INFO:_display_container: 2
2023-03-06 18:47:40,035:INFO:AdaBoostRegressor(random_state=8452)
2023-03-06 18:47:40,036:INFO:create_model() successfully completed......................................
2023-03-06 18:47:40,225:INFO:SubProcess create_model() end ==================================
2023-03-06 18:47:40,225:INFO:Creating metrics dataframe
2023-03-06 18:47:40,258:INFO:Initializing Gradient Boosting Regressor
2023-03-06 18:47:40,258:INFO:Total runtime is 0.3964912295341492 minutes
2023-03-06 18:47:40,268:INFO:SubProcess create_model() called ==================================
2023-03-06 18:47:40,268:INFO:Initializing create_model()
2023-03-06 18:47:40,268:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000205593D4790>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055D40A160>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:47:40,269:INFO:Checking exceptions
2023-03-06 18:47:40,270:INFO:Importing libraries
2023-03-06 18:47:40,270:INFO:Copying training dataset
2023-03-06 18:47:40,284:INFO:Defining folds
2023-03-06 18:47:40,284:INFO:Declaring metric variables
2023-03-06 18:47:40,294:INFO:Importing untrained model
2023-03-06 18:47:40,306:INFO:Gradient Boosting Regressor Imported successfully
2023-03-06 18:47:40,331:INFO:Starting cross validation
2023-03-06 18:47:40,334:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:47:41,452:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:47:41,514:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:47:41,548:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:47:41,553:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:47:41,585:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:47:41,594:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:47:41,596:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:47:41,651:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:47:42,742:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:47:42,785:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:47:43,015:INFO:Calculating mean and std
2023-03-06 18:47:43,019:INFO:Creating metrics dataframe
2023-03-06 18:47:43,026:INFO:Uploading results into container
2023-03-06 18:47:43,028:INFO:Uploading model into container now
2023-03-06 18:47:43,029:INFO:_master_model_container: 16
2023-03-06 18:47:43,029:INFO:_display_container: 2
2023-03-06 18:47:43,031:INFO:GradientBoostingRegressor(random_state=8452)
2023-03-06 18:47:43,031:INFO:create_model() successfully completed......................................
2023-03-06 18:47:43,223:INFO:SubProcess create_model() end ==================================
2023-03-06 18:47:43,223:INFO:Creating metrics dataframe
2023-03-06 18:47:43,253:INFO:Initializing Extreme Gradient Boosting
2023-03-06 18:47:43,254:INFO:Total runtime is 0.446427607536316 minutes
2023-03-06 18:47:43,262:INFO:SubProcess create_model() called ==================================
2023-03-06 18:47:43,263:INFO:Initializing create_model()
2023-03-06 18:47:43,263:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000205593D4790>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055D40A160>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:47:43,263:INFO:Checking exceptions
2023-03-06 18:47:43,264:INFO:Importing libraries
2023-03-06 18:47:43,264:INFO:Copying training dataset
2023-03-06 18:47:43,279:INFO:Defining folds
2023-03-06 18:47:43,280:INFO:Declaring metric variables
2023-03-06 18:47:43,289:INFO:Importing untrained model
2023-03-06 18:47:43,302:INFO:Extreme Gradient Boosting Imported successfully
2023-03-06 18:47:43,319:INFO:Starting cross validation
2023-03-06 18:47:43,323:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:47:46,500:INFO:Calculating mean and std
2023-03-06 18:47:46,505:INFO:Creating metrics dataframe
2023-03-06 18:47:46,513:INFO:Uploading results into container
2023-03-06 18:47:46,514:INFO:Uploading model into container now
2023-03-06 18:47:46,516:INFO:_master_model_container: 17
2023-03-06 18:47:46,516:INFO:_display_container: 2
2023-03-06 18:47:46,520:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=8452, ...)
2023-03-06 18:47:46,521:INFO:create_model() successfully completed......................................
2023-03-06 18:47:46,716:INFO:SubProcess create_model() end ==================================
2023-03-06 18:47:46,717:INFO:Creating metrics dataframe
2023-03-06 18:47:46,747:INFO:Initializing Light Gradient Boosting Machine
2023-03-06 18:47:46,747:INFO:Total runtime is 0.5046565413475037 minutes
2023-03-06 18:47:46,757:INFO:SubProcess create_model() called ==================================
2023-03-06 18:47:46,758:INFO:Initializing create_model()
2023-03-06 18:47:46,759:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000205593D4790>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055D40A160>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:47:46,759:INFO:Checking exceptions
2023-03-06 18:47:46,760:INFO:Importing libraries
2023-03-06 18:47:46,760:INFO:Copying training dataset
2023-03-06 18:47:46,777:INFO:Defining folds
2023-03-06 18:47:46,778:INFO:Declaring metric variables
2023-03-06 18:47:46,792:INFO:Importing untrained model
2023-03-06 18:47:46,806:INFO:Light Gradient Boosting Machine Imported successfully
2023-03-06 18:47:46,825:INFO:Starting cross validation
2023-03-06 18:47:46,829:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:47:47,556:INFO:Calculating mean and std
2023-03-06 18:47:47,560:INFO:Creating metrics dataframe
2023-03-06 18:47:47,568:INFO:Uploading results into container
2023-03-06 18:47:47,569:INFO:Uploading model into container now
2023-03-06 18:47:47,570:INFO:_master_model_container: 18
2023-03-06 18:47:47,570:INFO:_display_container: 2
2023-03-06 18:47:47,571:INFO:LGBMRegressor(random_state=8452)
2023-03-06 18:47:47,572:INFO:create_model() successfully completed......................................
2023-03-06 18:47:47,813:INFO:SubProcess create_model() end ==================================
2023-03-06 18:47:47,813:INFO:Creating metrics dataframe
2023-03-06 18:47:47,848:INFO:Initializing Dummy Regressor
2023-03-06 18:47:47,848:INFO:Total runtime is 0.522997542222341 minutes
2023-03-06 18:47:47,857:INFO:SubProcess create_model() called ==================================
2023-03-06 18:47:47,858:INFO:Initializing create_model()
2023-03-06 18:47:47,858:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000205593D4790>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055D40A160>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:47:47,858:INFO:Checking exceptions
2023-03-06 18:47:47,859:INFO:Importing libraries
2023-03-06 18:47:47,859:INFO:Copying training dataset
2023-03-06 18:47:47,872:INFO:Defining folds
2023-03-06 18:47:47,873:INFO:Declaring metric variables
2023-03-06 18:47:47,884:INFO:Importing untrained model
2023-03-06 18:47:47,895:INFO:Dummy Regressor Imported successfully
2023-03-06 18:47:47,919:INFO:Starting cross validation
2023-03-06 18:47:47,923:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:47:48,481:INFO:Calculating mean and std
2023-03-06 18:47:48,487:INFO:Creating metrics dataframe
2023-03-06 18:47:48,501:INFO:Uploading results into container
2023-03-06 18:47:48,504:INFO:Uploading model into container now
2023-03-06 18:47:48,505:INFO:_master_model_container: 19
2023-03-06 18:47:48,506:INFO:_display_container: 2
2023-03-06 18:47:48,506:INFO:DummyRegressor()
2023-03-06 18:47:48,507:INFO:create_model() successfully completed......................................
2023-03-06 18:47:48,776:INFO:SubProcess create_model() end ==================================
2023-03-06 18:47:48,777:INFO:Creating metrics dataframe
2023-03-06 18:47:48,835:INFO:Initializing create_model()
2023-03-06 18:47:48,835:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000205593D4790>, estimator=GradientBoostingRegressor(random_state=8452), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:47:48,837:INFO:Checking exceptions
2023-03-06 18:47:48,841:INFO:Importing libraries
2023-03-06 18:47:48,842:INFO:Copying training dataset
2023-03-06 18:47:48,851:INFO:Defining folds
2023-03-06 18:47:48,852:INFO:Declaring metric variables
2023-03-06 18:47:48,852:INFO:Importing untrained model
2023-03-06 18:47:48,853:INFO:Declaring custom model
2023-03-06 18:47:48,855:INFO:Gradient Boosting Regressor Imported successfully
2023-03-06 18:47:48,858:INFO:Cross validation set to False
2023-03-06 18:47:48,858:INFO:Fitting Model
2023-03-06 18:47:49,516:INFO:GradientBoostingRegressor(random_state=8452)
2023-03-06 18:47:49,516:INFO:create_model() successfully completed......................................
2023-03-06 18:47:49,809:INFO:_master_model_container: 19
2023-03-06 18:47:49,809:INFO:_display_container: 2
2023-03-06 18:47:49,810:INFO:GradientBoostingRegressor(random_state=8452)
2023-03-06 18:47:49,810:INFO:compare_models() successfully completed......................................
2023-03-06 18:47:49,829:INFO:Initializing evaluate_model()
2023-03-06 18:47:49,829:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000205593D4790>, estimator=GradientBoostingRegressor(random_state=8452), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-03-06 18:47:49,892:INFO:Initializing plot_model()
2023-03-06 18:47:49,893:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=8452), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x00000205593D4790>, system=True)
2023-03-06 18:47:49,893:INFO:Checking exceptions
2023-03-06 18:47:49,901:INFO:Preloading libraries
2023-03-06 18:47:49,926:INFO:Copying training dataset
2023-03-06 18:47:49,926:INFO:Plot type: pipeline
2023-03-06 18:47:50,115:INFO:Visual Rendered Successfully
2023-03-06 18:47:50,306:INFO:plot_model() successfully completed......................................
2023-03-06 18:48:29,679:INFO:PyCaret RegressionExperiment
2023-03-06 18:48:29,680:INFO:Logging name: reg-default-name
2023-03-06 18:48:29,680:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-06 18:48:29,680:INFO:version 3.0.0.rc9
2023-03-06 18:48:29,680:INFO:Initializing setup()
2023-03-06 18:48:29,681:INFO:self.USI: fe41
2023-03-06 18:48:29,682:INFO:self._variable_keys: {'_available_plots', 'fold_groups_param', 'fold_generator', 'pipeline', 'USI', 'logging_param', 'X', '_ml_usecase', 'idx', 'exp_id', 'fold_shuffle_param', 'X_train', 'exp_name_log', 'y_test', 'transform_target_param', 'y_train', 'seed', 'data', 'log_plots_param', 'gpu_n_jobs_param', 'target_param', 'n_jobs_param', 'X_test', 'memory', 'y', 'html_param', 'gpu_param'}
2023-03-06 18:48:29,682:INFO:Checking environment
2023-03-06 18:48:29,682:INFO:python_version: 3.9.13
2023-03-06 18:48:29,682:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-03-06 18:48:29,682:INFO:machine: AMD64
2023-03-06 18:48:29,682:INFO:platform: Windows-10-10.0.19044-SP0
2023-03-06 18:48:29,682:INFO:Memory: svmem(total=17009516544, available=4726845440, percent=72.2, used=12282671104, free=4726845440)
2023-03-06 18:48:29,682:INFO:Physical Core: 4
2023-03-06 18:48:29,682:INFO:Logical Core: 8
2023-03-06 18:48:29,683:INFO:Checking libraries
2023-03-06 18:48:29,683:INFO:System:
2023-03-06 18:48:29,683:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-03-06 18:48:29,683:INFO:executable: C:\Users\Alvaro\anaconda3\python.exe
2023-03-06 18:48:29,683:INFO:   machine: Windows-10-10.0.19044-SP0
2023-03-06 18:48:29,683:INFO:PyCaret required dependencies:
2023-03-06 18:48:29,683:INFO:                 pip: 22.2.2
2023-03-06 18:48:29,683:INFO:          setuptools: 63.4.1
2023-03-06 18:48:29,683:INFO:             pycaret: 3.0.0rc9
2023-03-06 18:48:29,684:INFO:             IPython: 7.31.1
2023-03-06 18:48:29,684:INFO:          ipywidgets: 7.6.5
2023-03-06 18:48:29,684:INFO:                tqdm: 4.64.1
2023-03-06 18:48:29,684:INFO:               numpy: 1.21.5
2023-03-06 18:48:29,684:INFO:              pandas: 1.4.4
2023-03-06 18:48:29,684:INFO:              jinja2: 2.11.3
2023-03-06 18:48:29,685:INFO:               scipy: 1.9.1
2023-03-06 18:48:29,685:INFO:              joblib: 1.2.0
2023-03-06 18:48:29,685:INFO:             sklearn: 1.0.2
2023-03-06 18:48:29,685:INFO:                pyod: 1.0.7
2023-03-06 18:48:29,685:INFO:            imblearn: 0.10.1
2023-03-06 18:48:29,685:INFO:   category_encoders: 2.6.0
2023-03-06 18:48:29,685:INFO:            lightgbm: 3.3.5
2023-03-06 18:48:29,687:INFO:               numba: 0.55.1
2023-03-06 18:48:29,687:INFO:            requests: 2.28.1
2023-03-06 18:48:29,687:INFO:          matplotlib: 3.5.2
2023-03-06 18:48:29,687:INFO:          scikitplot: 0.3.7
2023-03-06 18:48:29,687:INFO:         yellowbrick: 1.5
2023-03-06 18:48:29,687:INFO:              plotly: 5.9.0
2023-03-06 18:48:29,687:INFO:             kaleido: 0.2.1
2023-03-06 18:48:29,687:INFO:         statsmodels: 0.13.2
2023-03-06 18:48:29,687:INFO:              sktime: 0.16.1
2023-03-06 18:48:29,687:INFO:               tbats: 1.1.2
2023-03-06 18:48:29,687:INFO:            pmdarima: 2.0.2
2023-03-06 18:48:29,687:INFO:              psutil: 5.9.0
2023-03-06 18:48:29,687:INFO:PyCaret optional dependencies:
2023-03-06 18:48:29,687:INFO:                shap: Not installed
2023-03-06 18:48:29,687:INFO:           interpret: Not installed
2023-03-06 18:48:29,687:INFO:                umap: Not installed
2023-03-06 18:48:29,688:INFO:    pandas_profiling: Not installed
2023-03-06 18:48:29,688:INFO:  explainerdashboard: Not installed
2023-03-06 18:48:29,688:INFO:             autoviz: Not installed
2023-03-06 18:48:29,688:INFO:           fairlearn: Not installed
2023-03-06 18:48:29,688:INFO:             xgboost: 1.7.4
2023-03-06 18:48:29,688:INFO:            catboost: Not installed
2023-03-06 18:48:29,688:INFO:              kmodes: Not installed
2023-03-06 18:48:29,688:INFO:             mlxtend: Not installed
2023-03-06 18:48:29,689:INFO:       statsforecast: Not installed
2023-03-06 18:48:29,689:INFO:        tune_sklearn: Not installed
2023-03-06 18:48:29,689:INFO:                 ray: Not installed
2023-03-06 18:48:29,689:INFO:            hyperopt: Not installed
2023-03-06 18:48:29,689:INFO:              optuna: Not installed
2023-03-06 18:48:29,689:INFO:               skopt: Not installed
2023-03-06 18:48:29,689:INFO:              mlflow: Not installed
2023-03-06 18:48:29,689:INFO:              gradio: Not installed
2023-03-06 18:48:29,689:INFO:             fastapi: Not installed
2023-03-06 18:48:29,689:INFO:             uvicorn: Not installed
2023-03-06 18:48:29,690:INFO:              m2cgen: Not installed
2023-03-06 18:48:29,690:INFO:           evidently: Not installed
2023-03-06 18:48:29,690:INFO:               fugue: Not installed
2023-03-06 18:48:29,691:INFO:           streamlit: Not installed
2023-03-06 18:48:29,691:INFO:             prophet: Not installed
2023-03-06 18:48:29,691:INFO:None
2023-03-06 18:48:29,691:INFO:Set up data.
2023-03-06 18:48:29,706:INFO:Set up train/test split.
2023-03-06 18:48:29,717:INFO:Set up index.
2023-03-06 18:48:29,719:INFO:Set up folding strategy.
2023-03-06 18:48:29,719:INFO:Assigning column types.
2023-03-06 18:48:29,726:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-06 18:48:29,726:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-06 18:48:29,739:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-06 18:48:29,751:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-06 18:48:29,900:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:48:30,016:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 18:48:30,018:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:48:30,024:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:48:30,025:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-06 18:48:30,037:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-06 18:48:30,049:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-06 18:48:30,197:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:48:30,313:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 18:48:30,315:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:48:30,322:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:48:30,322:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-03-06 18:48:30,335:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-06 18:48:30,346:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-06 18:48:30,495:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:48:30,611:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 18:48:30,613:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:48:30,619:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:48:30,632:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-06 18:48:30,644:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-06 18:48:30,794:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:48:30,916:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 18:48:30,917:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:48:30,924:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:48:30,925:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-03-06 18:48:30,949:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-06 18:48:31,098:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:48:31,216:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 18:48:31,217:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:48:31,224:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:48:31,248:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-06 18:48:31,396:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:48:31,510:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 18:48:31,512:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:48:31,518:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:48:31,519:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-03-06 18:48:31,691:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:48:31,807:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 18:48:31,809:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:48:31,816:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:48:31,990:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:48:32,103:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 18:48:32,104:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:48:32,111:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:48:32,112:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-06 18:48:32,279:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:48:32,393:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:48:32,399:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:48:32,567:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:48:32,664:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:48:32,670:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:48:32,671:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-03-06 18:48:32,950:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:48:32,956:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:48:33,256:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:48:33,262:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:48:33,265:INFO:Preparing preprocessing pipeline...
2023-03-06 18:48:33,267:INFO:Set up simple imputation.
2023-03-06 18:48:33,273:INFO:Set up encoding of categorical features.
2023-03-06 18:48:33,524:INFO:Finished creating preprocessing pipeline.
2023-03-06 18:48:33,548:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Alvaro\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['work_year', 'remote_ratio'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['experience_level',
                                             'employment_type', 'job_title',
                                             'employee_residence',
                                             'company_location',
                                             'company_size'],
                                    t...
                                    transformer=OneHotEncoder(cols=['experience_level',
                                                                    'employment_type',
                                                                    'company_size'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['job_title', 'employee_residence',
                                             'company_location'],
                                    transformer=LeaveOneOutEncoder(cols=['job_title',
                                                                         'employee_residence',
                                                                         'company_location'],
                                                                   handle_missing='return_nan',
                                                                   random_state=7742)))])
2023-03-06 18:48:33,548:INFO:Creating final display dataframe.
2023-03-06 18:48:34,451:INFO:Setup _display_container:                     Description             Value
0                    Session id              7742
1                        Target     salary_in_usd
2                   Target type        Regression
3           Original data shape          (500, 9)
4        Transformed data shape         (500, 17)
5   Transformed train set shape         (350, 17)
6    Transformed test set shape         (150, 17)
7              Numeric features                 2
8          Categorical features                 6
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator             KFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  reg-default-name
21                          USI              fe41
2023-03-06 18:48:34,757:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:48:34,764:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:48:35,044:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:48:35,051:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:48:35,052:INFO:setup() successfully completed in 5.4s...............
2023-03-06 18:48:35,077:INFO:Initializing compare_models()
2023-03-06 18:48:35,078:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055EC42790>, include=None, fold=None, round=4, cross_validation=True, sort=RMSE, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000002055EC42790>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'RMSE', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-03-06 18:48:35,078:INFO:Checking exceptions
2023-03-06 18:48:35,084:INFO:Preparing display monitor
2023-03-06 18:48:35,189:INFO:Initializing Linear Regression
2023-03-06 18:48:35,190:INFO:Total runtime is 1.6550223032633465e-05 minutes
2023-03-06 18:48:35,200:INFO:SubProcess create_model() called ==================================
2023-03-06 18:48:35,202:INFO:Initializing create_model()
2023-03-06 18:48:35,203:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055EC42790>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205600A2CD0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:48:35,203:INFO:Checking exceptions
2023-03-06 18:48:35,203:INFO:Importing libraries
2023-03-06 18:48:35,203:INFO:Copying training dataset
2023-03-06 18:48:35,215:INFO:Defining folds
2023-03-06 18:48:35,216:INFO:Declaring metric variables
2023-03-06 18:48:35,223:INFO:Importing untrained model
2023-03-06 18:48:35,232:INFO:Linear Regression Imported successfully
2023-03-06 18:48:35,247:INFO:Starting cross validation
2023-03-06 18:48:35,252:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:48:37,239:INFO:Calculating mean and std
2023-03-06 18:48:37,242:INFO:Creating metrics dataframe
2023-03-06 18:48:37,252:INFO:Uploading results into container
2023-03-06 18:48:37,253:INFO:Uploading model into container now
2023-03-06 18:48:37,255:INFO:_master_model_container: 1
2023-03-06 18:48:37,255:INFO:_display_container: 2
2023-03-06 18:48:37,256:INFO:LinearRegression(n_jobs=-1)
2023-03-06 18:48:37,257:INFO:create_model() successfully completed......................................
2023-03-06 18:48:37,453:INFO:SubProcess create_model() end ==================================
2023-03-06 18:48:37,454:INFO:Creating metrics dataframe
2023-03-06 18:48:37,472:INFO:Initializing Lasso Regression
2023-03-06 18:48:37,473:INFO:Total runtime is 0.038054366906483963 minutes
2023-03-06 18:48:37,481:INFO:SubProcess create_model() called ==================================
2023-03-06 18:48:37,482:INFO:Initializing create_model()
2023-03-06 18:48:37,482:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055EC42790>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205600A2CD0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:48:37,483:INFO:Checking exceptions
2023-03-06 18:48:37,483:INFO:Importing libraries
2023-03-06 18:48:37,484:INFO:Copying training dataset
2023-03-06 18:48:37,498:INFO:Defining folds
2023-03-06 18:48:37,498:INFO:Declaring metric variables
2023-03-06 18:48:37,508:INFO:Importing untrained model
2023-03-06 18:48:37,521:INFO:Lasso Regression Imported successfully
2023-03-06 18:48:37,540:INFO:Starting cross validation
2023-03-06 18:48:37,544:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:48:39,828:INFO:Calculating mean and std
2023-03-06 18:48:39,834:INFO:Creating metrics dataframe
2023-03-06 18:48:39,847:INFO:Uploading results into container
2023-03-06 18:48:39,849:INFO:Uploading model into container now
2023-03-06 18:48:39,850:INFO:_master_model_container: 2
2023-03-06 18:48:39,851:INFO:_display_container: 2
2023-03-06 18:48:39,852:INFO:Lasso(random_state=7742)
2023-03-06 18:48:39,853:INFO:create_model() successfully completed......................................
2023-03-06 18:48:40,064:INFO:SubProcess create_model() end ==================================
2023-03-06 18:48:40,064:INFO:Creating metrics dataframe
2023-03-06 18:48:40,085:INFO:Initializing Ridge Regression
2023-03-06 18:48:40,086:INFO:Total runtime is 0.08161798715591431 minutes
2023-03-06 18:48:40,095:INFO:SubProcess create_model() called ==================================
2023-03-06 18:48:40,095:INFO:Initializing create_model()
2023-03-06 18:48:40,095:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055EC42790>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205600A2CD0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:48:40,097:INFO:Checking exceptions
2023-03-06 18:48:40,097:INFO:Importing libraries
2023-03-06 18:48:40,097:INFO:Copying training dataset
2023-03-06 18:48:40,110:INFO:Defining folds
2023-03-06 18:48:40,111:INFO:Declaring metric variables
2023-03-06 18:48:40,122:INFO:Importing untrained model
2023-03-06 18:48:40,134:INFO:Ridge Regression Imported successfully
2023-03-06 18:48:40,152:INFO:Starting cross validation
2023-03-06 18:48:40,156:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:48:41,968:INFO:Calculating mean and std
2023-03-06 18:48:41,972:INFO:Creating metrics dataframe
2023-03-06 18:48:41,981:INFO:Uploading results into container
2023-03-06 18:48:41,982:INFO:Uploading model into container now
2023-03-06 18:48:41,983:INFO:_master_model_container: 3
2023-03-06 18:48:41,984:INFO:_display_container: 2
2023-03-06 18:48:41,985:INFO:Ridge(random_state=7742)
2023-03-06 18:48:41,985:INFO:create_model() successfully completed......................................
2023-03-06 18:48:42,186:INFO:SubProcess create_model() end ==================================
2023-03-06 18:48:42,187:INFO:Creating metrics dataframe
2023-03-06 18:48:42,214:INFO:Initializing Elastic Net
2023-03-06 18:48:42,215:INFO:Total runtime is 0.1170921802520752 minutes
2023-03-06 18:48:42,228:INFO:SubProcess create_model() called ==================================
2023-03-06 18:48:42,229:INFO:Initializing create_model()
2023-03-06 18:48:42,230:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055EC42790>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205600A2CD0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:48:42,231:INFO:Checking exceptions
2023-03-06 18:48:42,231:INFO:Importing libraries
2023-03-06 18:48:42,232:INFO:Copying training dataset
2023-03-06 18:48:42,253:INFO:Defining folds
2023-03-06 18:48:42,253:INFO:Declaring metric variables
2023-03-06 18:48:42,265:INFO:Importing untrained model
2023-03-06 18:48:42,273:INFO:Elastic Net Imported successfully
2023-03-06 18:48:42,290:INFO:Starting cross validation
2023-03-06 18:48:42,295:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:48:44,064:INFO:Calculating mean and std
2023-03-06 18:48:44,067:INFO:Creating metrics dataframe
2023-03-06 18:48:44,075:INFO:Uploading results into container
2023-03-06 18:48:44,076:INFO:Uploading model into container now
2023-03-06 18:48:44,077:INFO:_master_model_container: 4
2023-03-06 18:48:44,078:INFO:_display_container: 2
2023-03-06 18:48:44,079:INFO:ElasticNet(random_state=7742)
2023-03-06 18:48:44,079:INFO:create_model() successfully completed......................................
2023-03-06 18:48:44,279:INFO:SubProcess create_model() end ==================================
2023-03-06 18:48:44,279:INFO:Creating metrics dataframe
2023-03-06 18:48:44,305:INFO:Initializing Least Angle Regression
2023-03-06 18:48:44,305:INFO:Total runtime is 0.15192523797353108 minutes
2023-03-06 18:48:44,315:INFO:SubProcess create_model() called ==================================
2023-03-06 18:48:44,316:INFO:Initializing create_model()
2023-03-06 18:48:44,316:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055EC42790>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205600A2CD0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:48:44,317:INFO:Checking exceptions
2023-03-06 18:48:44,317:INFO:Importing libraries
2023-03-06 18:48:44,317:INFO:Copying training dataset
2023-03-06 18:48:44,331:INFO:Defining folds
2023-03-06 18:48:44,332:INFO:Declaring metric variables
2023-03-06 18:48:44,343:INFO:Importing untrained model
2023-03-06 18:48:44,356:INFO:Least Angle Regression Imported successfully
2023-03-06 18:48:44,377:INFO:Starting cross validation
2023-03-06 18:48:44,384:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:48:44,881:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:48:45,017:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:48:45,043:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:48:45,064:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:48:45,131:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:48:45,136:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:48:45,150:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:48:45,166:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:48:45,776:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:48:45,813:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:48:46,268:INFO:Calculating mean and std
2023-03-06 18:48:46,271:INFO:Creating metrics dataframe
2023-03-06 18:48:46,279:INFO:Uploading results into container
2023-03-06 18:48:46,280:INFO:Uploading model into container now
2023-03-06 18:48:46,281:INFO:_master_model_container: 5
2023-03-06 18:48:46,282:INFO:_display_container: 2
2023-03-06 18:48:46,283:INFO:Lars(random_state=7742)
2023-03-06 18:48:46,283:INFO:create_model() successfully completed......................................
2023-03-06 18:48:46,486:INFO:SubProcess create_model() end ==================================
2023-03-06 18:48:46,486:INFO:Creating metrics dataframe
2023-03-06 18:48:46,517:INFO:Initializing Lasso Least Angle Regression
2023-03-06 18:48:46,518:INFO:Total runtime is 0.18880577484766642 minutes
2023-03-06 18:48:46,530:INFO:SubProcess create_model() called ==================================
2023-03-06 18:48:46,531:INFO:Initializing create_model()
2023-03-06 18:48:46,531:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055EC42790>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205600A2CD0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:48:46,532:INFO:Checking exceptions
2023-03-06 18:48:46,532:INFO:Importing libraries
2023-03-06 18:48:46,532:INFO:Copying training dataset
2023-03-06 18:48:46,550:INFO:Defining folds
2023-03-06 18:48:46,551:INFO:Declaring metric variables
2023-03-06 18:48:46,564:INFO:Importing untrained model
2023-03-06 18:48:46,576:INFO:Lasso Least Angle Regression Imported successfully
2023-03-06 18:48:46,601:INFO:Starting cross validation
2023-03-06 18:48:46,606:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:48:47,142:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:48:47,203:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:48:47,223:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:48:47,321:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:48:47,334:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:48:47,348:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:48:47,399:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:48:47,461:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:48:48,056:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:48:48,078:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:48:48,511:INFO:Calculating mean and std
2023-03-06 18:48:48,514:INFO:Creating metrics dataframe
2023-03-06 18:48:48,523:INFO:Uploading results into container
2023-03-06 18:48:48,525:INFO:Uploading model into container now
2023-03-06 18:48:48,526:INFO:_master_model_container: 6
2023-03-06 18:48:48,526:INFO:_display_container: 2
2023-03-06 18:48:48,527:INFO:LassoLars(random_state=7742)
2023-03-06 18:48:48,528:INFO:create_model() successfully completed......................................
2023-03-06 18:48:48,724:INFO:SubProcess create_model() end ==================================
2023-03-06 18:48:48,724:INFO:Creating metrics dataframe
2023-03-06 18:48:48,748:INFO:Initializing Orthogonal Matching Pursuit
2023-03-06 18:48:48,748:INFO:Total runtime is 0.22598141431808472 minutes
2023-03-06 18:48:48,755:INFO:SubProcess create_model() called ==================================
2023-03-06 18:48:48,756:INFO:Initializing create_model()
2023-03-06 18:48:48,757:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055EC42790>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205600A2CD0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:48:48,758:INFO:Checking exceptions
2023-03-06 18:48:48,758:INFO:Importing libraries
2023-03-06 18:48:48,759:INFO:Copying training dataset
2023-03-06 18:48:48,771:INFO:Defining folds
2023-03-06 18:48:48,771:INFO:Declaring metric variables
2023-03-06 18:48:48,782:INFO:Importing untrained model
2023-03-06 18:48:48,794:INFO:Orthogonal Matching Pursuit Imported successfully
2023-03-06 18:48:48,813:INFO:Starting cross validation
2023-03-06 18:48:48,816:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:48:49,289:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:48:49,380:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:48:49,401:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:48:49,441:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:48:49,487:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:48:49,543:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:48:49,557:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:48:49,678:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:48:50,152:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:48:50,192:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:48:50,529:INFO:Calculating mean and std
2023-03-06 18:48:50,532:INFO:Creating metrics dataframe
2023-03-06 18:48:50,540:INFO:Uploading results into container
2023-03-06 18:48:50,542:INFO:Uploading model into container now
2023-03-06 18:48:50,543:INFO:_master_model_container: 7
2023-03-06 18:48:50,543:INFO:_display_container: 2
2023-03-06 18:48:50,544:INFO:OrthogonalMatchingPursuit()
2023-03-06 18:48:50,545:INFO:create_model() successfully completed......................................
2023-03-06 18:48:50,740:INFO:SubProcess create_model() end ==================================
2023-03-06 18:48:50,740:INFO:Creating metrics dataframe
2023-03-06 18:48:50,765:INFO:Initializing Bayesian Ridge
2023-03-06 18:48:50,765:INFO:Total runtime is 0.2595882336298625 minutes
2023-03-06 18:48:50,773:INFO:SubProcess create_model() called ==================================
2023-03-06 18:48:50,773:INFO:Initializing create_model()
2023-03-06 18:48:50,774:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055EC42790>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205600A2CD0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:48:50,775:INFO:Checking exceptions
2023-03-06 18:48:50,775:INFO:Importing libraries
2023-03-06 18:48:50,775:INFO:Copying training dataset
2023-03-06 18:48:50,788:INFO:Defining folds
2023-03-06 18:48:50,789:INFO:Declaring metric variables
2023-03-06 18:48:50,801:INFO:Importing untrained model
2023-03-06 18:48:50,811:INFO:Bayesian Ridge Imported successfully
2023-03-06 18:48:50,832:INFO:Starting cross validation
2023-03-06 18:48:50,837:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:48:52,636:INFO:Calculating mean and std
2023-03-06 18:48:52,640:INFO:Creating metrics dataframe
2023-03-06 18:48:52,650:INFO:Uploading results into container
2023-03-06 18:48:52,651:INFO:Uploading model into container now
2023-03-06 18:48:52,653:INFO:_master_model_container: 8
2023-03-06 18:48:52,654:INFO:_display_container: 2
2023-03-06 18:48:52,655:INFO:BayesianRidge()
2023-03-06 18:48:52,655:INFO:create_model() successfully completed......................................
2023-03-06 18:48:52,848:INFO:SubProcess create_model() end ==================================
2023-03-06 18:48:52,849:INFO:Creating metrics dataframe
2023-03-06 18:48:52,878:INFO:Initializing Passive Aggressive Regressor
2023-03-06 18:48:52,879:INFO:Total runtime is 0.2948316295941671 minutes
2023-03-06 18:48:52,887:INFO:SubProcess create_model() called ==================================
2023-03-06 18:48:52,887:INFO:Initializing create_model()
2023-03-06 18:48:52,888:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055EC42790>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205600A2CD0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:48:52,888:INFO:Checking exceptions
2023-03-06 18:48:52,889:INFO:Importing libraries
2023-03-06 18:48:52,889:INFO:Copying training dataset
2023-03-06 18:48:52,903:INFO:Defining folds
2023-03-06 18:48:52,903:INFO:Declaring metric variables
2023-03-06 18:48:52,912:INFO:Importing untrained model
2023-03-06 18:48:52,925:INFO:Passive Aggressive Regressor Imported successfully
2023-03-06 18:48:52,947:INFO:Starting cross validation
2023-03-06 18:48:52,950:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:48:54,741:INFO:Calculating mean and std
2023-03-06 18:48:54,744:INFO:Creating metrics dataframe
2023-03-06 18:48:54,752:INFO:Uploading results into container
2023-03-06 18:48:54,753:INFO:Uploading model into container now
2023-03-06 18:48:54,754:INFO:_master_model_container: 9
2023-03-06 18:48:54,754:INFO:_display_container: 2
2023-03-06 18:48:54,755:INFO:PassiveAggressiveRegressor(random_state=7742)
2023-03-06 18:48:54,755:INFO:create_model() successfully completed......................................
2023-03-06 18:48:54,953:INFO:SubProcess create_model() end ==================================
2023-03-06 18:48:54,953:INFO:Creating metrics dataframe
2023-03-06 18:48:54,978:INFO:Initializing Huber Regressor
2023-03-06 18:48:54,979:INFO:Total runtime is 0.32983480691909794 minutes
2023-03-06 18:48:54,987:INFO:SubProcess create_model() called ==================================
2023-03-06 18:48:54,988:INFO:Initializing create_model()
2023-03-06 18:48:54,988:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055EC42790>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205600A2CD0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:48:54,988:INFO:Checking exceptions
2023-03-06 18:48:54,988:INFO:Importing libraries
2023-03-06 18:48:54,989:INFO:Copying training dataset
2023-03-06 18:48:55,003:INFO:Defining folds
2023-03-06 18:48:55,003:INFO:Declaring metric variables
2023-03-06 18:48:55,014:INFO:Importing untrained model
2023-03-06 18:48:55,026:INFO:Huber Regressor Imported successfully
2023-03-06 18:48:55,046:INFO:Starting cross validation
2023-03-06 18:48:55,052:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:48:56,819:INFO:Calculating mean and std
2023-03-06 18:48:56,823:INFO:Creating metrics dataframe
2023-03-06 18:48:56,831:INFO:Uploading results into container
2023-03-06 18:48:56,832:INFO:Uploading model into container now
2023-03-06 18:48:56,833:INFO:_master_model_container: 10
2023-03-06 18:48:56,833:INFO:_display_container: 2
2023-03-06 18:48:56,834:INFO:HuberRegressor()
2023-03-06 18:48:56,834:INFO:create_model() successfully completed......................................
2023-03-06 18:48:57,028:INFO:SubProcess create_model() end ==================================
2023-03-06 18:48:57,028:INFO:Creating metrics dataframe
2023-03-06 18:48:57,060:INFO:Initializing K Neighbors Regressor
2023-03-06 18:48:57,060:INFO:Total runtime is 0.3645030577977499 minutes
2023-03-06 18:48:57,069:INFO:SubProcess create_model() called ==================================
2023-03-06 18:48:57,070:INFO:Initializing create_model()
2023-03-06 18:48:57,070:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055EC42790>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205600A2CD0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:48:57,071:INFO:Checking exceptions
2023-03-06 18:48:57,071:INFO:Importing libraries
2023-03-06 18:48:57,072:INFO:Copying training dataset
2023-03-06 18:48:57,086:INFO:Defining folds
2023-03-06 18:48:57,086:INFO:Declaring metric variables
2023-03-06 18:48:57,096:INFO:Importing untrained model
2023-03-06 18:48:57,109:INFO:K Neighbors Regressor Imported successfully
2023-03-06 18:48:57,130:INFO:Starting cross validation
2023-03-06 18:48:57,134:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:48:58,847:INFO:Calculating mean and std
2023-03-06 18:48:58,851:INFO:Creating metrics dataframe
2023-03-06 18:48:58,858:INFO:Uploading results into container
2023-03-06 18:48:58,860:INFO:Uploading model into container now
2023-03-06 18:48:58,860:INFO:_master_model_container: 11
2023-03-06 18:48:58,861:INFO:_display_container: 2
2023-03-06 18:48:58,861:INFO:KNeighborsRegressor(n_jobs=-1)
2023-03-06 18:48:58,862:INFO:create_model() successfully completed......................................
2023-03-06 18:48:59,058:INFO:SubProcess create_model() end ==================================
2023-03-06 18:48:59,059:INFO:Creating metrics dataframe
2023-03-06 18:48:59,090:INFO:Initializing Decision Tree Regressor
2023-03-06 18:48:59,090:INFO:Total runtime is 0.3983384450276693 minutes
2023-03-06 18:48:59,100:INFO:SubProcess create_model() called ==================================
2023-03-06 18:48:59,101:INFO:Initializing create_model()
2023-03-06 18:48:59,101:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055EC42790>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205600A2CD0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:48:59,102:INFO:Checking exceptions
2023-03-06 18:48:59,102:INFO:Importing libraries
2023-03-06 18:48:59,103:INFO:Copying training dataset
2023-03-06 18:48:59,116:INFO:Defining folds
2023-03-06 18:48:59,116:INFO:Declaring metric variables
2023-03-06 18:48:59,127:INFO:Importing untrained model
2023-03-06 18:48:59,137:INFO:Decision Tree Regressor Imported successfully
2023-03-06 18:48:59,164:INFO:Starting cross validation
2023-03-06 18:48:59,169:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:49:00,969:INFO:Calculating mean and std
2023-03-06 18:49:00,973:INFO:Creating metrics dataframe
2023-03-06 18:49:00,981:INFO:Uploading results into container
2023-03-06 18:49:00,982:INFO:Uploading model into container now
2023-03-06 18:49:00,983:INFO:_master_model_container: 12
2023-03-06 18:49:00,983:INFO:_display_container: 2
2023-03-06 18:49:00,984:INFO:DecisionTreeRegressor(random_state=7742)
2023-03-06 18:49:00,984:INFO:create_model() successfully completed......................................
2023-03-06 18:49:01,180:INFO:SubProcess create_model() end ==================================
2023-03-06 18:49:01,180:INFO:Creating metrics dataframe
2023-03-06 18:49:01,210:INFO:Initializing Random Forest Regressor
2023-03-06 18:49:01,210:INFO:Total runtime is 0.43367648124694824 minutes
2023-03-06 18:49:01,221:INFO:SubProcess create_model() called ==================================
2023-03-06 18:49:01,222:INFO:Initializing create_model()
2023-03-06 18:49:01,223:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055EC42790>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205600A2CD0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:49:01,223:INFO:Checking exceptions
2023-03-06 18:49:01,224:INFO:Importing libraries
2023-03-06 18:49:01,225:INFO:Copying training dataset
2023-03-06 18:49:01,241:INFO:Defining folds
2023-03-06 18:49:01,242:INFO:Declaring metric variables
2023-03-06 18:49:01,254:INFO:Importing untrained model
2023-03-06 18:49:01,270:INFO:Random Forest Regressor Imported successfully
2023-03-06 18:49:01,293:INFO:Starting cross validation
2023-03-06 18:49:01,298:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:49:05,887:INFO:Calculating mean and std
2023-03-06 18:49:05,890:INFO:Creating metrics dataframe
2023-03-06 18:49:05,898:INFO:Uploading results into container
2023-03-06 18:49:05,899:INFO:Uploading model into container now
2023-03-06 18:49:05,900:INFO:_master_model_container: 13
2023-03-06 18:49:05,900:INFO:_display_container: 2
2023-03-06 18:49:05,902:INFO:RandomForestRegressor(n_jobs=-1, random_state=7742)
2023-03-06 18:49:05,902:INFO:create_model() successfully completed......................................
2023-03-06 18:49:06,119:INFO:SubProcess create_model() end ==================================
2023-03-06 18:49:06,120:INFO:Creating metrics dataframe
2023-03-06 18:49:06,153:INFO:Initializing Extra Trees Regressor
2023-03-06 18:49:06,153:INFO:Total runtime is 0.5160571773846945 minutes
2023-03-06 18:49:06,162:INFO:SubProcess create_model() called ==================================
2023-03-06 18:49:06,162:INFO:Initializing create_model()
2023-03-06 18:49:06,163:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055EC42790>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205600A2CD0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:49:06,164:INFO:Checking exceptions
2023-03-06 18:49:06,164:INFO:Importing libraries
2023-03-06 18:49:06,164:INFO:Copying training dataset
2023-03-06 18:49:06,176:INFO:Defining folds
2023-03-06 18:49:06,177:INFO:Declaring metric variables
2023-03-06 18:49:06,187:INFO:Importing untrained model
2023-03-06 18:49:06,199:INFO:Extra Trees Regressor Imported successfully
2023-03-06 18:49:06,221:INFO:Starting cross validation
2023-03-06 18:49:06,225:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:49:10,204:INFO:Calculating mean and std
2023-03-06 18:49:10,208:INFO:Creating metrics dataframe
2023-03-06 18:49:10,216:INFO:Uploading results into container
2023-03-06 18:49:10,218:INFO:Uploading model into container now
2023-03-06 18:49:10,219:INFO:_master_model_container: 14
2023-03-06 18:49:10,219:INFO:_display_container: 2
2023-03-06 18:49:10,220:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=7742)
2023-03-06 18:49:10,221:INFO:create_model() successfully completed......................................
2023-03-06 18:49:10,455:INFO:SubProcess create_model() end ==================================
2023-03-06 18:49:10,455:INFO:Creating metrics dataframe
2023-03-06 18:49:10,487:INFO:Initializing AdaBoost Regressor
2023-03-06 18:49:10,488:INFO:Total runtime is 0.588302493095398 minutes
2023-03-06 18:49:10,497:INFO:SubProcess create_model() called ==================================
2023-03-06 18:49:10,498:INFO:Initializing create_model()
2023-03-06 18:49:10,498:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055EC42790>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205600A2CD0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:49:10,499:INFO:Checking exceptions
2023-03-06 18:49:10,500:INFO:Importing libraries
2023-03-06 18:49:10,500:INFO:Copying training dataset
2023-03-06 18:49:10,515:INFO:Defining folds
2023-03-06 18:49:10,516:INFO:Declaring metric variables
2023-03-06 18:49:10,524:INFO:Importing untrained model
2023-03-06 18:49:10,535:INFO:AdaBoost Regressor Imported successfully
2023-03-06 18:49:10,559:INFO:Starting cross validation
2023-03-06 18:49:10,563:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:49:13,506:INFO:Calculating mean and std
2023-03-06 18:49:13,511:INFO:Creating metrics dataframe
2023-03-06 18:49:13,518:INFO:Uploading results into container
2023-03-06 18:49:13,519:INFO:Uploading model into container now
2023-03-06 18:49:13,520:INFO:_master_model_container: 15
2023-03-06 18:49:13,520:INFO:_display_container: 2
2023-03-06 18:49:13,521:INFO:AdaBoostRegressor(random_state=7742)
2023-03-06 18:49:13,522:INFO:create_model() successfully completed......................................
2023-03-06 18:49:13,718:INFO:SubProcess create_model() end ==================================
2023-03-06 18:49:13,718:INFO:Creating metrics dataframe
2023-03-06 18:49:13,751:INFO:Initializing Gradient Boosting Regressor
2023-03-06 18:49:13,751:INFO:Total runtime is 0.6426874240239461 minutes
2023-03-06 18:49:13,763:INFO:SubProcess create_model() called ==================================
2023-03-06 18:49:13,764:INFO:Initializing create_model()
2023-03-06 18:49:13,765:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055EC42790>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205600A2CD0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:49:13,765:INFO:Checking exceptions
2023-03-06 18:49:13,765:INFO:Importing libraries
2023-03-06 18:49:13,766:INFO:Copying training dataset
2023-03-06 18:49:13,781:INFO:Defining folds
2023-03-06 18:49:13,781:INFO:Declaring metric variables
2023-03-06 18:49:13,792:INFO:Importing untrained model
2023-03-06 18:49:13,804:INFO:Gradient Boosting Regressor Imported successfully
2023-03-06 18:49:13,826:INFO:Starting cross validation
2023-03-06 18:49:13,833:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:49:17,137:INFO:Calculating mean and std
2023-03-06 18:49:17,143:INFO:Creating metrics dataframe
2023-03-06 18:49:17,155:INFO:Uploading results into container
2023-03-06 18:49:17,160:INFO:Uploading model into container now
2023-03-06 18:49:17,160:INFO:_master_model_container: 16
2023-03-06 18:49:17,160:INFO:_display_container: 2
2023-03-06 18:49:17,161:INFO:GradientBoostingRegressor(random_state=7742)
2023-03-06 18:49:17,161:INFO:create_model() successfully completed......................................
2023-03-06 18:49:17,367:INFO:SubProcess create_model() end ==================================
2023-03-06 18:49:17,368:INFO:Creating metrics dataframe
2023-03-06 18:49:17,398:INFO:Initializing Extreme Gradient Boosting
2023-03-06 18:49:17,398:INFO:Total runtime is 0.7034757812817891 minutes
2023-03-06 18:49:17,407:INFO:SubProcess create_model() called ==================================
2023-03-06 18:49:17,408:INFO:Initializing create_model()
2023-03-06 18:49:17,408:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055EC42790>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205600A2CD0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:49:17,408:INFO:Checking exceptions
2023-03-06 18:49:17,408:INFO:Importing libraries
2023-03-06 18:49:17,409:INFO:Copying training dataset
2023-03-06 18:49:17,419:INFO:Defining folds
2023-03-06 18:49:17,420:INFO:Declaring metric variables
2023-03-06 18:49:17,430:INFO:Importing untrained model
2023-03-06 18:49:17,442:INFO:Extreme Gradient Boosting Imported successfully
2023-03-06 18:49:17,466:INFO:Starting cross validation
2023-03-06 18:49:17,470:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:49:19,983:INFO:Calculating mean and std
2023-03-06 18:49:19,988:INFO:Creating metrics dataframe
2023-03-06 18:49:19,995:INFO:Uploading results into container
2023-03-06 18:49:19,996:INFO:Uploading model into container now
2023-03-06 18:49:19,997:INFO:_master_model_container: 17
2023-03-06 18:49:19,997:INFO:_display_container: 2
2023-03-06 18:49:20,002:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=7742, ...)
2023-03-06 18:49:20,003:INFO:create_model() successfully completed......................................
2023-03-06 18:49:20,218:INFO:SubProcess create_model() end ==================================
2023-03-06 18:49:20,218:INFO:Creating metrics dataframe
2023-03-06 18:49:20,249:INFO:Initializing Light Gradient Boosting Machine
2023-03-06 18:49:20,249:INFO:Total runtime is 0.7509970943133036 minutes
2023-03-06 18:49:20,262:INFO:SubProcess create_model() called ==================================
2023-03-06 18:49:20,263:INFO:Initializing create_model()
2023-03-06 18:49:20,263:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055EC42790>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205600A2CD0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:49:20,263:INFO:Checking exceptions
2023-03-06 18:49:20,264:INFO:Importing libraries
2023-03-06 18:49:20,265:INFO:Copying training dataset
2023-03-06 18:49:20,278:INFO:Defining folds
2023-03-06 18:49:20,278:INFO:Declaring metric variables
2023-03-06 18:49:20,291:INFO:Importing untrained model
2023-03-06 18:49:20,302:INFO:Light Gradient Boosting Machine Imported successfully
2023-03-06 18:49:20,322:INFO:Starting cross validation
2023-03-06 18:49:20,327:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:49:22,320:INFO:Calculating mean and std
2023-03-06 18:49:22,327:INFO:Creating metrics dataframe
2023-03-06 18:49:22,337:INFO:Uploading results into container
2023-03-06 18:49:22,338:INFO:Uploading model into container now
2023-03-06 18:49:22,339:INFO:_master_model_container: 18
2023-03-06 18:49:22,339:INFO:_display_container: 2
2023-03-06 18:49:22,340:INFO:LGBMRegressor(random_state=7742)
2023-03-06 18:49:22,340:INFO:create_model() successfully completed......................................
2023-03-06 18:49:22,575:INFO:SubProcess create_model() end ==================================
2023-03-06 18:49:22,576:INFO:Creating metrics dataframe
2023-03-06 18:49:22,628:INFO:Initializing Dummy Regressor
2023-03-06 18:49:22,628:INFO:Total runtime is 0.7906508525212605 minutes
2023-03-06 18:49:22,640:INFO:SubProcess create_model() called ==================================
2023-03-06 18:49:22,641:INFO:Initializing create_model()
2023-03-06 18:49:22,642:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055EC42790>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205600A2CD0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:49:22,642:INFO:Checking exceptions
2023-03-06 18:49:22,642:INFO:Importing libraries
2023-03-06 18:49:22,643:INFO:Copying training dataset
2023-03-06 18:49:22,663:INFO:Defining folds
2023-03-06 18:49:22,663:INFO:Declaring metric variables
2023-03-06 18:49:22,674:INFO:Importing untrained model
2023-03-06 18:49:22,684:INFO:Dummy Regressor Imported successfully
2023-03-06 18:49:22,707:INFO:Starting cross validation
2023-03-06 18:49:22,712:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:49:24,550:INFO:Calculating mean and std
2023-03-06 18:49:24,554:INFO:Creating metrics dataframe
2023-03-06 18:49:24,562:INFO:Uploading results into container
2023-03-06 18:49:24,564:INFO:Uploading model into container now
2023-03-06 18:49:24,565:INFO:_master_model_container: 19
2023-03-06 18:49:24,565:INFO:_display_container: 2
2023-03-06 18:49:24,565:INFO:DummyRegressor()
2023-03-06 18:49:24,566:INFO:create_model() successfully completed......................................
2023-03-06 18:49:24,765:INFO:SubProcess create_model() end ==================================
2023-03-06 18:49:24,765:INFO:Creating metrics dataframe
2023-03-06 18:49:24,827:INFO:Initializing create_model()
2023-03-06 18:49:24,828:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055EC42790>, estimator=BayesianRidge(), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:49:24,829:INFO:Checking exceptions
2023-03-06 18:49:24,833:INFO:Importing libraries
2023-03-06 18:49:24,833:INFO:Copying training dataset
2023-03-06 18:49:24,849:INFO:Defining folds
2023-03-06 18:49:24,849:INFO:Declaring metric variables
2023-03-06 18:49:24,850:INFO:Importing untrained model
2023-03-06 18:49:24,850:INFO:Declaring custom model
2023-03-06 18:49:24,851:INFO:Bayesian Ridge Imported successfully
2023-03-06 18:49:24,855:INFO:Cross validation set to False
2023-03-06 18:49:24,855:INFO:Fitting Model
2023-03-06 18:49:25,185:INFO:BayesianRidge()
2023-03-06 18:49:25,185:INFO:create_model() successfully completed......................................
2023-03-06 18:49:25,525:INFO:_master_model_container: 19
2023-03-06 18:49:25,525:INFO:_display_container: 2
2023-03-06 18:49:25,526:INFO:BayesianRidge()
2023-03-06 18:49:25,527:INFO:compare_models() successfully completed......................................
2023-03-06 18:49:25,550:INFO:Initializing evaluate_model()
2023-03-06 18:49:25,550:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055EC42790>, estimator=BayesianRidge(), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-03-06 18:49:25,608:INFO:Initializing plot_model()
2023-03-06 18:49:25,608:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=BayesianRidge(), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055EC42790>, system=True)
2023-03-06 18:49:25,608:INFO:Checking exceptions
2023-03-06 18:49:25,614:INFO:Preloading libraries
2023-03-06 18:49:25,615:INFO:Copying training dataset
2023-03-06 18:49:25,615:INFO:Plot type: pipeline
2023-03-06 18:49:25,850:INFO:Visual Rendered Successfully
2023-03-06 18:49:26,047:INFO:plot_model() successfully completed......................................
2023-03-06 18:51:31,517:INFO:Initializing predict_model()
2023-03-06 18:51:31,517:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055EC42790>, estimator=BayesianRidge(), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x000002055F01E040>)
2023-03-06 18:51:31,517:INFO:Checking exceptions
2023-03-06 18:51:31,518:INFO:Preloading libraries
2023-03-06 18:51:31,523:INFO:Set up data.
2023-03-06 18:51:31,577:INFO:Set up index.
2023-03-06 18:51:38,150:INFO:Initializing evaluate_model()
2023-03-06 18:51:38,151:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055EC42790>, estimator=BayesianRidge(), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-03-06 18:51:38,194:INFO:Initializing plot_model()
2023-03-06 18:51:38,195:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=BayesianRidge(), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055EC42790>, system=True)
2023-03-06 18:51:38,195:INFO:Checking exceptions
2023-03-06 18:51:38,200:INFO:Preloading libraries
2023-03-06 18:51:38,201:INFO:Copying training dataset
2023-03-06 18:51:38,201:INFO:Plot type: pipeline
2023-03-06 18:51:38,409:INFO:Visual Rendered Successfully
2023-03-06 18:51:38,602:INFO:plot_model() successfully completed......................................
2023-03-06 18:51:39,517:INFO:Initializing predict_model()
2023-03-06 18:51:39,517:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055EC42790>, estimator=BayesianRidge(), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x000002055EEFE550>)
2023-03-06 18:51:39,517:INFO:Checking exceptions
2023-03-06 18:51:39,517:INFO:Preloading libraries
2023-03-06 18:51:39,523:INFO:Set up data.
2023-03-06 18:51:39,573:INFO:Set up index.
2023-03-06 18:52:47,959:INFO:PyCaret RegressionExperiment
2023-03-06 18:52:47,960:INFO:Logging name: reg-default-name
2023-03-06 18:52:47,960:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-06 18:52:47,960:INFO:version 3.0.0.rc9
2023-03-06 18:52:47,960:INFO:Initializing setup()
2023-03-06 18:52:47,960:INFO:self.USI: 425a
2023-03-06 18:52:47,960:INFO:self._variable_keys: {'_available_plots', 'fold_groups_param', 'fold_generator', 'pipeline', 'USI', 'logging_param', 'X', '_ml_usecase', 'idx', 'exp_id', 'fold_shuffle_param', 'X_train', 'exp_name_log', 'y_test', 'transform_target_param', 'y_train', 'seed', 'data', 'log_plots_param', 'gpu_n_jobs_param', 'target_param', 'n_jobs_param', 'X_test', 'memory', 'y', 'html_param', 'gpu_param'}
2023-03-06 18:52:47,960:INFO:Checking environment
2023-03-06 18:52:47,960:INFO:python_version: 3.9.13
2023-03-06 18:52:47,961:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-03-06 18:52:47,961:INFO:machine: AMD64
2023-03-06 18:52:47,961:INFO:platform: Windows-10-10.0.19044-SP0
2023-03-06 18:52:47,961:INFO:Memory: svmem(total=17009516544, available=4712095744, percent=72.3, used=12297420800, free=4712095744)
2023-03-06 18:52:47,961:INFO:Physical Core: 4
2023-03-06 18:52:47,961:INFO:Logical Core: 8
2023-03-06 18:52:47,961:INFO:Checking libraries
2023-03-06 18:52:47,962:INFO:System:
2023-03-06 18:52:47,962:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-03-06 18:52:47,962:INFO:executable: C:\Users\Alvaro\anaconda3\python.exe
2023-03-06 18:52:47,962:INFO:   machine: Windows-10-10.0.19044-SP0
2023-03-06 18:52:47,962:INFO:PyCaret required dependencies:
2023-03-06 18:52:47,963:INFO:                 pip: 22.2.2
2023-03-06 18:52:47,963:INFO:          setuptools: 63.4.1
2023-03-06 18:52:47,963:INFO:             pycaret: 3.0.0rc9
2023-03-06 18:52:47,963:INFO:             IPython: 7.31.1
2023-03-06 18:52:47,963:INFO:          ipywidgets: 7.6.5
2023-03-06 18:52:47,963:INFO:                tqdm: 4.64.1
2023-03-06 18:52:47,964:INFO:               numpy: 1.21.5
2023-03-06 18:52:47,964:INFO:              pandas: 1.4.4
2023-03-06 18:52:47,964:INFO:              jinja2: 2.11.3
2023-03-06 18:52:47,964:INFO:               scipy: 1.9.1
2023-03-06 18:52:47,964:INFO:              joblib: 1.2.0
2023-03-06 18:52:47,964:INFO:             sklearn: 1.0.2
2023-03-06 18:52:47,964:INFO:                pyod: 1.0.7
2023-03-06 18:52:47,964:INFO:            imblearn: 0.10.1
2023-03-06 18:52:47,964:INFO:   category_encoders: 2.6.0
2023-03-06 18:52:47,965:INFO:            lightgbm: 3.3.5
2023-03-06 18:52:47,965:INFO:               numba: 0.55.1
2023-03-06 18:52:47,965:INFO:            requests: 2.28.1
2023-03-06 18:52:47,965:INFO:          matplotlib: 3.5.2
2023-03-06 18:52:47,965:INFO:          scikitplot: 0.3.7
2023-03-06 18:52:47,965:INFO:         yellowbrick: 1.5
2023-03-06 18:52:47,965:INFO:              plotly: 5.9.0
2023-03-06 18:52:47,965:INFO:             kaleido: 0.2.1
2023-03-06 18:52:47,965:INFO:         statsmodels: 0.13.2
2023-03-06 18:52:47,965:INFO:              sktime: 0.16.1
2023-03-06 18:52:47,966:INFO:               tbats: 1.1.2
2023-03-06 18:52:47,966:INFO:            pmdarima: 2.0.2
2023-03-06 18:52:47,966:INFO:              psutil: 5.9.0
2023-03-06 18:52:47,966:INFO:PyCaret optional dependencies:
2023-03-06 18:52:47,966:INFO:                shap: Not installed
2023-03-06 18:52:47,966:INFO:           interpret: Not installed
2023-03-06 18:52:47,966:INFO:                umap: Not installed
2023-03-06 18:52:47,967:INFO:    pandas_profiling: Not installed
2023-03-06 18:52:47,967:INFO:  explainerdashboard: Not installed
2023-03-06 18:52:47,967:INFO:             autoviz: Not installed
2023-03-06 18:52:47,967:INFO:           fairlearn: Not installed
2023-03-06 18:52:47,967:INFO:             xgboost: 1.7.4
2023-03-06 18:52:47,967:INFO:            catboost: Not installed
2023-03-06 18:52:47,967:INFO:              kmodes: Not installed
2023-03-06 18:52:47,967:INFO:             mlxtend: Not installed
2023-03-06 18:52:47,968:INFO:       statsforecast: Not installed
2023-03-06 18:52:47,968:INFO:        tune_sklearn: Not installed
2023-03-06 18:52:47,968:INFO:                 ray: Not installed
2023-03-06 18:52:47,968:INFO:            hyperopt: Not installed
2023-03-06 18:52:47,968:INFO:              optuna: Not installed
2023-03-06 18:52:47,968:INFO:               skopt: Not installed
2023-03-06 18:52:47,968:INFO:              mlflow: Not installed
2023-03-06 18:52:47,968:INFO:              gradio: Not installed
2023-03-06 18:52:47,969:INFO:             fastapi: Not installed
2023-03-06 18:52:47,969:INFO:             uvicorn: Not installed
2023-03-06 18:52:47,969:INFO:              m2cgen: Not installed
2023-03-06 18:52:47,969:INFO:           evidently: Not installed
2023-03-06 18:52:47,969:INFO:               fugue: Not installed
2023-03-06 18:52:47,969:INFO:           streamlit: Not installed
2023-03-06 18:52:47,969:INFO:             prophet: Not installed
2023-03-06 18:52:47,969:INFO:None
2023-03-06 18:52:47,970:INFO:Set up data.
2023-03-06 18:52:48,094:INFO:Set up train/test split.
2023-03-06 18:52:48,106:INFO:Set up index.
2023-03-06 18:52:48,106:INFO:Set up folding strategy.
2023-03-06 18:52:48,106:INFO:Assigning column types.
2023-03-06 18:52:48,114:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-06 18:52:48,114:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-06 18:52:48,126:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-06 18:52:48,137:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-06 18:52:48,279:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:52:48,392:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 18:52:48,393:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:52:48,400:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:52:48,401:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-06 18:52:48,412:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-06 18:52:48,424:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-06 18:52:48,569:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:52:48,696:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 18:52:48,697:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:52:48,704:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:52:48,705:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-03-06 18:52:48,717:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-06 18:52:48,729:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-06 18:52:48,872:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:52:48,984:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 18:52:48,985:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:52:48,994:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:52:49,015:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-06 18:52:49,032:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-06 18:52:49,215:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:52:49,336:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 18:52:49,338:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:52:49,345:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:52:49,345:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-03-06 18:52:49,367:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-06 18:52:49,505:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:52:49,618:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 18:52:49,621:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:52:49,626:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:52:49,649:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-06 18:52:49,795:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:52:49,907:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 18:52:49,908:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:52:49,915:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:52:49,915:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-03-06 18:52:50,082:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:52:50,194:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 18:52:50,195:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:52:50,202:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:52:50,363:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:52:50,472:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-06 18:52:50,473:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:52:50,479:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:52:50,481:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-06 18:52:50,644:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:52:50,757:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:52:50,764:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:52:50,927:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-06 18:52:51,039:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:52:51,045:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:52:51,046:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-03-06 18:52:51,325:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:52:51,332:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:52:51,611:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:52:51,617:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:52:51,620:INFO:Preparing preprocessing pipeline...
2023-03-06 18:52:51,622:INFO:Set up column name cleaning.
2023-03-06 18:52:51,622:INFO:Set up simple imputation.
2023-03-06 18:52:51,694:INFO:Finished creating preprocessing pipeline.
2023-03-06 18:52:51,709:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Alvaro\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('numerical_imputer',
                 TransformerWrapper(include=['work_year', 'remote_ratio',
                                             'experience_level_EX',
                                             'experience_level_MI',
                                             'experience_level_SE',
                                             'employment_type_FL',
                                             'employment_type_FT',
                                             'emplo...
                                             'job_title_Data Architect',
                                             'job_title_Data Engineer',
                                             'job_title_Data Engineering '
                                             'Manager',
                                             'job_title_Data Science '
                                             'Consultant',
                                             'job_title_Data Science Engineer',
                                             'job_title_Data Science Manager',
                                             'job_title_Data Scientist', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent')))])
2023-03-06 18:52:51,709:INFO:Creating final display dataframe.
2023-03-06 18:52:52,178:INFO:Setup _display_container:                     Description             Value
0                    Session id              6768
1                        Target     salary_in_usd
2                   Target type        Regression
3           Original data shape        (500, 148)
4        Transformed data shape        (500, 148)
5   Transformed train set shape        (350, 148)
6    Transformed test set shape        (150, 148)
7              Numeric features               147
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator             KFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  reg-default-name
18                          USI              425a
2023-03-06 18:52:52,480:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:52:52,486:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:52:52,775:INFO:Soft dependency imported: xgboost: 1.7.4
2023-03-06 18:52:52,783:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-03-06 18:52:52,784:INFO:setup() successfully completed in 4.83s...............
2023-03-06 18:52:52,808:INFO:Initializing compare_models()
2023-03-06 18:52:52,809:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055FECCAC0>, include=None, fold=None, round=4, cross_validation=True, sort=RMSE, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000002055FECCAC0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'RMSE', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-03-06 18:52:52,809:INFO:Checking exceptions
2023-03-06 18:52:52,815:INFO:Preparing display monitor
2023-03-06 18:52:52,930:INFO:Initializing Linear Regression
2023-03-06 18:52:52,930:INFO:Total runtime is 2.757708231608073e-06 minutes
2023-03-06 18:52:52,945:INFO:SubProcess create_model() called ==================================
2023-03-06 18:52:52,946:INFO:Initializing create_model()
2023-03-06 18:52:52,947:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055FECCAC0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055FECCFA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:52:52,947:INFO:Checking exceptions
2023-03-06 18:52:52,948:INFO:Importing libraries
2023-03-06 18:52:52,949:INFO:Copying training dataset
2023-03-06 18:52:52,969:INFO:Defining folds
2023-03-06 18:52:52,969:INFO:Declaring metric variables
2023-03-06 18:52:52,980:INFO:Importing untrained model
2023-03-06 18:52:52,991:INFO:Linear Regression Imported successfully
2023-03-06 18:52:53,015:INFO:Starting cross validation
2023-03-06 18:52:53,019:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:52:53,654:INFO:Calculating mean and std
2023-03-06 18:52:53,655:INFO:Creating metrics dataframe
2023-03-06 18:52:53,662:INFO:Uploading results into container
2023-03-06 18:52:53,663:INFO:Uploading model into container now
2023-03-06 18:52:53,664:INFO:_master_model_container: 1
2023-03-06 18:52:53,664:INFO:_display_container: 2
2023-03-06 18:52:53,664:INFO:LinearRegression(n_jobs=-1)
2023-03-06 18:52:53,664:INFO:create_model() successfully completed......................................
2023-03-06 18:52:53,911:INFO:SubProcess create_model() end ==================================
2023-03-06 18:52:53,912:INFO:Creating metrics dataframe
2023-03-06 18:52:53,933:INFO:Initializing Lasso Regression
2023-03-06 18:52:53,933:INFO:Total runtime is 0.016717636585235597 minutes
2023-03-06 18:52:53,942:INFO:SubProcess create_model() called ==================================
2023-03-06 18:52:53,943:INFO:Initializing create_model()
2023-03-06 18:52:53,943:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055FECCAC0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055FECCFA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:52:53,944:INFO:Checking exceptions
2023-03-06 18:52:53,945:INFO:Importing libraries
2023-03-06 18:52:53,945:INFO:Copying training dataset
2023-03-06 18:52:53,960:INFO:Defining folds
2023-03-06 18:52:53,960:INFO:Declaring metric variables
2023-03-06 18:52:53,973:INFO:Importing untrained model
2023-03-06 18:52:53,987:INFO:Lasso Regression Imported successfully
2023-03-06 18:52:54,011:INFO:Starting cross validation
2023-03-06 18:52:54,015:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:52:54,611:INFO:Calculating mean and std
2023-03-06 18:52:54,616:INFO:Creating metrics dataframe
2023-03-06 18:52:54,628:INFO:Uploading results into container
2023-03-06 18:52:54,630:INFO:Uploading model into container now
2023-03-06 18:52:54,633:INFO:_master_model_container: 2
2023-03-06 18:52:54,633:INFO:_display_container: 2
2023-03-06 18:52:54,635:INFO:Lasso(random_state=6768)
2023-03-06 18:52:54,636:INFO:create_model() successfully completed......................................
2023-03-06 18:52:54,924:INFO:SubProcess create_model() end ==================================
2023-03-06 18:52:54,924:INFO:Creating metrics dataframe
2023-03-06 18:52:54,946:INFO:Initializing Ridge Regression
2023-03-06 18:52:54,947:INFO:Total runtime is 0.03361513614654541 minutes
2023-03-06 18:52:54,954:INFO:SubProcess create_model() called ==================================
2023-03-06 18:52:54,955:INFO:Initializing create_model()
2023-03-06 18:52:54,955:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055FECCAC0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055FECCFA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:52:54,955:INFO:Checking exceptions
2023-03-06 18:52:54,956:INFO:Importing libraries
2023-03-06 18:52:54,956:INFO:Copying training dataset
2023-03-06 18:52:54,967:INFO:Defining folds
2023-03-06 18:52:54,967:INFO:Declaring metric variables
2023-03-06 18:52:54,976:INFO:Importing untrained model
2023-03-06 18:52:54,985:INFO:Ridge Regression Imported successfully
2023-03-06 18:52:55,003:INFO:Starting cross validation
2023-03-06 18:52:55,010:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:52:55,562:INFO:Calculating mean and std
2023-03-06 18:52:55,565:INFO:Creating metrics dataframe
2023-03-06 18:52:55,574:INFO:Uploading results into container
2023-03-06 18:52:55,575:INFO:Uploading model into container now
2023-03-06 18:52:55,576:INFO:_master_model_container: 3
2023-03-06 18:52:55,576:INFO:_display_container: 2
2023-03-06 18:52:55,577:INFO:Ridge(random_state=6768)
2023-03-06 18:52:55,577:INFO:create_model() successfully completed......................................
2023-03-06 18:52:55,776:INFO:SubProcess create_model() end ==================================
2023-03-06 18:52:55,776:INFO:Creating metrics dataframe
2023-03-06 18:52:55,804:INFO:Initializing Elastic Net
2023-03-06 18:52:55,805:INFO:Total runtime is 0.04793057044347127 minutes
2023-03-06 18:52:55,813:INFO:SubProcess create_model() called ==================================
2023-03-06 18:52:55,814:INFO:Initializing create_model()
2023-03-06 18:52:55,814:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055FECCAC0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055FECCFA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:52:55,815:INFO:Checking exceptions
2023-03-06 18:52:55,816:INFO:Importing libraries
2023-03-06 18:52:55,816:INFO:Copying training dataset
2023-03-06 18:52:55,829:INFO:Defining folds
2023-03-06 18:52:55,829:INFO:Declaring metric variables
2023-03-06 18:52:55,840:INFO:Importing untrained model
2023-03-06 18:52:55,851:INFO:Elastic Net Imported successfully
2023-03-06 18:52:55,875:INFO:Starting cross validation
2023-03-06 18:52:55,879:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:52:56,476:INFO:Calculating mean and std
2023-03-06 18:52:56,481:INFO:Creating metrics dataframe
2023-03-06 18:52:56,494:INFO:Uploading results into container
2023-03-06 18:52:56,496:INFO:Uploading model into container now
2023-03-06 18:52:56,497:INFO:_master_model_container: 4
2023-03-06 18:52:56,498:INFO:_display_container: 2
2023-03-06 18:52:56,499:INFO:ElasticNet(random_state=6768)
2023-03-06 18:52:56,500:INFO:create_model() successfully completed......................................
2023-03-06 18:52:56,809:INFO:SubProcess create_model() end ==================================
2023-03-06 18:52:56,809:INFO:Creating metrics dataframe
2023-03-06 18:52:56,835:INFO:Initializing Least Angle Regression
2023-03-06 18:52:56,836:INFO:Total runtime is 0.06509933471679688 minutes
2023-03-06 18:52:56,845:INFO:SubProcess create_model() called ==================================
2023-03-06 18:52:56,846:INFO:Initializing create_model()
2023-03-06 18:52:56,846:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055FECCAC0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055FECCFA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:52:56,846:INFO:Checking exceptions
2023-03-06 18:52:56,847:INFO:Importing libraries
2023-03-06 18:52:56,847:INFO:Copying training dataset
2023-03-06 18:52:56,862:INFO:Defining folds
2023-03-06 18:52:56,863:INFO:Declaring metric variables
2023-03-06 18:52:56,873:INFO:Importing untrained model
2023-03-06 18:52:56,887:INFO:Least Angle Regression Imported successfully
2023-03-06 18:52:56,908:INFO:Starting cross validation
2023-03-06 18:52:56,912:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:52:57,120:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:52:57,137:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=3.143e-03, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,146:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.320e-03, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,155:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:52:57,172:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=1.153e-03, with an active set of 65 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,195:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:52:57,210:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:52:57,217:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=3.591e+02, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,220:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.369e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,224:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.166e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,226:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 144 iterations, i.e. alpha=4.010e+02, with an active set of 106 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,228:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 145 iterations, i.e. alpha=4.110e+02, with an active set of 107 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,233:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 150 iterations, i.e. alpha=4.077e+02, with an active set of 111 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,233:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 150 iterations, i.e. alpha=3.967e+02, with an active set of 111 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,234:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 150 iterations, i.e. alpha=2.837e+02, with an active set of 111 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,235:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=1.467e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,236:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 152 iterations, i.e. alpha=2.307e+02, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,237:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=1.382e-03, with an active set of 53 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,238:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 153 iterations, i.e. alpha=1.768e+02, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,238:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 153 iterations, i.e. alpha=1.505e+02, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,241:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 156 iterations, i.e. alpha=1.195e+02, with an active set of 116 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,242:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.000e-03, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,242:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=1.839e-03, with an active set of 79 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-06 18:52:57,243:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=1.083e-03, with an active set of 61 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,243:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 157 iterations, i.e. alpha=8.213e+01, with an active set of 117 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,245:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 158 iterations, i.e. alpha=7.113e+01, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,245:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 158 iterations, i.e. alpha=7.047e+00, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,246:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 158 iterations, i.e. alpha=6.443e+00, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,247:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 158 iterations, i.e. alpha=1.060e+00, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,249:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=1.684e-03, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,252:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=1.653e-03, with an active set of 49 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,256:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=6.988e-04, with an active set of 78 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,257:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:52:57,258:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=6.636e-04, with an active set of 79 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,260:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=6.317e-04, with an active set of 81 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,263:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=6.494e-04, with an active set of 83 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,264:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=1.203e+00, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,265:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=1.167e+00, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,265:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=1.094e+00, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,267:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=1.082e+00, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,268:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 93 iterations, i.e. alpha=4.381e-04, with an active set of 89 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,272:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 98 iterations, i.e. alpha=3.572e-04, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,274:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=2.610e-04, with an active set of 98 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,277:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:52:57,279:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 144 iterations, i.e. alpha=1.093e+07, with an active set of 109 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,280:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=3.145e-01, with an active set of 82 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-06 18:52:57,280:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 144 iterations, i.e. alpha=8.853e+06, with an active set of 109 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,281:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 144 iterations, i.e. alpha=7.123e+06, with an active set of 109 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,281:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 144 iterations, i.e. alpha=5.118e+06, with an active set of 109 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,282:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 144 iterations, i.e. alpha=3.426e+06, with an active set of 109 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,282:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.794e-03, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,286:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:52:57,291:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=1.461e-03, with an active set of 53 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,291:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 127 iterations, i.e. alpha=1.110e+03, with an active set of 107 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,291:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 127 iterations, i.e. alpha=1.100e+03, with an active set of 107 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,292:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 127 iterations, i.e. alpha=1.096e+03, with an active set of 107 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,292:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=1.402e-03, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,292:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 127 iterations, i.e. alpha=1.070e+03, with an active set of 107 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,293:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 127 iterations, i.e. alpha=1.068e+03, with an active set of 107 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,294:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 127 iterations, i.e. alpha=1.030e+03, with an active set of 107 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,294:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 127 iterations, i.e. alpha=1.003e+03, with an active set of 107 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,295:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 127 iterations, i.e. alpha=9.921e+02, with an active set of 107 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,297:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=1.179e+04, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,298:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=9.130e+03, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,298:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=7.487e+02, with an active set of 109 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,298:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=6.510e+03, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,299:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=6.362e+03, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,299:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=5.115e+03, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,301:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=7.399e+02, with an active set of 110 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,301:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=7.350e+02, with an active set of 110 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,301:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=4.390e+03, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,301:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=7.285e+02, with an active set of 110 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,303:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=7.205e+02, with an active set of 110 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,303:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 126 iterations, i.e. alpha=4.236e+03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,303:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=7.123e+02, with an active set of 110 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,303:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 126 iterations, i.e. alpha=4.172e+03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,303:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 126 iterations, i.e. alpha=3.811e+03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,305:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=7.077e-04, with an active set of 70 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,305:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 127 iterations, i.e. alpha=3.026e+03, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,305:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 138 iterations, i.e. alpha=6.826e+02, with an active set of 111 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,306:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 127 iterations, i.e. alpha=2.860e+03, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,307:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 140 iterations, i.e. alpha=6.494e+02, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,307:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=2.466e+03, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,308:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=2.382e+03, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,308:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 141 iterations, i.e. alpha=1.600e+02, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,308:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=1.957e+03, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,309:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 141 iterations, i.e. alpha=1.547e+02, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,309:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=1.907e+03, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,310:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 142 iterations, i.e. alpha=1.404e+02, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,310:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=4.914e-04, with an active set of 78 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,310:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 130 iterations, i.e. alpha=1.695e+03, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,310:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 142 iterations, i.e. alpha=1.330e+02, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,310:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 142 iterations, i.e. alpha=1.211e+02, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,311:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 142 iterations, i.e. alpha=1.133e+02, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,311:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=1.426e+03, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,312:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 142 iterations, i.e. alpha=1.046e+02, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,312:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=1.331e+03, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,312:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 142 iterations, i.e. alpha=6.261e+01, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,313:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=1.234e+03, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,313:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 142 iterations, i.e. alpha=7.943e+00, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,313:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=1.223e+03, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,313:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 142 iterations, i.e. alpha=7.227e+00, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,313:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:52:57,313:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=9.758e+02, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,314:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=8.740e+02, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,314:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=7.444e+02, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,315:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=4.479e+02, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,315:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 89 iterations, i.e. alpha=4.281e-04, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,316:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=3.234e+02, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,317:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 89 iterations, i.e. alpha=3.947e-04, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,317:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=2.180e+02, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,317:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=1.956e+02, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,320:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=1.398e-03, with an active set of 65 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,333:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 99 iterations, i.e. alpha=6.007e-02, with an active set of 80 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,337:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=1.019e+04, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,339:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 133 iterations, i.e. alpha=1.004e+04, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,346:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=1.179e-01, with an active set of 83 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,350:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=1.072e-03, with an active set of 62 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,351:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=1.041e-03, with an active set of 63 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,353:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=8.572e-04, with an active set of 65 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,354:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=8.571e-04, with an active set of 65 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,354:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 156 iterations, i.e. alpha=8.099e+04, with an active set of 112 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,354:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 156 iterations, i.e. alpha=7.912e+04, with an active set of 112 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,357:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 158 iterations, i.e. alpha=8.246e+04, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,359:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 160 iterations, i.e. alpha=7.095e+04, with an active set of 116 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,360:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 142 iterations, i.e. alpha=1.590e+00, with an active set of 109 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,360:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 160 iterations, i.e. alpha=7.022e+04, with an active set of 116 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,360:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 142 iterations, i.e. alpha=1.252e+00, with an active set of 109 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,360:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 160 iterations, i.e. alpha=6.883e+04, with an active set of 116 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,362:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 161 iterations, i.e. alpha=6.544e+04, with an active set of 117 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,362:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 161 iterations, i.e. alpha=5.824e+03, with an active set of 117 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,363:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 161 iterations, i.e. alpha=5.639e+03, with an active set of 117 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,364:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 161 iterations, i.e. alpha=1.494e+03, with an active set of 117 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,364:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 161 iterations, i.e. alpha=5.132e+02, with an active set of 117 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,365:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 161 iterations, i.e. alpha=4.648e+02, with an active set of 117 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,366:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 161 iterations, i.e. alpha=3.196e+02, with an active set of 117 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,369:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 152 iterations, i.e. alpha=1.458e+00, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,369:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 89 iterations, i.e. alpha=5.158e-04, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,369:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 139 iterations, i.e. alpha=1.017e-01, with an active set of 110 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,370:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 152 iterations, i.e. alpha=1.191e+00, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,370:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 152 iterations, i.e. alpha=1.187e+00, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,371:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 152 iterations, i.e. alpha=8.825e-01, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,372:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 152 iterations, i.e. alpha=2.833e-01, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,372:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 92 iterations, i.e. alpha=4.410e-04, with an active set of 87 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,374:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 155 iterations, i.e. alpha=1.003e+00, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,375:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 145 iterations, i.e. alpha=7.343e-02, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,375:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 155 iterations, i.e. alpha=2.119e-01, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,376:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 155 iterations, i.e. alpha=2.033e-01, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,377:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 155 iterations, i.e. alpha=1.667e-01, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,377:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 147 iterations, i.e. alpha=6.826e-02, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,377:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 155 iterations, i.e. alpha=7.728e-02, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,378:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 147 iterations, i.e. alpha=4.106e-02, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,380:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 149 iterations, i.e. alpha=5.041e-05, with an active set of 116 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,381:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 149 iterations, i.e. alpha=4.211e-05, with an active set of 116 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,405:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 149 iterations, i.e. alpha=4.931e+05, with an active set of 111 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,407:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 151 iterations, i.e. alpha=2.187e+05, with an active set of 112 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,409:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 153 iterations, i.e. alpha=1.974e+05, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,409:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 153 iterations, i.e. alpha=1.737e+05, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,411:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 155 iterations, i.e. alpha=1.582e+05, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,411:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 155 iterations, i.e. alpha=1.222e+05, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,412:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 155 iterations, i.e. alpha=5.461e+04, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,413:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 155 iterations, i.e. alpha=4.064e+04, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,414:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 156 iterations, i.e. alpha=1.441e+04, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,414:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 156 iterations, i.e. alpha=1.153e+04, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,415:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 156 iterations, i.e. alpha=1.089e+04, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,415:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 156 iterations, i.e. alpha=9.860e+03, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,416:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 156 iterations, i.e. alpha=7.281e+03, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,416:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 156 iterations, i.e. alpha=6.978e+03, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,660:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:52:57,685:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.337e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,687:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.230e-03, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,690:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=2.080e-03, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,691:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=2.030e-03, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,693:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=1.952e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,710:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=1.234e-03, with an active set of 59 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,710:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=1.204e-03, with an active set of 59 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,711:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=1.203e-03, with an active set of 59 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,749:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 128 iterations, i.e. alpha=6.800e+00, with an active set of 89 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,754:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=8.564e+00, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,769:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 161 iterations, i.e. alpha=1.575e+06, with an active set of 108 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,770:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 162 iterations, i.e. alpha=1.143e+06, with an active set of 109 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,770:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 162 iterations, i.e. alpha=1.108e+06, with an active set of 109 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,771:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 162 iterations, i.e. alpha=1.083e+06, with an active set of 109 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,771:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 162 iterations, i.e. alpha=9.126e+05, with an active set of 109 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,772:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 162 iterations, i.e. alpha=8.585e+05, with an active set of 109 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,772:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 162 iterations, i.e. alpha=7.483e+05, with an active set of 109 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,773:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 162 iterations, i.e. alpha=6.749e+05, with an active set of 109 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,775:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 163 iterations, i.e. alpha=2.772e+05, with an active set of 110 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,775:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 163 iterations, i.e. alpha=1.513e+05, with an active set of 110 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,776:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 163 iterations, i.e. alpha=9.408e+04, with an active set of 110 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,776:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 163 iterations, i.e. alpha=7.049e+04, with an active set of 110 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,777:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 163 iterations, i.e. alpha=6.340e+04, with an active set of 110 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,777:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 163 iterations, i.e. alpha=1.227e+04, with an active set of 110 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:57,778:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 163 iterations, i.e. alpha=4.090e+03, with an active set of 110 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:58,018:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:52:58,073:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=2.749e-03, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:58,084:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 144 iterations, i.e. alpha=1.856e+04, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:58,085:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 144 iterations, i.e. alpha=1.320e+04, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:58,086:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 145 iterations, i.e. alpha=1.272e+04, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:58,086:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 145 iterations, i.e. alpha=1.224e+04, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:58,088:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 149 iterations, i.e. alpha=1.240e+04, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:58,091:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 153 iterations, i.e. alpha=9.852e+03, with an active set of 106 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:58,091:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 153 iterations, i.e. alpha=7.872e+03, with an active set of 106 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:58,091:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 153 iterations, i.e. alpha=7.056e+03, with an active set of 106 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:58,092:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 153 iterations, i.e. alpha=5.739e+03, with an active set of 106 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:58,092:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 153 iterations, i.e. alpha=4.517e+03, with an active set of 106 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-06 18:52:58,262:INFO:Calculating mean and std
2023-03-06 18:52:58,265:INFO:Creating metrics dataframe
2023-03-06 18:52:58,273:INFO:Uploading results into container
2023-03-06 18:52:58,274:INFO:Uploading model into container now
2023-03-06 18:52:58,276:INFO:_master_model_container: 5
2023-03-06 18:52:58,276:INFO:_display_container: 2
2023-03-06 18:52:58,277:INFO:Lars(random_state=6768)
2023-03-06 18:52:58,277:INFO:create_model() successfully completed......................................
2023-03-06 18:52:58,472:INFO:SubProcess create_model() end ==================================
2023-03-06 18:52:58,472:INFO:Creating metrics dataframe
2023-03-06 18:52:58,497:INFO:Initializing Lasso Least Angle Regression
2023-03-06 18:52:58,498:INFO:Total runtime is 0.09280690749486288 minutes
2023-03-06 18:52:58,507:INFO:SubProcess create_model() called ==================================
2023-03-06 18:52:58,508:INFO:Initializing create_model()
2023-03-06 18:52:58,508:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055FECCAC0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055FECCFA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:52:58,509:INFO:Checking exceptions
2023-03-06 18:52:58,509:INFO:Importing libraries
2023-03-06 18:52:58,509:INFO:Copying training dataset
2023-03-06 18:52:58,522:INFO:Defining folds
2023-03-06 18:52:58,523:INFO:Declaring metric variables
2023-03-06 18:52:58,534:INFO:Importing untrained model
2023-03-06 18:52:58,544:INFO:Lasso Least Angle Regression Imported successfully
2023-03-06 18:52:58,561:INFO:Starting cross validation
2023-03-06 18:52:58,564:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:52:58,788:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:52:58,813:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:52:58,859:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:52:58,864:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:52:58,922:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:52:58,949:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:52:58,954:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:52:58,995:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:52:59,042:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:52:59,068:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-06 18:52:59,178:INFO:Calculating mean and std
2023-03-06 18:52:59,181:INFO:Creating metrics dataframe
2023-03-06 18:52:59,195:INFO:Uploading results into container
2023-03-06 18:52:59,197:INFO:Uploading model into container now
2023-03-06 18:52:59,198:INFO:_master_model_container: 6
2023-03-06 18:52:59,199:INFO:_display_container: 2
2023-03-06 18:52:59,200:INFO:LassoLars(random_state=6768)
2023-03-06 18:52:59,201:INFO:create_model() successfully completed......................................
2023-03-06 18:52:59,559:INFO:SubProcess create_model() end ==================================
2023-03-06 18:52:59,560:INFO:Creating metrics dataframe
2023-03-06 18:52:59,585:INFO:Initializing Orthogonal Matching Pursuit
2023-03-06 18:52:59,586:INFO:Total runtime is 0.11093813975652059 minutes
2023-03-06 18:52:59,594:INFO:SubProcess create_model() called ==================================
2023-03-06 18:52:59,595:INFO:Initializing create_model()
2023-03-06 18:52:59,595:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055FECCAC0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055FECCFA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:52:59,595:INFO:Checking exceptions
2023-03-06 18:52:59,596:INFO:Importing libraries
2023-03-06 18:52:59,596:INFO:Copying training dataset
2023-03-06 18:52:59,609:INFO:Defining folds
2023-03-06 18:52:59,610:INFO:Declaring metric variables
2023-03-06 18:52:59,620:INFO:Importing untrained model
2023-03-06 18:52:59,633:INFO:Orthogonal Matching Pursuit Imported successfully
2023-03-06 18:52:59,658:INFO:Starting cross validation
2023-03-06 18:52:59,662:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:52:59,891:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:52:59,916:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:52:59,935:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:53:00,008:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:53:00,028:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:53:00,084:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:53:00,115:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:53:00,176:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:53:00,181:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-06 18:53:00,310:INFO:Calculating mean and std
2023-03-06 18:53:00,314:INFO:Creating metrics dataframe
2023-03-06 18:53:00,322:INFO:Uploading results into container
2023-03-06 18:53:00,324:INFO:Uploading model into container now
2023-03-06 18:53:00,325:INFO:_master_model_container: 7
2023-03-06 18:53:00,325:INFO:_display_container: 2
2023-03-06 18:53:00,326:INFO:OrthogonalMatchingPursuit()
2023-03-06 18:53:00,326:INFO:create_model() successfully completed......................................
2023-03-06 18:53:00,567:INFO:SubProcess create_model() end ==================================
2023-03-06 18:53:00,568:INFO:Creating metrics dataframe
2023-03-06 18:53:00,595:INFO:Initializing Bayesian Ridge
2023-03-06 18:53:00,596:INFO:Total runtime is 0.12777167558670044 minutes
2023-03-06 18:53:00,604:INFO:SubProcess create_model() called ==================================
2023-03-06 18:53:00,606:INFO:Initializing create_model()
2023-03-06 18:53:00,606:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055FECCAC0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055FECCFA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:53:00,606:INFO:Checking exceptions
2023-03-06 18:53:00,607:INFO:Importing libraries
2023-03-06 18:53:00,607:INFO:Copying training dataset
2023-03-06 18:53:00,621:INFO:Defining folds
2023-03-06 18:53:00,622:INFO:Declaring metric variables
2023-03-06 18:53:00,634:INFO:Importing untrained model
2023-03-06 18:53:00,646:INFO:Bayesian Ridge Imported successfully
2023-03-06 18:53:00,667:INFO:Starting cross validation
2023-03-06 18:53:00,673:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:53:01,325:INFO:Calculating mean and std
2023-03-06 18:53:01,329:INFO:Creating metrics dataframe
2023-03-06 18:53:01,336:INFO:Uploading results into container
2023-03-06 18:53:01,337:INFO:Uploading model into container now
2023-03-06 18:53:01,338:INFO:_master_model_container: 8
2023-03-06 18:53:01,338:INFO:_display_container: 2
2023-03-06 18:53:01,340:INFO:BayesianRidge()
2023-03-06 18:53:01,340:INFO:create_model() successfully completed......................................
2023-03-06 18:53:01,578:INFO:SubProcess create_model() end ==================================
2023-03-06 18:53:01,579:INFO:Creating metrics dataframe
2023-03-06 18:53:01,605:INFO:Initializing Passive Aggressive Regressor
2023-03-06 18:53:01,605:INFO:Total runtime is 0.1445857048034668 minutes
2023-03-06 18:53:01,614:INFO:SubProcess create_model() called ==================================
2023-03-06 18:53:01,615:INFO:Initializing create_model()
2023-03-06 18:53:01,616:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055FECCAC0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055FECCFA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:53:01,616:INFO:Checking exceptions
2023-03-06 18:53:01,616:INFO:Importing libraries
2023-03-06 18:53:01,616:INFO:Copying training dataset
2023-03-06 18:53:01,630:INFO:Defining folds
2023-03-06 18:53:01,630:INFO:Declaring metric variables
2023-03-06 18:53:01,641:INFO:Importing untrained model
2023-03-06 18:53:01,654:INFO:Passive Aggressive Regressor Imported successfully
2023-03-06 18:53:01,674:INFO:Starting cross validation
2023-03-06 18:53:01,679:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:53:02,298:INFO:Calculating mean and std
2023-03-06 18:53:02,303:INFO:Creating metrics dataframe
2023-03-06 18:53:02,311:INFO:Uploading results into container
2023-03-06 18:53:02,313:INFO:Uploading model into container now
2023-03-06 18:53:02,314:INFO:_master_model_container: 9
2023-03-06 18:53:02,315:INFO:_display_container: 2
2023-03-06 18:53:02,316:INFO:PassiveAggressiveRegressor(random_state=6768)
2023-03-06 18:53:02,316:INFO:create_model() successfully completed......................................
2023-03-06 18:53:02,533:INFO:SubProcess create_model() end ==================================
2023-03-06 18:53:02,533:INFO:Creating metrics dataframe
2023-03-06 18:53:02,559:INFO:Initializing Huber Regressor
2023-03-06 18:53:02,559:INFO:Total runtime is 0.16049362023671468 minutes
2023-03-06 18:53:02,567:INFO:SubProcess create_model() called ==================================
2023-03-06 18:53:02,568:INFO:Initializing create_model()
2023-03-06 18:53:02,568:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055FECCAC0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055FECCFA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:53:02,568:INFO:Checking exceptions
2023-03-06 18:53:02,569:INFO:Importing libraries
2023-03-06 18:53:02,569:INFO:Copying training dataset
2023-03-06 18:53:02,582:INFO:Defining folds
2023-03-06 18:53:02,582:INFO:Declaring metric variables
2023-03-06 18:53:02,592:INFO:Importing untrained model
2023-03-06 18:53:02,604:INFO:Huber Regressor Imported successfully
2023-03-06 18:53:02,626:INFO:Starting cross validation
2023-03-06 18:53:02,632:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:53:03,180:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-06 18:53:03,771:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-06 18:53:04,208:INFO:Calculating mean and std
2023-03-06 18:53:04,213:INFO:Creating metrics dataframe
2023-03-06 18:53:04,221:INFO:Uploading results into container
2023-03-06 18:53:04,224:INFO:Uploading model into container now
2023-03-06 18:53:04,225:INFO:_master_model_container: 10
2023-03-06 18:53:04,226:INFO:_display_container: 2
2023-03-06 18:53:04,227:INFO:HuberRegressor()
2023-03-06 18:53:04,227:INFO:create_model() successfully completed......................................
2023-03-06 18:53:04,431:INFO:SubProcess create_model() end ==================================
2023-03-06 18:53:04,432:INFO:Creating metrics dataframe
2023-03-06 18:53:04,458:INFO:Initializing K Neighbors Regressor
2023-03-06 18:53:04,459:INFO:Total runtime is 0.19216342369715372 minutes
2023-03-06 18:53:04,467:INFO:SubProcess create_model() called ==================================
2023-03-06 18:53:04,468:INFO:Initializing create_model()
2023-03-06 18:53:04,469:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055FECCAC0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055FECCFA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:53:04,469:INFO:Checking exceptions
2023-03-06 18:53:04,469:INFO:Importing libraries
2023-03-06 18:53:04,469:INFO:Copying training dataset
2023-03-06 18:53:04,478:INFO:Defining folds
2023-03-06 18:53:04,478:INFO:Declaring metric variables
2023-03-06 18:53:04,487:INFO:Importing untrained model
2023-03-06 18:53:04,498:INFO:K Neighbors Regressor Imported successfully
2023-03-06 18:53:04,518:INFO:Starting cross validation
2023-03-06 18:53:04,521:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:53:05,159:INFO:Calculating mean and std
2023-03-06 18:53:05,163:INFO:Creating metrics dataframe
2023-03-06 18:53:05,172:INFO:Uploading results into container
2023-03-06 18:53:05,173:INFO:Uploading model into container now
2023-03-06 18:53:05,175:INFO:_master_model_container: 11
2023-03-06 18:53:05,176:INFO:_display_container: 2
2023-03-06 18:53:05,177:INFO:KNeighborsRegressor(n_jobs=-1)
2023-03-06 18:53:05,178:INFO:create_model() successfully completed......................................
2023-03-06 18:53:05,403:INFO:SubProcess create_model() end ==================================
2023-03-06 18:53:05,403:INFO:Creating metrics dataframe
2023-03-06 18:53:05,439:INFO:Initializing Decision Tree Regressor
2023-03-06 18:53:05,439:INFO:Total runtime is 0.20848644971847535 minutes
2023-03-06 18:53:05,452:INFO:SubProcess create_model() called ==================================
2023-03-06 18:53:05,453:INFO:Initializing create_model()
2023-03-06 18:53:05,454:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055FECCAC0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055FECCFA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:53:05,455:INFO:Checking exceptions
2023-03-06 18:53:05,455:INFO:Importing libraries
2023-03-06 18:53:05,456:INFO:Copying training dataset
2023-03-06 18:53:05,474:INFO:Defining folds
2023-03-06 18:53:05,474:INFO:Declaring metric variables
2023-03-06 18:53:05,489:INFO:Importing untrained model
2023-03-06 18:53:05,504:INFO:Decision Tree Regressor Imported successfully
2023-03-06 18:53:05,533:INFO:Starting cross validation
2023-03-06 18:53:05,539:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:53:06,148:INFO:Calculating mean and std
2023-03-06 18:53:06,152:INFO:Creating metrics dataframe
2023-03-06 18:53:06,159:INFO:Uploading results into container
2023-03-06 18:53:06,161:INFO:Uploading model into container now
2023-03-06 18:53:06,162:INFO:_master_model_container: 12
2023-03-06 18:53:06,162:INFO:_display_container: 2
2023-03-06 18:53:06,162:INFO:DecisionTreeRegressor(random_state=6768)
2023-03-06 18:53:06,163:INFO:create_model() successfully completed......................................
2023-03-06 18:53:06,365:INFO:SubProcess create_model() end ==================================
2023-03-06 18:53:06,366:INFO:Creating metrics dataframe
2023-03-06 18:53:06,394:INFO:Initializing Random Forest Regressor
2023-03-06 18:53:06,394:INFO:Total runtime is 0.22441007296244303 minutes
2023-03-06 18:53:06,404:INFO:SubProcess create_model() called ==================================
2023-03-06 18:53:06,404:INFO:Initializing create_model()
2023-03-06 18:53:06,405:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055FECCAC0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055FECCFA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:53:06,406:INFO:Checking exceptions
2023-03-06 18:53:06,406:INFO:Importing libraries
2023-03-06 18:53:06,407:INFO:Copying training dataset
2023-03-06 18:53:06,420:INFO:Defining folds
2023-03-06 18:53:06,421:INFO:Declaring metric variables
2023-03-06 18:53:06,433:INFO:Importing untrained model
2023-03-06 18:53:06,442:INFO:Random Forest Regressor Imported successfully
2023-03-06 18:53:06,457:INFO:Starting cross validation
2023-03-06 18:53:06,460:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:53:08,288:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:53:08,291:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:53:08,291:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:53:08,370:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:53:08,393:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:53:08,404:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:53:08,497:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:53:09,836:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:53:10,143:INFO:Calculating mean and std
2023-03-06 18:53:10,145:INFO:Creating metrics dataframe
2023-03-06 18:53:10,153:INFO:Uploading results into container
2023-03-06 18:53:10,154:INFO:Uploading model into container now
2023-03-06 18:53:10,155:INFO:_master_model_container: 13
2023-03-06 18:53:10,156:INFO:_display_container: 2
2023-03-06 18:53:10,156:INFO:RandomForestRegressor(n_jobs=-1, random_state=6768)
2023-03-06 18:53:10,157:INFO:create_model() successfully completed......................................
2023-03-06 18:53:10,358:INFO:SubProcess create_model() end ==================================
2023-03-06 18:53:10,358:INFO:Creating metrics dataframe
2023-03-06 18:53:10,387:INFO:Initializing Extra Trees Regressor
2023-03-06 18:53:10,387:INFO:Total runtime is 0.2909526268641154 minutes
2023-03-06 18:53:10,397:INFO:SubProcess create_model() called ==================================
2023-03-06 18:53:10,398:INFO:Initializing create_model()
2023-03-06 18:53:10,398:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055FECCAC0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055FECCFA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:53:10,398:INFO:Checking exceptions
2023-03-06 18:53:10,399:INFO:Importing libraries
2023-03-06 18:53:10,399:INFO:Copying training dataset
2023-03-06 18:53:10,412:INFO:Defining folds
2023-03-06 18:53:10,412:INFO:Declaring metric variables
2023-03-06 18:53:10,425:INFO:Importing untrained model
2023-03-06 18:53:10,436:INFO:Extra Trees Regressor Imported successfully
2023-03-06 18:53:10,457:INFO:Starting cross validation
2023-03-06 18:53:10,463:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:53:12,302:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:53:12,368:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:53:12,376:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:53:12,428:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:53:12,440:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:53:12,455:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:53:12,493:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:53:12,513:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:53:14,186:INFO:Calculating mean and std
2023-03-06 18:53:14,189:INFO:Creating metrics dataframe
2023-03-06 18:53:14,198:INFO:Uploading results into container
2023-03-06 18:53:14,200:INFO:Uploading model into container now
2023-03-06 18:53:14,200:INFO:_master_model_container: 14
2023-03-06 18:53:14,201:INFO:_display_container: 2
2023-03-06 18:53:14,202:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=6768)
2023-03-06 18:53:14,202:INFO:create_model() successfully completed......................................
2023-03-06 18:53:14,400:INFO:SubProcess create_model() end ==================================
2023-03-06 18:53:14,401:INFO:Creating metrics dataframe
2023-03-06 18:53:14,431:INFO:Initializing AdaBoost Regressor
2023-03-06 18:53:14,432:INFO:Total runtime is 0.35837775468826294 minutes
2023-03-06 18:53:14,440:INFO:SubProcess create_model() called ==================================
2023-03-06 18:53:14,441:INFO:Initializing create_model()
2023-03-06 18:53:14,442:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055FECCAC0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055FECCFA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:53:14,442:INFO:Checking exceptions
2023-03-06 18:53:14,442:INFO:Importing libraries
2023-03-06 18:53:14,442:INFO:Copying training dataset
2023-03-06 18:53:14,455:INFO:Defining folds
2023-03-06 18:53:14,456:INFO:Declaring metric variables
2023-03-06 18:53:14,465:INFO:Importing untrained model
2023-03-06 18:53:14,478:INFO:AdaBoost Regressor Imported successfully
2023-03-06 18:53:14,500:INFO:Starting cross validation
2023-03-06 18:53:14,506:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:53:15,937:INFO:Calculating mean and std
2023-03-06 18:53:15,941:INFO:Creating metrics dataframe
2023-03-06 18:53:15,950:INFO:Uploading results into container
2023-03-06 18:53:15,951:INFO:Uploading model into container now
2023-03-06 18:53:15,952:INFO:_master_model_container: 15
2023-03-06 18:53:15,952:INFO:_display_container: 2
2023-03-06 18:53:15,953:INFO:AdaBoostRegressor(random_state=6768)
2023-03-06 18:53:15,954:INFO:create_model() successfully completed......................................
2023-03-06 18:53:16,152:INFO:SubProcess create_model() end ==================================
2023-03-06 18:53:16,152:INFO:Creating metrics dataframe
2023-03-06 18:53:16,181:INFO:Initializing Gradient Boosting Regressor
2023-03-06 18:53:16,182:INFO:Total runtime is 0.38753852049509685 minutes
2023-03-06 18:53:16,190:INFO:SubProcess create_model() called ==================================
2023-03-06 18:53:16,191:INFO:Initializing create_model()
2023-03-06 18:53:16,191:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055FECCAC0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055FECCFA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:53:16,191:INFO:Checking exceptions
2023-03-06 18:53:16,191:INFO:Importing libraries
2023-03-06 18:53:16,192:INFO:Copying training dataset
2023-03-06 18:53:16,202:INFO:Defining folds
2023-03-06 18:53:16,203:INFO:Declaring metric variables
2023-03-06 18:53:16,214:INFO:Importing untrained model
2023-03-06 18:53:16,227:INFO:Gradient Boosting Regressor Imported successfully
2023-03-06 18:53:16,247:INFO:Starting cross validation
2023-03-06 18:53:16,252:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:53:17,353:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:53:17,365:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:53:17,383:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:53:17,402:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:53:17,427:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:53:17,478:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:53:17,482:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:53:17,527:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:53:18,606:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:53:18,608:WARNING:C:\Users\Alvaro\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:252: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-06 18:53:18,862:INFO:Calculating mean and std
2023-03-06 18:53:18,865:INFO:Creating metrics dataframe
2023-03-06 18:53:18,873:INFO:Uploading results into container
2023-03-06 18:53:18,875:INFO:Uploading model into container now
2023-03-06 18:53:18,877:INFO:_master_model_container: 16
2023-03-06 18:53:18,877:INFO:_display_container: 2
2023-03-06 18:53:18,877:INFO:GradientBoostingRegressor(random_state=6768)
2023-03-06 18:53:18,878:INFO:create_model() successfully completed......................................
2023-03-06 18:53:19,074:INFO:SubProcess create_model() end ==================================
2023-03-06 18:53:19,074:INFO:Creating metrics dataframe
2023-03-06 18:53:19,105:INFO:Initializing Extreme Gradient Boosting
2023-03-06 18:53:19,105:INFO:Total runtime is 0.4362552007039388 minutes
2023-03-06 18:53:19,115:INFO:SubProcess create_model() called ==================================
2023-03-06 18:53:19,116:INFO:Initializing create_model()
2023-03-06 18:53:19,116:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055FECCAC0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055FECCFA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:53:19,116:INFO:Checking exceptions
2023-03-06 18:53:19,117:INFO:Importing libraries
2023-03-06 18:53:19,117:INFO:Copying training dataset
2023-03-06 18:53:19,129:INFO:Defining folds
2023-03-06 18:53:19,129:INFO:Declaring metric variables
2023-03-06 18:53:19,139:INFO:Importing untrained model
2023-03-06 18:53:19,148:INFO:Extreme Gradient Boosting Imported successfully
2023-03-06 18:53:19,166:INFO:Starting cross validation
2023-03-06 18:53:19,169:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:53:22,211:INFO:Calculating mean and std
2023-03-06 18:53:22,215:INFO:Creating metrics dataframe
2023-03-06 18:53:22,223:INFO:Uploading results into container
2023-03-06 18:53:22,224:INFO:Uploading model into container now
2023-03-06 18:53:22,225:INFO:_master_model_container: 17
2023-03-06 18:53:22,225:INFO:_display_container: 2
2023-03-06 18:53:22,229:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=6768, ...)
2023-03-06 18:53:22,229:INFO:create_model() successfully completed......................................
2023-03-06 18:53:22,427:INFO:SubProcess create_model() end ==================================
2023-03-06 18:53:22,427:INFO:Creating metrics dataframe
2023-03-06 18:53:22,461:INFO:Initializing Light Gradient Boosting Machine
2023-03-06 18:53:22,461:INFO:Total runtime is 0.49218140840530394 minutes
2023-03-06 18:53:22,473:INFO:SubProcess create_model() called ==================================
2023-03-06 18:53:22,475:INFO:Initializing create_model()
2023-03-06 18:53:22,475:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055FECCAC0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055FECCFA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:53:22,477:INFO:Checking exceptions
2023-03-06 18:53:22,477:INFO:Importing libraries
2023-03-06 18:53:22,477:INFO:Copying training dataset
2023-03-06 18:53:22,495:INFO:Defining folds
2023-03-06 18:53:22,497:INFO:Declaring metric variables
2023-03-06 18:53:22,515:INFO:Importing untrained model
2023-03-06 18:53:22,529:INFO:Light Gradient Boosting Machine Imported successfully
2023-03-06 18:53:22,562:INFO:Starting cross validation
2023-03-06 18:53:22,568:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:53:23,527:INFO:Calculating mean and std
2023-03-06 18:53:23,531:INFO:Creating metrics dataframe
2023-03-06 18:53:23,538:INFO:Uploading results into container
2023-03-06 18:53:23,540:INFO:Uploading model into container now
2023-03-06 18:53:23,540:INFO:_master_model_container: 18
2023-03-06 18:53:23,541:INFO:_display_container: 2
2023-03-06 18:53:23,542:INFO:LGBMRegressor(random_state=6768)
2023-03-06 18:53:23,542:INFO:create_model() successfully completed......................................
2023-03-06 18:53:23,752:INFO:SubProcess create_model() end ==================================
2023-03-06 18:53:23,752:INFO:Creating metrics dataframe
2023-03-06 18:53:23,784:INFO:Initializing Dummy Regressor
2023-03-06 18:53:23,785:INFO:Total runtime is 0.514260462919871 minutes
2023-03-06 18:53:23,793:INFO:SubProcess create_model() called ==================================
2023-03-06 18:53:23,794:INFO:Initializing create_model()
2023-03-06 18:53:23,794:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055FECCAC0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002055FECCFA0>, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:53:23,794:INFO:Checking exceptions
2023-03-06 18:53:23,796:INFO:Importing libraries
2023-03-06 18:53:23,796:INFO:Copying training dataset
2023-03-06 18:53:23,808:INFO:Defining folds
2023-03-06 18:53:23,808:INFO:Declaring metric variables
2023-03-06 18:53:23,818:INFO:Importing untrained model
2023-03-06 18:53:23,828:INFO:Dummy Regressor Imported successfully
2023-03-06 18:53:23,846:INFO:Starting cross validation
2023-03-06 18:53:23,850:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-06 18:53:24,374:INFO:Calculating mean and std
2023-03-06 18:53:24,379:INFO:Creating metrics dataframe
2023-03-06 18:53:24,389:INFO:Uploading results into container
2023-03-06 18:53:24,391:INFO:Uploading model into container now
2023-03-06 18:53:24,392:INFO:_master_model_container: 19
2023-03-06 18:53:24,392:INFO:_display_container: 2
2023-03-06 18:53:24,393:INFO:DummyRegressor()
2023-03-06 18:53:24,393:INFO:create_model() successfully completed......................................
2023-03-06 18:53:24,632:INFO:SubProcess create_model() end ==================================
2023-03-06 18:53:24,632:INFO:Creating metrics dataframe
2023-03-06 18:53:24,701:INFO:Initializing create_model()
2023-03-06 18:53:24,701:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055FECCAC0>, estimator=GradientBoostingRegressor(random_state=6768), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-06 18:53:24,702:INFO:Checking exceptions
2023-03-06 18:53:24,709:INFO:Importing libraries
2023-03-06 18:53:24,709:INFO:Copying training dataset
2023-03-06 18:53:24,723:INFO:Defining folds
2023-03-06 18:53:24,723:INFO:Declaring metric variables
2023-03-06 18:53:24,724:INFO:Importing untrained model
2023-03-06 18:53:24,725:INFO:Declaring custom model
2023-03-06 18:53:24,726:INFO:Gradient Boosting Regressor Imported successfully
2023-03-06 18:53:24,730:INFO:Cross validation set to False
2023-03-06 18:53:24,731:INFO:Fitting Model
2023-03-06 18:53:25,350:INFO:GradientBoostingRegressor(random_state=6768)
2023-03-06 18:53:25,350:INFO:create_model() successfully completed......................................
2023-03-06 18:53:25,665:INFO:_master_model_container: 19
2023-03-06 18:53:25,666:INFO:_display_container: 2
2023-03-06 18:53:25,667:INFO:GradientBoostingRegressor(random_state=6768)
2023-03-06 18:53:25,668:INFO:compare_models() successfully completed......................................
2023-03-06 18:53:25,695:INFO:Initializing evaluate_model()
2023-03-06 18:53:25,695:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055FECCAC0>, estimator=GradientBoostingRegressor(random_state=6768), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-03-06 18:53:25,755:INFO:Initializing plot_model()
2023-03-06 18:53:25,756:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=6768), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055FECCAC0>, system=True)
2023-03-06 18:53:25,757:INFO:Checking exceptions
2023-03-06 18:53:25,764:INFO:Preloading libraries
2023-03-06 18:53:25,790:INFO:Copying training dataset
2023-03-06 18:53:25,790:INFO:Plot type: pipeline
2023-03-06 18:53:25,979:INFO:Visual Rendered Successfully
2023-03-06 18:53:26,189:INFO:plot_model() successfully completed......................................
2023-03-06 18:53:26,212:INFO:Initializing predict_model()
2023-03-06 18:53:26,212:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055FECCAC0>, estimator=GradientBoostingRegressor(random_state=6768), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x0000020560E53EE0>)
2023-03-06 18:53:26,212:INFO:Checking exceptions
2023-03-06 18:53:26,212:INFO:Preloading libraries
2023-03-06 18:53:26,218:INFO:Set up data.
2023-03-06 18:53:26,270:INFO:Set up index.
2023-03-06 18:53:43,913:INFO:Initializing predict_model()
2023-03-06 18:53:43,914:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002055FECCAC0>, estimator=GradientBoostingRegressor(random_state=6768), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x0000020560E535E0>)
2023-03-06 18:53:43,914:INFO:Checking exceptions
2023-03-06 18:53:43,914:INFO:Preloading libraries
2023-03-06 18:53:43,919:INFO:Set up data.
2023-03-06 18:53:43,974:INFO:Set up index.
